% Please compile this document using LuaLaTeX.
% because of the use of unicode-math
% XeLaTeX and PDFLaTeX will result in error.

\newif\iffull\fulltrue
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

% page number
\pagestyle{plain}

% \usepackage{cite}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%%%%% PACKAGES
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[margin=2.5cm]{geometry}

% allow page break in align environment
\allowdisplaybreaks

% Biblatex
\usepackage[style=numeric]{biblatex}
\addbibresource{refs.bib}

% For adding inline comments in the text.
\usepackage[margin=false,inline=true]{fixme}
\FXRegisterAuthor{aaa}{anaaa}{\color{cyan}AAA}
\FXRegisterAuthor{mg}{anmg}{\color{red}MG}
\FXRegisterAuthor{cz}{ancz}{\color{orange}CZ}
% \newcommand{\aaa}[1]{\aaanote{#1}}
% \newcommand{\mg}[1]{\mgnote{#1}}
% \newcommand{\cz}[1]{\cznote{#1}}
\newcommand{\aaa}[1]{}
\newcommand{\mg}[1]{}
\newcommand{\cz}[1]{}

\usepackage{stmaryrd}

\usepackage{stackengine}
\usepackage{mathrsfs}
\usepackage{braket}
\usepackage{annotate-equations}
\usepackage{scalerel}

% graphs
\usepackage{tikz}
\usetikzlibrary{arrows,automata,positioning}

% commutative diagram
\usepackage{tikz-cd}

% ref
\usepackage{hyperref}
\usepackage{cleveref}

% inference rule
\usepackage{mathpartir}
% cross-referencing infer rule
% based on https://tex.stackexchange.com/questions/340788/cross-referencing-inference-rules
\makeatletter
\let\originferrule\inferrule
\DeclareDocumentCommand \inferrule { s O {} m m}{%
  \IfBooleanTF{#1}%
  {%
    \mpr@inferstar[#2]{#3}{#4}%
  }{%
    \mpr@inferrule[#2]{#3}{#4}%
  }%
  \IfValueT{#2}%
  {%
    \my@name@inferrule{#2}%
  }%
}
\NewDocumentCommand \my@name@inferrule { m }{%
  \def\@currentlabelname{\textsc{#1}}%
}
\makeatother

% item spacing
\usepackage{enumitem}

% for code
\usepackage{listings}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{framextopmargin=50pt,frame=bottomline}
\lstset{language=caml, escapeinside={[*}{*]}}

% for algorithm
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
% switch statement for pattern matching
\algnewcommand\algorithmicmatch{\textbf{match}}
\algnewcommand\algorithmicwith{\textbf{with}}
\algnewcommand\algorithmiccase{\textbf{case}}
\algnewcommand\algorithmicdefault{\textbf{default}}
\algnewcommand\Continue{\textbf{continue}}
\algdef{SE}[MATCH]{Match}{EndMatch}[1]{\algorithmicmatch\ #1\ \algorithmicwith}{\algorithmicend\ \algorithmicmatch}%
\algdef{SE}[CASE]{Case}{EndCase}[1]{\algorithmiccase\ #1 \algorithmicthen}{\algorithmicend\ \algorithmiccase}%
\algdef{SE}[DEFAULT]{Default}{EndDefault}{\hskip\algorithmicindent\algorithmicdefault}{\algorithmicend\algorithmicdefault}%
\algtext*{EndMatch}%
\algtext*{EndCase}%
\algtext*{EndDefault}%

% for better table
\usepackage{booktabs}

% subcaption
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption

% unicode math symbols
\usepackage{unicode-math}
% support for hat, overline, underline, vec, and sim combining charactors
\protected\def\afteracc{\directlua{
    local nest = tex.nest[tex.nest.ptr]
    local last = nest.tail
    if not (last and last.id == 18) then
      error'I can only put accents on simple noads.'
    end
    if last.sub or last.sup then
      error'If you want accents on a superscript or subscript, please use braces.'
    end
    local acc = node.new(21, 1)
    acc.nucleus = last.nucleus
    last.nucleus = nil
    local is_bottom = token.scan_keyword'bot' and 'bot_accent' or 'accent'
    acc[is_bottom] = node.new(23)
    acc[is_bottom].fam, acc[is_bottom].char = 0, token.scan_int()
    nest.head = node.insert_after(node.remove(nest.head, last), nil, acc)
    nest.tail = acc
    node.flush_node(last)
  }}
\AtBeginDocument{
\begingroup
  \def\UnicodeMathSymbol#1#2#3#4{%
    \ifx#3\mathaccent
      \def\mytmpmacro{\afteracc#1 }%
      \global\letcharcode#1=\mytmpmacro
      \global\mathcode#1="8000
    \else\ifx#3\mathbotaccentwide
      \def\mytmpmacro{\afteracc bot#1 }%
      \global\letcharcode#1=\mytmpmacro
      \global\mathcode#1="8000
    \fi\fi
  }
  \input{unicode-math-table}
\endgroup
}

% math font, this is needed to render \setminus command
\setmathfont{latinmodern-math}
\setmathfont[range=\setminus]{STIX Two Math}
\setmathfont[range=\similarrightarrow]{STIX Two Math}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}

\newtheorem{definition}{Definition}
\newtheorem*{definition*}{Definition}
\newtheorem{example}[definition]{Example}
\newtheorem{remark}[definition]{Remark}

%%%% Macros %%%%%

% Math?
\newcommand{\true}{\mathrm{true}}
\newcommand{\false}{\mathrm{false}}
\newcommand{\At}{\mathbf{At}}


% operators
\newcommand{\dom}[1]{\mathrm{dom}(#1)}
\newcommand{\cod}[1]{\mathrm{cod}(#1)}
\DeclareMathOperator{\post}{\mathrm{post}}
\newcommand{\reject}{\mathinner{\mathrm{rej}}}
\newcommand{\accept}{\mathinner{\mathrm{acc}}}
\DeclareMathOperator*{\bigplus}{\scalerel*{+}{\sum}}
\newcommand{\clos}[1]{\mathrel{\overline{#1}}}
\DeclareMathOperator{\norm}{\mathrm{norm}}
\DeclareMathOperator{\dead}{\mathrm{dead}}
\DeclareMathOperator{\symb}{\mathrm{symb}}
\DeclareMathOperator{\unsymb}{\symb^{-1}}


% commands 
\newcommand{\command}[1]{{\mathtt{#1}}}
\newcommand{\comAssume}[1]{\command{assume}~#1}
\newcommand{\comITE}[3]{\command{if}~#1~\command{then}~#2~\command{else}~#3}
\newcommand{\comWhile}[2]{\command{while}~#1~\command{do}~#2}

% set of models
\newcommand{\theoryOf}[1]{\ensuremath{\mathsf{#1}}}
\newcommand{\Exp}{\theoryOf{Exp}}
\newcommand{\GKAT}{\theoryOf{GKAT}}
\newcommand{\Bool}{\theoryOf{Bool}}
\DeclareMathOperator{\GS}{\mathrm{GS}}

\newcommand\altxrightarrow[2][0pt]{\mathrel{\ensurestackMath{\stackengine%
  {\dimexpr#1-7.5pt}{\xrightarrow{\phantom{#2}}}{\scriptstyle\!#2\,}%
  {O}{c}{F}{F}{S}}}}
\newcommand{\transvia}[1]{
    \mathrel{\raisebox{-2px}{\(\altxrightarrow[-2px]{#1}\)}}
}
\newcommand{\transAcc}[2]{⇒_{#1} #2}

 

\begin{document}

\title{Symbolic On-the-fly Algorithms for GKAT Equivalences
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Cheng Zhang\textsuperscript{\textsection}}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{University College London}\\
London, United Kingdom \\
0000-0002-8197-6181}
\and
\IEEEauthorblockN{Qiancheng Fu}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
email address or ORCID}
\and
\IEEEauthorblockN{Hang Ji}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
email address or ORCID}
\and
\IEEEauthorblockN{Ines Santacruz}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
email address or ORCID}
\and
\IEEEauthorblockN{Marco Gaboardi}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
email address or ORCID}
}

\maketitle
\begingroup\renewcommand\thefootnote{\textsection}
\footnotetext{Work largely performed at Boston University}
\endgroup

\begin{abstract}
This document is a model and instructions for \LaTeX.
This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert.
\end{IEEEkeywords}



\section{Introduction}

\paragraph{Notation: } In this paper, we will use un-curried notation to apply curried functions, for example, given a function \(δ: X → Y → Z\), we will write the function applications as follow \(δ(x): Y → Z\) and \(δ(x, y): Z\). And when drawing commutative diagram, we will leave function restriction implicit. Specifically given \(A' ⊆ A\), and a function \(h: A → B\), we will draw:
\[
    \begin{tikzcd}
        A' \ar{r}{h} & B
    \end{tikzcd}
\]
where the function \(h\) is implicitly restricted to \(A'\).
For bifunctors like \((-) × (-)\), we will write function lifting by applying the bifunctors on these functions: for example, given \(h₁: A₁ → B₁\) and \(h₂: A₂ → B₂\), we will use \[h₁ × h₂: A₁ × A₂ → B₁ × B₂\] to denote the bifunctorial lift of \(h₁\) and \(h₂\) via product \((-) × (-)\).

\section{Background on Coalgebra and GKAT}

\subsection{Concepts in Universal Coalgebra}

In this paper, we will make heavy use of coalgebraic theory, thus it is empirical for us to recall some notions and useful theorems in universal coalgebra.
Given a functor \(F\) on the category of set and functions, a \emph{coalgebra over \(F\)} or \emph{\(F\)-coalgebra} consists of a set \(S\) and a function \(σ_S: S → F(S)\).
We typically call elements in \(S\) the \emph{states} of the coalgebra, and \(σ_S(s)\) the \emph{dynamic} of state \(s\).
We will sometimes use the states \(S\) to denote the coalgebra, when no ambiguity can arise. 

A homomorphism between two \(F\)-coalgebra \(S\) and \(U\) is a map \(h: S → U\) that preserves the function \(σ\); diagrammatically, the following diagram commutes:
\[
    \begin{tikzcd}
        S \ar{r}{h} \ar[swap]{d}{σ_S} & U \ar{d}{σ_U} \\  
        F(S) \ar{r}{F(h)} & F(U)
    \end{tikzcd}    
\]

When we can restrict the homomorphism map into a inclusion map \(i: S' → S\) for \(S' ⊆ S\), then we say that \(S'\) is a \emph{sub-coalgebra} of \(S\), denoted as \(S' ⊑ S\). Specifically, the following diagram commutes when \(S' ⊑ S\):
\[
    \begin{tikzcd}
        S' \ar[hook]{r}{i} \ar[swap]{d}{σ_{S'}} & S \ar{d}{σ_S} \\  
        F(S') \ar[hook]{r}{F(i)} & F(S)
    \end{tikzcd}    
\]
In fact, the function \(σ_{S'}\) is uniquely determined by the states \(S'\)~\cite[Proposition 6.1]{rutten_UniversalCoalgebraTheory_2000}.

The sub-coalgebras are preserved under homomorphic images and pre-images: 
\begin{lemma}[Theorem 6.3~\cite{rutten_UniversalCoalgebraTheory_2000}]\label{thm:hom-(pre)img-preserve-sub-coalg}
    given a homomorphism \(h: S → U\), and sub-coalgebras \(S' ⊑ S\) and \(U' ⊑ U\), then 
    \[h(S') ⊑ U \text{ and } h^{-1}(U') ⊑ S.\]
\end{lemma}

One particularly important sub-coalgebra of and coalgebra \(S\) is the least coalgebra generated by a single element \(s\). 
We will denote this sub-coalgebra as \(⟨s⟩_{S}\), and call it \emph{principle sub-coalgebra} generated by \(s\). 
We sometimes omit the subscript \(S\) when it can be inferred from context or irrelevant.
Intuitively, we usually think of principle sub-coalgebra \(⟨s⟩_S\) as the sub-coalgebra that is formed by all the ``reachable state'' form the state \(s\).
This coalgebraic characterization of reachable state can allow us to avoid induction on the length of path from \(s\) to reach another state.

For all coalgebra \(S\) and a state \(s ∈ S\), principle sub-coalgebra \(⟨s⟩_S\) always exists and is unique, because sub-coalgebra of any coalgebra forms a complete lattice~\cite[theorem 6.4]{rutten_UniversalCoalgebraTheory_2000}; thus taking the meet of all the sub-coalgebra that contains \(s\) will yield \(⟨s⟩_S\).

Similar to sub-coalgebra, principle sub-coalgebra is also preserved under homomorphic image:
\begin{theorem}\label{thm:homo-img-preserve-principle-sub-coalg}
    Homomorphic image preserves principle sub-GKAT coalgebra. Specifically, given a homomorphism \(h: S → U\):
    \[h(⟨s⟩_S) = ⟨h(s)⟩_U\]
\end{theorem}

\begin{proof}
    We will need to show that \(h(⟨s⟩_{S})\) is the smallest sub-GKAT coalgebra of \(S\) that contain \(h(s)\). First by definition of image, \(h(s) ∈ h(⟨s⟩_{S})\); second by \cref{thm:hom-(pre)img-preserve-sub-coalg}, \(h(⟨S⟩_S) ⊑ U\).

    Finally, take any \(U' ⊑ U\) and \(h(s) ∈ U'\), recall that by \cref{thm:hom-(pre)img-preserve-sub-coalg}, \(h^{-1}(U') ⊑ S\). We can then derive that \(h(⟨s⟩_S) ⊑ U'\): 
    \begin{align*}
        h(s) ∈ U' 
        & ⟹ s ∈ h^{-1}(U') \\  
        & ⟹ ⟨s⟩_S ⊑ h^{-1}(U') & \text{definition of \(⟨s⟩_S\)}\\  
        & ⟹ h(⟨s⟩_S) ⊑ U' & \text{\cref{thm:hom-(pre)img-preserve-sub-coalg}}
    \end{align*}
    Hence \(h(⟨s⟩_S)\) is the smallest sub-GKAT coalgebra of \(U\) that contains \(h(s)\).
\end{proof}

% bisimulation

A \emph{final coalgebra} \(ℱ\) over a signature \(F\), sometimes called the \emph{behavior} of coalgebras over \(F\), is a \(F\)-coalgebra s.t. for all \(F\)-coalgebra \(S\), there exists a unique homomorphism \(⟦-⟧_S: S → ℱ\).

Given two \(F\)-coalgebra \(S\) and \(U\), the \emph{behavioral equivalence} between states in \(S\) and \(U\) can be computed by a notion called \emph{bisimulation}.
A relation \({∼} ⊆ S × U\) is called a \emph{bisimulation relation} if it forms an \(F\)-coalgebra: \[σ_{∼}: {∼} → F(∼),\] 
And its projections \(π₁: S × U → S\) and \(π₂: S × U → U\) are both homomorphisms:
\[
    \begin{tikzcd}
        S \ar[swap]{d}{σ_S}
            & ∼ \ar[swap]{l}{π₁} \ar{r}{π₂} \ar{d}{σ_{∼}} 
            & U \ar{d}{σ_T}\\  
        F(S) & F(∼) \ar{l}{F(π₁)} \ar[swap]{r}{F(π₂)} & F(T)
    \end{tikzcd}
\]
In a special case, when there exists a homomorphism \(h: S → U\), then we can simply pick \({∼} ⊆ S × U\) to be \(\{(s, h(s)) ∣ s ∈ S\}\), which gives us a bisimulation with the lift \(σ_{∼} ≜ σ_S × σ_U\).

\begin{corollary}
    Given two \(F\)-coalgebra \(S, U\) and a homomorphism \(h: S → U\), then all for all \(s\), \(⟦s⟧_{S} = ⟦h(s)⟧_{U}.\)
\end{corollary}

Finally, we are interested in bisimulation equivalence.
Given a coalgebra \(S\), a bisimulation \({≃} ⊆ S × S\) is a bisimulation equivalence when \(≃\) is a bisimulation and also an equivalence relation.
As it turns out, searching for bisimulation is equivalent to bisimulation equivalence. 
This result is very important, as it allows us to leverage more efficient data structure, like union-find, to search for equivalence relation instead of relations in general.

\begin{theorem}\label{thm:bisim-iff-bisim-eq}
    For any coalgebra \(S\), there exists a bisimulation \({∼} ⊆ S × S\) s.t. \(s₁ ∼ s₂\) if and only if there exists a bisimulation equivalence \({≃} ⊆ S × S\) s.t. \(s₁ ≃ s₂\)
\end{theorem}

\begin{proof}
    First show the \(⟹\) direction, we consider the maximal bisimulation between \(S\) and itself \({≡} ⊆ S × S\), which is known to be a bisimulation equivalence~\cite[Corollary 5.6]{rutten_UniversalCoalgebraTheory_2000}.
    Because \(≡\) is maximal, therefore \({∼} ⊆ {≡}\) and \(s₁ ≡ s₂\).

    Then the \(⟸\) direction can be proven by setting \({∼} ≜ {≃}\).
\end{proof}

\subsection{GKAT and Its Coalgebra}

Guarded Kleene Algebra with Tests, or GKAT~\cite{smolka_GuardedKleeneAlgebra_2020}, is a deterministic fragment of Kleene Algebra with Tests. 
The syntax of GKAT over a set of primitive actions \(K\) and a set of primitive tests \(T\) can be defined in two sorts, boolean expressions \(\Bool\) and expressions \(\Exp\):
\begin{align*}
    a, b, c ∈ \Bool 
        & ≜ 1 ∣ 0 ∣ t ∈ T ∣ b ∧ c ∣ b ∨ c ∣ \overline{b} \\  
    e, f ∈ \Exp 
        & ≜ p ∈ K ∣ b ∈ \Bool ∣ e +_b f ∣ e ; f ∣ e^{(b)} 
\end{align*}
where \(e +_b f\) is the if statement with condition \(b\); \(e;f\) is the sequencing of expression \(e\) and \(f\); finally \(e^{(b)}\) is the while loop with body \(e\) and condition \(b\).
A GKAT expression can be unfolded into a KAT expression in the usual manner~\cite{kozen_KleeneAlgebraTests_1997c}:
\begin{align*}
    e +_b f & ≜ b; e + \overline{b}; f &
    e^{(b)} & ≜ (b; e)^*; \overline{b}.
\end{align*}
Then the semantics of each expression \(⟦e⟧\) can be computed by the semantics of Kleene Algebra with tests~\cite{kozen_KleeneAlgebraTests_1997c}.
An important construct in the semantics is \emph{atoms}, which are a string of all the primitive tests either in its positive or negative form: for \(B ≜ \{b₁, b₂, …, bₙ\}\)
\[\At_B ≜ \{b₁' ⋅ b₂' ⋅ ⋯ ⋅ bₙ' ∣ bᵢ' ∈ \{bᵢ, \overline{bᵢ}\}\}.\]
Follow the conventional notation, we denote an atom using \(α, β\); and we sometimes omit the subscript \(B\) when no confusing can arise.
Intuitively, atoms are truth assignments to each primitive tests, indicating which primitive tests is satisfied in the current program states.

For the sake of brevity, we omit the complete definition of GKAT and KAT semantics, we refer the reader to previous works~\cite{smolka_GuardedKleeneAlgebra_2020,schmid_GuardedKleeneAlgebra_2021,kozen_KleeneAlgebraTests_1997c}, which explains these semantics in detail.
Our work avoids directly interact with the semantics by leveraging results in the coalgebraic theory of GKAT, which we will recap below.

Formally, GKAT coalgebras over primitive actions \(K\) and primitive tests \(T\) are coalgebras over the following functor:
\[G(S) ≜ (\{\accept, \reject\} + S × K)^{\At_B}.\] 
Intuitively, given a state \(s ∈ S\) and an atom \(α ∈ \At\), \(δ(s, α)\) will deterministically execute one of the following: reject \(α\), denoted as \(δ(s, α) = \reject\); accept \(α\), denoted \(δ(s, α) = \accept\); or transition to a state \(s' ∈ S\) and execute action \(p ∈ K\), denoted as \(δ(s, α) = (s', p)\).

This deterministic behavior contrast that of Kleene coalgebra with tests~\cite{kozen_CoalgebraicTheoryKleene_2017}, where for each atom, the state can accept or  reject the atom (but not both), yet the state can also non-deterministically transition to multiple different state via the same atom, while executing different actions.
% TODO: I think this belongs in the intro
As we will see later, the deterministic behavior of GKAT coalgebra not only enables a more versatile symbolic algorithm than KCT~\cite{pous_SymbolicAlgorithmsLanguage_2015}, but also present challenges. 
Specifically, GKAT coalgebra requires normalization to compute finite trace equivalences~\cite{smolka_GuardedKleeneAlgebra_2020}, where we will remove all the state that cannot lead to acceptance, which are called ``dead states''. 

In previous works, these dead states are detected after all the necessary state and transition is computed.
This approach requires storing all the information of the coalgebra in memory, which do not allow on-the-fly computation, which can not only terminate whenever a counter-example is found, but can also erase past states from memory. 

However, to truly understand the on-the-fly algorithm, we will first need to define ``dead states'', and its role in defining the coalgebraic semantic of GKAT. 

\subsection{Liveness and Sub-GKAT coalgebras}

% liveness

Traditionally, live and dead states are defined by whether they can reach an accepting state~\cite{smolka_GuardedKleeneAlgebra_2020}. However, it is straightforward to show that principle sub-coalgebra \(⟨s⟩_S\) exactly correspondents to reachable states of \(s\) in coalgebra \(S\). 
Thus, the classical definition is equivalent to the following:
\begin{definition}[liveness of states]\label{def:liveness-of-states}
    A state \(s\) is \emph{accepting} if there exists a \(α ∈ \At\) s.t. \(δ(s, α) = \accept\). A state \(s'\) is \emph{live} if there exists an accepting state \(s' ∈ ⟨s⟩\). A state \(s'\) is \emph{dead} if there is no accepting state in \(⟨s⟩\).
\end{definition}
This alternative liveness definition can help us formally prove important theorems regarding reachability and liveness without performing induction on traces. We can show the following lemmas as examples:
\begin{lemma}\label{thm:dead-iff-all-reachable-dead}
    A state \(s\) is dead if and only if all elements in \(⟨s⟩\) is dead.
\end{lemma}
\begin{proof}
    \(⟸\) direction is true, because \(s ∈ ⟨s⟩\): if all \(⟨s⟩\) is dead, then \(s\) is dead. 
    \(⟹\) direction can be proven as follows.
    Take \(s' ∈ ⟨s⟩\), then \(⟨s'⟩ ⊑ ⟨s⟩\) by definition. 
    Since there is no accepting state in \(⟨s⟩\), thus there cannot be any accepting state in \(⟨s'⟩\), hence \(⟨s'⟩\) is also dead.
\end{proof}

\begin{theorem}[homomorphism perserves liveness]\label{thm:hom-preserve-liveness}
    Given a homomorphism \(h: S → U\) and a state \(s ∈ S\):
    \[\text{\(s\) is live} ⟺ \text{\(h(s)\) is live}\]
\end{theorem}

\begin{proof}
    Because homomorphic image preserves principle sub-GKAT coalgebra (\Cref{thm:homo-img-preserve-principle-sub-coalg})
    \[h(⟨s⟩_S) = ⟨h(s)⟩_U;\]
    therefore for any state \(s' ∈ S\):
    \[s' ∈ ⟨s⟩_S ⟺ h(s') ∈ h(⟨s⟩_S) ⟺ h(s') ∈ ⟨h(s)⟩_U.\]
    And because \(s'\) is accepting if and only if \(h(s')\) accepting by definition of homomorphism; then \(⟨s⟩_S\) contains an accepting state if and only if \(h(⟨s⟩_S) = ⟨h(s)⟩_U\) contains an accepting state. 
    Therefore, \(s\) is live in \(S\) if and only if \(h(s)\) is live in \(U\).
\end{proof}

The above theorem then leads to several interesting liveness preservation properties for important structures on coalgebras, like sub-coalgebra and bisimulation.

\begin{corollary}[sub-coalgebra perserves liveness]\label{thm:sub-coalg-preserve-liveness}
    For a sub-coalgebra \(S' ⊑ S\) and a state \(s ∈ S'\), \(s\) is live in \(S'\) if and only if \(s\) is live in \(S\).
\end{corollary}

\begin{proof}
    Let the homomorphism \(h\) in \cref{thm:hom-preserve-liveness} be the inclusion homomorphism \(i: S' → S\).
\end{proof}

\begin{corollary}[bisimulation preserves liveness]\label{thm:bisim-preserve-liveness}
    If there exists a bisimulation \(∼\) between GKAT coalgebra \(S\) and \(U\) s.t. \(s ∼ u\) for some states \(s ∈ S\) and \(u ∈ U\), then \(s\) and \(u\) has to be either both accepting, both live or both dead.
\end{corollary}

\begin{proof}
    Because for a \(∼\) is a bisimulation when both \(π₁: {∼} → S\) and \(π₂: {∼} → U\) are homomorphisms.
    Therefore, 
    \begin{align*}
        s \text{ is live in } S 
        & ⟺ π₁((s, u)) \text{ is live in } S \\        
        & ⟺ (s, u) \text{ is live in } {∼} \\  
        & ⟺ π₂((s, u)) \text{ is live in } U  \\
        & ⟺ u \text{ is live in } U. 
        \qedhere
    \end{align*}
\end{proof}


% trace semantics
\subsection{Normalization And Semantics}

(Possibly infinite) trace model \(𝒢_ω\) is the final coalgebra of GKAT coalgebras~\cite{schmid_GuardedKleeneAlgebra_2021}.
The finality of the model implies that every state in any GKAT coalgebra \(S\) can be assigned a semantics under the unique homomorphism \(⟦-⟧^{ω}_{S}: S → 𝒢_ω\); and such semantic equivalences can indeed be identified by bisimulation~\cite{schmid_GuardedKleeneAlgebra_2021}:
\(⟦s⟧^{ω}_{S} = ⟦t⟧^{ω}_{T}\) if and only if there exists a bisimulation \({∼} ⊆ S × T\), s.t. \(s ∼ t\).

The infinite trace equivalences can be directly computed with bisimulation on derivative, which supports on-the-fly algorithm as demonstrated by similar systems~\cite{kozen_CoalgebraicTheoryKleene_2017,almeida_DecidingKATHoare_2012,pous_SymbolicAlgorithmsLanguage_2015}. 
However, the \emph{finite} trace model \(𝒢\) is the final coalgebra of GKAT coalgebras without dead states, which we call \emph{normal GKAT coalgebra}~\cite{smolka_GuardedKleeneAlgebra_2020}. 
Fortunately every GKAT coalgebra can be normalized by rerouting all the transition from dead states to rejection.
Concretely, given a GKAT coalgebra \(S ≜ (S, δ_S)\), we let \(δ_{\norm(S)} : S → G(S)\) is defined as \(δ_{\norm(S)}(s, α) ≜ \reject\) when \(δ_S(s, α) = (s', p)\) and \(s'\) is dead in \(S\); and \(δ_{\norm(S)}(s, α) ≜ δ_S(s, α)\) otherwise; then \((S, δ_{\norm(S)})\) is the \emph{normalized} coalgebra of \(S ≜ (S, δ_S)\) denoted as \(\norm(S)\).

The finality of \(𝒢\) means that the finite trace semantics \(⟦-⟧\) is the unique coalgebra homomorphism \(\norm(S) → 𝒢\). 
Furthermore, the finite trace equivalence between \(s ∈ S\) and \(u ∈ U\) can be computed by first normalizing \(S\) and \(U\), then decide whether there is a bisimulation on \(\norm(S)\) and \(\norm(U)\) that includes \((s, t)\).
For a more detailed explanation on the finite trace semantics, we refer the reader to the work of \Citeauthor{smolka_GuardedKleeneAlgebra_2020}~\cite{smolka_GuardedKleeneAlgebra_2020}.

Besides giving us the finite-trace semantics, the normalization operation also connects the finite and infinite trace semantics, because it is an endofunctor on the category of GKAT coalgebra.

\begin{theorem}\label{thm:norm-functor}
    \(\norm\) is an endofunctor in the category GKAT coalgebra.
    More specifically, if \(h: S → U\) is a GKAT homomorphism, then \(h: \norm(S) → \norm(U)\) is also a homomorphism.
\end{theorem}

\begin{proof}
    Recall that \(h\) is a homomorphism if and only if for all \(s ∈ S\) and \(α ∈ \At\):
    \begin{itemize}[nosep]
        \item for a result \(r ∈ \{\reject, \accept\}\), 
        \[δ_S(s, α) = r ⟺ δ_U(h(s), α) = r;\]
        \item for any \(s' ∈ S\) and \(p ∈ K\), 
        \[δ_S(s, α) = (s', p) ⟺ δ_{U}(h(s), α) = (h(s'), p).\]
    \end{itemize}

    Then we show that \(h: \norm(S) → \norm(U)\) is a homomorphism, this is a consequence of homomorphism preserves liveness (\Cref{thm:hom-preserve-liveness}): for all \(s ∈ \norm(S)\) and \(α ∈ \At\):
    \begin{align*}
        & δ_{\norm(S)}(s, α) = \accept \\*
        ⟺{}& δ_S(s, α) = \accept \\*  
        ⟺{}& δ_U(h(s), α) = \accept \\*  
        ⟺{}& δ_{\norm(U)}(h(s), α) = \accept;\\
        & δ_{\norm(S)}(s, α) = \reject \\*  
        ⟺{}& δ_{S}(s, α) = \reject 
        \text{ or } δ_{S}(s, α) = (s', p), s' \text{ is dead} \\*
        ⟺{}& δ_{U}(h(s), α) = \reject \\*
        & \text{ or } δ_{U}(h(s), α) = (h(s'), p), h(s') \text{ is dead} \\*  
        ⟺{}& δ_{\norm(U)}(h(s), α) = \reject;\\
        & δ_{\norm(S)}(s, α) = (s', p) \\*
        ⟺{}& δ_{S}(s, α) = (s', p), s' \text{ is live} \\*  
        ⟺{}& δ_{U}(h(s), α) = (h(s'), p), h(s') \text{ is live}\\* 
        ⟺{}& δ_{\norm(U)}(h(s), α) = (h(s'), p).
        \qedhere
    \end{align*}
\end{proof}

\begin{corollary}\label{thm:norm-sub-coalg}
    Normalization preserves sub-coalgebra, i.e. if \(S' ⊑ S\) then \(\norm(S') ⊑ \norm(S)\).
\end{corollary}

\begin{proof}
    By letting the homomorphism in~\Cref{thm:norm-functor} to be the inclusion homomorphism \(i: S' → S\)
\end{proof}

Because of the functoriality, we can show that two states are infinite-trace equivalent implies these two states are finite-trance equivalent.
This gives us more tool in proving semantic equivalence between two states in GKAT coalgebras: proving bisimulation in the GKAT coalgebra is already enough to obtain the semantic equivalence for two states.

\begin{corollary}\label{thm:inf-trace-equiv-implies-fin-trace-equiv}
    Given two states in two GKAT coalgebra \(s ∈ S, u ∈ U\), \(⟦s⟧^{ω}_{S} = ⟦u⟧^{ω}_{U} ⟹ ⟦s⟧_{S} = ⟦u⟧_{U}\).
\end{corollary}

\begin{proof}
    Because \(⟦s⟧^{ω}_{S} = ⟦u⟧^{ω}_{U}\), there exists a bisimulation \({∼} ⊆ S × U\) s.t. \(s ∼ u\)~\cite{schmid_GuardedKleeneAlgebra_2021}.
    Therefore, we have the following span in the category of GKAT coalgebra:
    \[\begin{tikzcd}
        S & ∼ \ar{r}{π₂} \ar[swap]{l}{π₁} & U
    \end{tikzcd}\]
    Then by~\Cref{thm:norm-functor}, \(\norm(∼)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\):
    \[\begin{tikzcd}
        \norm(S) 
        & \norm(∼) \ar{r}{π₂} \ar[swap]{l}{π₁} 
        & \norm(U)
    \end{tikzcd}\]
    And because normalization operation preserves states in \(∼\), therefore \((s, u) ∈ \norm(∼)\), and because \(\norm(∼)\) is a bisimulation between the normalization of \(S\) and \(U\), therefore \(⟦s⟧_{S} = ⟦u⟧_{U}\)
\end{proof}

\section{On-The-Fly Bisimulation}

The original algorithm for deciding GKAT equivalences~\cite{smolka_GuardedKleeneAlgebra_2020} requires the entire automaton to be known prior to the execution of the bisimulation algorithm; specifically, in order to compute the liveness of a state \(s\), it is necessary iterate through all its reachable states \(⟨s⟩\) to see if there are any accepting states within.
This limitation poses challenges to design an efficient on-the-fly algorithm for GKAT.
In order to make the decision procedure scalable, we will need to merge the normalization and bisimulation procedure, so that our algorithm can normalized the automaton only when we need to.

In this section, we introduce an algorithm that merges bisimulation and normalization where we only need to test the liveness of the state when a disparity in the bisimulation has been found.
For example, when one automaton leads to reject where the other transition to a state, then we will need to verify whether that state is dead or not.

This on-the-fly algorithm inherits the efficiency of the original algorithm~\cite{smolka_GuardedKleeneAlgebra_2020}, where the worst case will require two passes of the automaton, where one pass will try to establish a bisimulation, when failed the other pass will kick in and compute whether the failed states are dead.
In some special case, the on-the-fly algorithm can even out perform the original algorithm; for example, when the two input automata are bisimular (even when they are not normal), the on-the-fly algorithm can skip the liveness checking, only performing the bisimulation.

TODO: I think we should move the next couple theorem to the background.

\begin{theorem}[sub-coalgebras perserve and reflect bisimulation]\label{thm:sub-coalg-preserve-bisim}
    Given any sub-coalgebra \(S' ⊑ S\) and \(T' ⊑ T\),
    \begin{itemize}
        \item Given a bisimulation \(∼\) between \(S'\) and \(T'\), then \(∼\) is also a bisimulation between \(S\) and \(T\);
        \item if there exists a bisimulation \(∼\) between \(S\) and \(T\), then the restriction 
        \begin{mathpar}
            ∼_{S', T'} ≜ \{(s, t) ∣ s ∈ S', t ∈ T', s ∼ t\}
        \end{mathpar}
        forms a bisimulation between \(S'\) and \(T'\).
    \end{itemize}
\end{theorem}

\begin{proof}
    To prove that bisimulation \(∼\) between \(S'\) and \(T'\) is also a bisimulation of \(S\) and \(T\), we can simply enlarge the diagram by the inclusion homomorphism
    \[
        \begin{tikzcd}
            S \ar{d}{δ_S} & S' \ar[hook',swap]{l}{i} \ar{d}{δ_S}
            & ∼ \ar[swap]{l}{π₁} \ar{r}{π₂} \ar{d}{δ_∼}
            & T' \ar[hook]{r}{i} \ar{d}{δ_S} & T \ar{d}{δ_T}\\  
            G(S) & G(S') \ar[hook',swap]{l}{G(i)} 
            & G(∼) \ar[swap]{l}{G(π₁)} \ar{r}{G(π₂)} & T' \ar[hook]{r}{G(i)} & T \\  
        \end{tikzcd}
    \]
    Because the inclusion homomorphism \(i\) doesn't change the input thus, we have:
    \begin{mathpar}
        {∼} \xrightarrow{π₁} S' \xrightarrow{i} S = {∼} \xrightarrow{π₁} S \and 
        {∼} \xrightarrow{π₂} T' \xrightarrow{i} T = {∼} \xrightarrow{π₂} T
    \end{mathpar}

    To prove that the bisimulation can be restricted, we first realize that \(∼_{S', T'}\) is a pre-image of the maximal bisimulation \(≣_{S', T'}\) along the inclusion homomorphism \(i: {∼} → {≡_{S, T}}\).
    This means that \(∼_{S', T'}\) can be formed by a pullback square:
    \[
        \begin{tikzcd}
            ∼_{S', T'} \ar{r}{i} \ar[swap]{d}{i} \ar[phantom, very near start]{dr}{\scalebox{1.5}{\(\lrcorner\)}} & ≣_{S', T'} \ar{d}{i}\\ 
            {∼} \ar[swap]{r}{i} & {≡_{S, T}}
        \end{tikzcd}
    \]
    Recall that elementary polynomial functor~\cite{jacobs_IntroductionCoalgebraMathematics_2016} like \(G\) preserves pullback, hence the pullback also uniquely generates a GKAT coalgebra~\cite{rutten_UniversalCoalgebraTheory_2000}
\end{proof}

\Cref{thm:sub-coalg-preserve-bisim} allows us to only search for bisimulation on a sub-coalgebra, speeding up our search algorithm.
Another way to speed up the algorithm is to use efficient data structures to find a bisimulation equivalence instead of a bisimulation relation.
This optimization is a special case of the \emph{up-to technique}~\cite{bonchi_GeneralAccountCoinduction_2017a}.
Specifically, we will extend~\Cref{thm:bisim-iff-bisim-eq} to a setting where the bisimulation is no longer over the same coalgebra.

\begin{theorem}\label{thm:GKAT-bisim-iff-bisim-eq}
    Given two states in two coalgebras \(s ∈ S, u ∈ U\), let \(S + U\) be the coproduct coalgebra of \(S\) and \(U\)~\cite{rutten_UniversalCoalgebraTheory_2000}. 
    There exists a bisimulation \({∼} ⊆ S × U\) s.t. \(s ∼ u\) if and only if there exists a bisimulation equivalence on \({≃} ⊆ (S + U) × (S + U)\), s.t. \(s ≃ u\).
\end{theorem}

\begin{proof}
    Notice that both \(S\) and \(U\) are sub-coalgebra of \(S + U\) witnessed by the canonical injection \(\mathrm{inj}ₗ: S → S + U\) and \(\mathrm{inj}ᵣ: U → S + U\).

    Let \(∼\) be a bisimulation and \(≃\) be a bisimulation equivalence, by~\Cref{thm:sub-coalg-preserve-bisim} and~\Cref{thm:bisim-iff-bisim-eq}:
    \begin{align*}
        & ∃~ {∼} ⊆ S × U, s ∼ u \\
        ⟺{}& ∃~ {∼} ⊆ (S+U) × (S+U), s ∼ u \\  
        ⟺{}& ∃~ {≃} ⊆ (S+U) × (S+U), s ≃ u.
        \qedhere
    \end{align*}
\end{proof}

After we justify the above optimizations of the algorithm, we will show the core theorem that establishes the correctness of our algorithm.

\begin{lemma}[bisimulation between dead states]\label{thm:bisim-between-dead}
    Given two dead states \(s ∈ S\) and \(u ∈ U\), then the singleton bisimulation \({∼} ⊆ \norm(S) × \norm(U)\):
    \begin{mathpar}
        {∼} ≜ \{(s,u)\} \and 
        δ_{∼}((s,u), α) ≜ \reject
    \end{mathpar}
    is a bisimulation between \(\norm(S)\) and \(\norm(U)\).
\end{lemma}

\begin{proof}
    By computation
\end{proof}

\begin{theorem}[Recursive Construction]\label{thm:recursive-construction}
    Given two GKAT coalgebra \(S\) and \(U\), and two of their elements \(s ∈ S\) and \(u ∈ U\),
    there exists a bisimulation \({∼} ⊆ \norm(⟨s⟩) × \norm(⟨u⟩)\) s.t. \(s ∼ u\), if and only if all the following holds:
    \begin{enumerate}
        \item\label{itm:acc-condition} for all \(α ∈ \At\), \(δ_{S}(s, α) = \accept ⟺ δ_{U}(u, α) = \accept\);
        \item\label{itm:rej-or-dead} \(s\) reject \(α\) or transition to a dead state via \(α\) if and only if \(u\) rejects \(α\) or transition to a dead state via \(α\);  
        \item\label{itm:transition-bisim} If \(δ_{S}(s, α) = (s', p)\) and \(δ_{U}(u, α) = (u', p)\), then there exists a bisimulation \({∼'} ⊆ \norm(⟨s'⟩) × \norm(⟨u'⟩)\), s.t. \(s' ∼' u'\);
        \item\label{itm:transition-dead} If \(δ_{S}(s, α) = (s', p)\) and \(δ_{U}(u, α) = (u', q)\), s.t. \(p ≠ q\), then both \(s'\) and \(t'\) are dead.
    \end{enumerate}
\end{theorem}

\begin{proof}
    We first prove \(⟹\) direction, recall that there exists a bisimulation \({∼} ⊆ \norm(⟨s⟩) × \norm(⟨u⟩)\) if and only if for all \(s₁ ∼ u₁\):
    \begin{itemize}
        \item for all results \(r ∈ \{\accept, \reject\}\): \(δ_{\norm(S)}(s₁, α) = r ⟺ δ_{\norm(U)}(u₂, α) = r\);
        \item otherwise, let \((s₂, p) ≜ δ_{\norm(S)}(s₁, α)\) and \((u₂, q) ≜ δ_{\norm(u)}(u₁, α)\), then \(p = q\) and \(s₂ ∼ u₂\)
    \end{itemize}

    The condition \labelcref{itm:acc-condition} holds:
    \begin{align*}
        δ_S(s, α) = \accept 
        ⟺{}& δ_{\norm(S)}(s, α) = \accept \\
        ⟺{}& δ_∼((s,u), α) = \accept \\ 
        ⟺{}& δ_{\norm(U)}(u, α) = \accept \\
        ⟺{}& δ_U(u, α) = \accept
    \end{align*}

    The condition~\labelcref{itm:rej-or-dead} holds: 
    \begin{align*}
        & δ_S(s, α) \text{ rejects or transition to dead states} \\
        ⟺{}& δ_{\norm(S)}(s, α) = \reject \\
        ⟺{}& δ_{\norm(U)}(u, α) = \reject \\
        ⟺{}& δ_U(u, α) \text{ rejects or transition to dead states}.
    \end{align*}

    The condition~\labelcref{itm:transition-bisim} holds, by case analysis on the liveness of \(s'\) and \(u'\).
    First note that \(s'\) and \(u'\) has to be both live or both dead: because \(δ_S(s, α) = (s', p)\), then \(\norm(δ_S)(s', α)\) can either be rejection or \((s',p)\), and so is \(\norm(δ_U)(u', α)\):
    \begin{align*}
        s' \text{ is live} 
        & ⟺ δ_{\norm(S)}(s, α) = (s', p) \\
        & ⟺ δ_{\norm(U)}(u, α) = (u', p) \\
        & ⟺ u' \text{ is live}.
    \end{align*}
    \begin{itemize}
        \item If both \(s'\) and \(u'\) are live, then \(s' ∼ u'\). By~\cref{thm:sub-coalg-preserve-bisim}, the bisimulation \(∼'\) is just \(∼\) restricted to \(⟨s'⟩\) and \(⟨u'⟩\).
        \item If both \(s'\) and \(u'\) are dead, then \(∼'\) can just be the singleton relation, according to~\cref{thm:bisim-between-dead}.
    \end{itemize}

    The condition~\labelcref{itm:transition-dead} holds: by the proof of condition~\labelcref{itm:transition-bisim}, \(s'\) and \(u'\) has to be either both live or both dead; if they are both live, then there cannot be a element in \(G(∼)\) that can project to \((s', p)\) under \(π₁\) but projects to \((t', q)\) under \(π₂\). Thus both \(s'\) and \(t'\) has to be dead.

    We then show the \(⟸\) direction, for arbitrary \(s' ∈ S\) and \(u' ∈ U\), we use \(≡_{s', u'}\) to denote the maximal bisimulation between \(\norm(⟨s'⟩)\) and \(\norm(⟨u'⟩)\).
    \begin{align*}
        {∼'} ≜ ⋃ \{≡_{s', u'} & ∣ ∃ α ∈ \At, p ∈ K, \\*
            & δ_{\norm(S)}(s, α) = (s', p) \\*
            & \text{ and } δ_{\norm(U)}(u, α) = (u', p)\}.
    \end{align*}
    For all the \(s'\) and \(u'\) in the above definition, \(⟨s'⟩ ⊑ ⟨s⟩\) and \(⟨u'⟩ ⊑ ⟨u⟩\), therefore by~\Cref{thm:norm-sub-coalg}, \(\norm(⟨s'⟩) ⊑ \norm(⟨s⟩)\) and \(\norm(⟨u'⟩) ⊑ \norm(⟨u⟩)\). 
    By~\Cref{thm:sub-coalg-preserve-bisim}, every \(≡_{s', u'}\) is a bisimulation between \(\norm(⟨s⟩)\) and \(\norm(⟨t⟩)\), and because bisimulation is closed under arbitrary union~\cite{rutten_UniversalCoalgebraTheory_2000}, \(∼'\) is a bisimulation between \(\norm(⟨s⟩)\) and \(\norm(⟨t⟩)\).

    To obtain the desired bisimulation \({∼}\) between \(\norm(⟨s⟩)\) and \(\norm(⟨u⟩)\), we add the pair \((s, t)\) to \(∼'\), 
    \[{∼} ≜ {∼'} ∪ \{(s, u)\},\] 
    with the following transition \(δ_{∼}\): for all \(α ∈ \At\),
    \begin{itemize}[nosep]
        \item if \(δ_{\norm(S)}(s, α) = δ_{\norm(U)}(u, α) = \accept\), then \(δ_{∼}((s, u), α) ≜ \accept\);
        \item if \(δ_{\norm(S)}(s, α) = δ_{\norm(U)}(u, α) = \reject\), then \(δ_{∼}((s, u), α) ≜ \reject\);
        \item if \(δ_{\norm(S)}(s, α) = (s', p)\) and \(δ_{\norm(U)}(u, α) = (u', p)\), then \(δ_{∼}((s, u), α) = ((s', u'), p)\);
        \item for all \((s', u') ∈ {∼}'\) that is not equal to \((s, u)\), we let \(δ_∼\) inherits the transition of \(δ_{∼'}\), i.e. \(δ_{∼}((s', u'), α) = δ_{∼'}((s', u'), α)\)
    \end{itemize}
    \emph{if \(δ_{∼}\) is well-defined}, then we can verify that \(∼\) is indeed a bisimulation between \(\norm(⟨s⟩)\) and \(\norm(⟨u⟩)\) where \(s ∼ u\). 
    We show the slightly more complicated case: \(δ_{\norm(S)}(s, α) = (s', p)\) and \(δ_{\norm(U)}(u, α) = (u', p)\) implies \(s' ∼ u'\) as an example. 
    By condition~\labelcref{itm:transition-bisim}, there exists a bisimulation \(∼_{s', u'} ⊆ \norm(⟨s'⟩) × \norm(⟨u'⟩)\), and because \(≡_{s', u'}\) is the maximal bisimulation between \(\norm(⟨s'⟩)\) and \(\norm(⟨u'⟩)\),
    \[(s', u') ∈ {∼_{s', u'}} ⊆ {≡_{s', u'}} ⊆ {∼}.\]

    Finally, we demonstrate that \(δ_∼\) is well-defined by leveraging the conditions in~\Cref{thm:recursive-construction}. 
    Specifically, we will show that the definition of \(δ_∼\) covers all the possible cases, by case analysis on the result of \(δ_S\): for all \(α ∈ \At\),
    \begin{itemize}
        \item If \(δ_S(s, α) = \accept\), then by condition~\labelcref{itm:acc-condition}, \(δ_U(u, α) = \accept\); therefore \[(δ_{\norm(S)})(s, α) = \norm(δ_{\norm(U)})(u, α) = \accept.\]
        \item If \(δ_S(s, α)\) transitions to a dead state or reject, then by condition~\labelcref{itm:rej-or-dead} \(δ_U(u, α)\) will also transition to a dead state or reject, then \[δ_{\norm(S)}(s, α) = δ_{\norm(U)}(u, α) = \reject.\]
        \item If \(δ_S(s, α) = (s', p)\) and \(s'\) is live, then \(δ_U(u, α) = (u', p)\) necessarily holds, otherwise it would violate one of conditions~\labelcref{itm:acc-condition,itm:rej-or-dead,itm:transition-dead}. 

        By condition~\labelcref{itm:transition-bisim}, there exists a bisimulation \(∼_{s', u'}\) between \(\norm(⟨s'⟩)\) and \(\norm(⟨u'⟩)\) s.t. \(s' ∼_{s', u'} u'\). Because bisimulation preserves liveness (\Cref{thm:bisim-preserve-liveness}), \(s', u'\) has to be both dead or live. 
        Finally, because \(s'\) is live, therefore \(u'\) is also live, and we obtain the final case in the definition of \(δ_{∼}\): \(δ_{\norm(S)}(s, α) = (s', p)\) and \(δ_{\norm(U)}(u, α) = (u', p)\).
        \qedhere
    \end{itemize}
\end{proof}

\begin{corollary}[Correctness]\label{thm:recursive-construction-correctness}
    For any two states in two GKAT coalgebra \(s ∈ S, u ∈ U\), \(s\) and \(u\) are finite-trace equivalent if and only if there exists a \emph{bisimulation equivalence} \(≃\) in \(\norm(⟨s⟩) + \norm(⟨u⟩)\), which is equivalent to all the following conditions hold:
    \begin{enumerate}
        \item for all \(α ∈ \At\), \(δ_{S}(s, α) = \accept ⟺ δ_{U}(u, α) = \accept\);
        \item \(s\) reject \(α\) or transition to a dead state via \(α\) if and only if \(u\) rejects \(α\) or transition to a dead state via \(α\);  
        \item if \(δ_{S}(s, α) = (s', p)\) and \(δ_{U}(u, α) = (u', p)\), then there exists a bisimulation equivalence \({≃'}\) in \(\norm(⟨s'⟩) + \norm(⟨u'⟩)\), s.t. \(s' ≃' u'\);
        \item if \(δ_{S}(s, α) = (s', p)\) and \(δ_{U}(u, α) = (u', q)\), s.t. \(p ≠ q\), then both \(s'\) and \(t'\) are dead.
    \end{enumerate}
    Notice that above condition is similar to those in~\cref{thm:recursive-construction}, except bisimulation is replaced with bisimulation \emph{equivalence}.
\end{corollary}

\begin{proof}
    By the standard argument with normalization preserves sub-coalgebra (\Cref{thm:norm-sub-coalg}), we can obtain 
    \(\norm(⟨s⟩) ⊑ \norm(S)\) and \(\norm(⟨u⟩) ⊑ \norm(U)\).
    Because sub-coalgebra reflects and preserves bisimulation (\Cref{thm:sub-coalg-preserve-bisim}) and the correspondence between bisimulations and bisimulation equivalences in GKAT (\cref{thm:GKAT-bisim-iff-bisim-eq}): let \({∼_{s, u}} ⊆ \norm(⟨s⟩) × \norm(⟨u⟩)\) and \({∼} ⊆ \norm(S) × \norm(U)\) be bisimulations and \(≃\) be a bisimulation equivalence in \(\norm(⟨s⟩) + \norm(⟨u⟩)\):
    \begin{align*}
        & \text{Conditions in \cref{thm:recursive-construction} is satisfied} \\  
        ⟺{}& \text{Conditions in current theorem is satisfied}\\
        ⟺{}& ∃~ {≃}, s ≃ u 
        ⟺ ∃~ {∼_{s, u}}, s ∼ u \\
        ⟺{}& ∃~ {∼} , s ∼ u 
        ⟺ ⟦s⟧_{S} = ⟦u⟧_{U}. \qedhere
    \end{align*}
\end{proof}

The above theorem already gives us an algorithm to recursively decide whether \(⟦s⟧_{S} = ⟦u⟧_U\), when \(⟨s⟩_S\) and \(⟨u⟩_U\) is finite.
However, this algorithm can be further optimized in several ways: we will then derive that a dead state can never relate to live states. This means that when checking the bisimulation of states \(s\) and \(t\), if we already know one of them is dead, we only need to check whether the other is dead, instead of going through all the conditions in~\cref{thm:recursive-construction}.

\begin{theorem}\label{thm:bisim-one-dead}
    Given two states \(s ∈ S\) and \(t ∈ T\), if \(s\) is a dead state in \(S\), then there exists a bisimulation equivalence \(≃\) in \(S + T\) where \(s ≃ t\) if and only if \(t\) is dead. Similarly, for \(t ∈ T\).
\end{theorem}

\begin{proof}
    Notice because of the homomorphism \(\mathrm{inj_l}: S → S + T\) and homomorphism preserves liveness (\cref{thm:hom-preserve-liveness}), therefore \(s ∈ S\) is dead if and only if \(s ∈ S + T\) is dead; similarly for \(t ∈ T\).

    If there exists a bisimulation equivalence \(≃\), s.t. \(s ≃ t\), because \(s\) is dead and bisimulation preserves liveness~\cref{thm:bisim-preserve-liveness}, then \(t\) is dead. 

    And if both \(t\) and \(s\) is dead, then a bisimulation between \(S\) and \(T\) can be constructed in~\cref{thm:bisim-between-dead}, which induces a bisimulation equivalence via~\Cref{thm:GKAT-bisim-iff-bisim-eq}.
\end{proof}

Before we write down the optimized algorithm, we will first briefly sketch the dead state detection algorithm, which will use a DFS to check whether the there exists any accepting states in reachable states of \(s ∈ S\): if there exists any accepting state in \(⟨s⟩_S\), then the algorithm terminates immediately, and return that \(s\) is live, otherwise it would return all the states in \(⟨s⟩\), and by~\cref{thm:dead-iff-all-reachable-dead}, all the states in \(⟨s⟩\) is dead.

We will cache all the known dead states from all the previous searches, we use the call \Call{knownDead\(_S\)}{$s$} to check whether the state \(s\) is in the cached dead states. 
And \Call{isDead\(_S\)}{$s$} will first check if \(s\) is known to be dead, and invoke the depth-first-search algorithm in the coalgebra \(s\), if \(s\) is not in the cached dead states.
Because the non-symbolic version of dead state checking is similar to the symbolic version, we only provide the symbolic version of this algorithm in appendix
TODO: give the link to the section.


\begin{algorithm}
    \caption{On-the-fly bisimulation algorithm}\label{alg:bisim}
    \begin{algorithmic}
        \Function{equiv}{$s ∈ S, u ∈ U$}
        \If {\Call{eq}{$s, u$}} {\Return true} \EndIf
        \If {\Call{knownDead\(_S\)}{s}} 
            \State\Return \Call{isDead\(_U\)}{$u$}
        \EndIf
        \If {\Call{knownDead\(_U\)}{u}} 
            \State\Return \Call{isDead\(_S\)}{$s$}
        \EndIf
        \For{\(α ∈ \At\)}{}{
            \Match{$δ_{S}(s, α), δ_{U}(u, α)$}
            \Case{\(\accept, \accept\)} {\Continue} \EndCase
            \Case{\(\reject, \reject\)} {\Continue} \EndCase
            \Case{\(\reject, (u', q)\)} 
                \State\Return \Call{isDead\(_U\)}{$u'$} 
            \EndCase
            \Case{\((s', p), \reject\)}
                \State\Return\Call{isDead\(_S\)}{$s'$}
            \EndCase
            \Case{\((s', p), (u', q)\)} {
                \If {\(p = q\)}
                    \State\Call{union}{$s, u$}
                    \State\Return \Call{equiv}{$s, t$} 
                \EndIf
                \If{\Call{isDead\(_S\)}{$s$} and \Call{isDead\(_U\)}{$u$}}
                    \State\Continue
                \EndIf
                \State\Return false
            } \EndCase
            \Default { \Return false } \EndDefault
            \EndMatch
        }\EndFor
        \State\Return true
        \EndFunction
    \end{algorithmic}
\end{algorithm}

TODO: I am not sure if we need to give some intuition for this algorithm here.

Finally, we present our finite-trace equivalent checking algorithm as~\cref{alg:bisim}, where we search for a bisimulation equivalence \({≃}\) in \(\norm(⟨s⟩) + \norm(⟨u⟩)\), s.t. \(s ≃ u\), using the condition in~\Cref{thm:recursive-construction-correctness}.
Seeking an equivalence relation allows us to organize our explored state pairs into equivalence classes, as opposed to a set of pairs, and enables us to use union-find data structures to avoid looking for two states that are already in the same equivalence class.
Specifically, we use the call \(\Call{union}{$s, u$}\) to union the equivalence class of \(s\) and \(u\), and use \(\Call{eq}{$s, u$}\) to check whether \(s\) and \(u\) is in the same equivalence class. 
In this algorithm, we first check if one of \(s\) and \(u\) is known to be dead, if so we only need to check whether the other is dead, because of~\Cref{thm:bisim-one-dead}.
Otherwise, we will check the conditions in~\Cref{thm:recursive-construction-correctness}, which is a minor modification of the conditions in~\cref{thm:recursive-construction}.


% \section{The Algorithm}

% In this section we will present the pseudo-code for our on-the-fly algorithm. 
% In order to implement the the inductive construction theorem (\cref{thm:recursive-construction}), we will need to determine the liveness of the state. This can be simply computed via a DFS from the state being checked. 

% TODO: we should merge the two so that it is easier to 
% \begin{algorithm}
%     \caption{Check whether a state \(s\) is dead}\label{alg:check-dead-main}
%     \begin{algorithmic}
%         \Function{isDeadLoop}{$s ∈ S$, explored}
%         \If {\(s ∈\) explored} {\Return explored} 
%         \Else { 
%             \For{\(α ∈ \At\)}{}{
%                 \Match{\(δ_{S}(s, α)\)}
%                 \Case{\(\accept\)} {\Return none} \Comment{\(s\) transition to accept}
%                 \EndCase
%                 \Case{\(\reject\)} {\texttt{continue}}
%                 \Comment{skip if \(s\) transition to reject}
%                 \EndCase
%                 \Case{($s', p$)}{ 
%                     \If{\Call{IsDeadLoop}{$s'$} = none} {\Return none} \Comment{\(s\) transitions to a live state \(s'\)}
%                     \Else { explored \(←\) (explored \(∪\) \Call{isDeadLoop}{$s'$, explored}) } \EndIf
%                 } \EndCase
%                 \EndMatch
%             }\EndFor}
%         \EndIf
%         \State {\Return explored}    
%         \EndFunction
%     \end{algorithmic}
% \end{algorithm}

% By~\cref{thm:dead-iff-all-reachable-dead}, if \(s\) is dead then all the reachable states of \(s\) (denoted by \(⟨s⟩\)). Then by returning all the reachable states of \(s\), we can cache these states to avoid checking them again. To encapsulate the caching, we have the following function, which we will actually use in our bisimulation algorithm.

% \begin{algorithm}
%     \caption{A cached algorithm to check whether a state is dead}\label{alg:is-dead}
%     \begin{algorithmic}
%         \State{deadStates \(← ∅\)}

%         \Function{isDead}{$s ∈ S$}
%         \If {\(s ∈\) deadStates} {\Return true} 
%         \ElsIf {\Call{isDeadLoop}{$s, ∅$} = none} {\Return false}
%         \Else 
%             \State {deadStates \(←\) (deadStates \(∪\) \Call{isDeadLoop}{$s, ∅$})}
%             \State {\Return {true}}
%         \EndIf
%         \EndFunction
%     \end{algorithmic}
% \end{algorithm}

% Given the direct correspondence between bisimulation and bisimulation equivalence and bisimulation in sub-algebra:
% \begin{align*}
%     & ∃ \text{ bisimulation } {∼} ⊆ ⟨s⟩ × ⟨t⟩ \text{ s.t. } s ∼ t \\
%     & ⟺ ∃ \text{ bisimulation } {∼} ⊆ (⟨s⟩∪⟨t⟩) × (⟨s⟩∪⟨t⟩) \text{ s.t. } s ∼ t 
%         & \text{\cref{thm:sub-coalg-preserve-bisim}}\\  
%     & ⟺ ∃ \text{ bisimulation equivalence } {≃} ⊆ (⟨s⟩∪⟨t⟩) × (⟨s⟩∪⟨t⟩) \text{ s.t. } s ≃ t 
%         & \text{\cref{thm:bisim-iff-bisim-equiv}}
% \end{align*}
% we can safely replace the bisimulation in inductive construction (\cref{thm:recursive-construction}) with bisimulation equivalence. 
% Dealing with equivalence relations allows us to leverage efficient data structures like union find in our bisimulation algorithm. 

% We will use \(\Call{union}{$s,t$}\) to denote the operation to equate \(s\) and \(t\) in a union-find, and use \(\Call{eq}{$s,t$}\) to check if \(s\) and \(t\) belongs to the same equivalence class, i.e. share the same representative.
% Specifically, we will use the union-find structures to keep track of the equivalence classes that we are in the process of checking, hence avoiding repeatedly checking the same pair of states to remove infinite loops.

% Our on-the-fly bisimulation algorithm will decide whether there exists a bisimulation relation in \(⟨s⟩ ∪ ⟨t⟩\) s.t. \(s ∼ t\). This algorithm generally reproduce the setting of inductive construction theorem~\cref{thm:recursive-construction};
% except by~\cref{thm:bisim-one-dead}, in the special case where \(s\) or \(t\) is dead, then we will only need to check whether the other is dead.

% Because the dead state detection algorithm is coalgebra-specific, we use a subscript on ``deadStates'' and ``\textsc{IsDead}'' to indicate the coalgebra. 
% The soundness and completeness of~\cref{alg:bisim} can be observed by the fact that \emph{when the algorithm terminate}, the algorithm returns true if and only if there exists a bisimulation between \(⟨s⟩\) and \(⟨t⟩\) s.t. \(s ∼ t\), which is then logically equivalent to trace equivalence.
% Such equivalence is a direct consequence of~\cref{thm:bisim-one-dead,thm:recursive-construction}.

% \begin{remark}
%     The caching of dead state and the shortcut to check whether \(s\) is dead when \(t\) is dead and vise versa, is not essential to the soundness and completeness of algorithm, they are here to trade speed with memory. 
%     In a memory-constraint situation, the ``\textnormal{deadStates}'' variable can be cleared periodically to save memory.
% \end{remark}

\section{Symbolic Algorithm}

Given the alphabet \(K, B\), a \emph{symbolic GKAT coalgebra} \(Ŝ ≜ ⟨S, ϵ̂, δ̂⟩\) consists of a state set \(S\) and a accepting function \(ϵ̂\) and a transition function \(δ̂\):
\begin{mathpar}
    ϵ̂: S → 𝒫(\Bool_B), \and
    δ̂: S → 𝒫(\Bool_B × S × K),
\end{mathpar}
where \(\Bool_B\) is the free boolean algebra over \(B\) (boolean expressions modulo boolean algebra axioms); for all states \(s ∈ S\), all the booleans are ``disjoint''; namely the conjunction of any two expression from the set \(\{ϵ̂(s)\} ∪ \{b ∣ ∃ (b, s', p) ∈ δ(s)\}\) are false. 
We will then use \(ρ̂(s): \Bool_B\) to denote the boolean expressions that contain all the atoms that the state \(s\) rejects, and \(ρ̂(s)\) can be computed as follows:
\[ρ̂(s) ≜ ¬ ϵ̂(s) ∨ ¬ \left( ⋁_{(b, s', p) ∈ δ(s)} b \right)\]

Instead of modeling each atom individually in the automata, we group them into boolean expressions, this leads to a much more space efficient automata, and enables efficient bisimulation algorithms using off-the-shelf SAT solvers.

With the above intuition in mind, a symbolic GKAT coalgebra \(Ŝ ≜ ⟨S, ϵ̂, δ̂⟩\) can be lowered into a GKAT coalgebra \(⟨S, δ⟩\) in the following manner:
\begin{align}\label{cons:lowering}
δ(s, α) ≜ \begin{cases}
    \accept & ∃ b ∈ ϵ̂(s), α ≤ b \\  
    (s', p) & ∃ b ∈ \Bool_B, α ≤ b \text{ and } δ(s, b) = (s', p) \\  
    \reject & \text{otherwise}
\end{cases}
\end{align}
This is well-defined, i.e. no more than one clause can be satisfied precisely because the boolean expressions appear in \(ϵ̂\) and \(δ̂\) are disjoint.
The trace semantics of a GKAT coalgebra \(⟨S, ϵ̂, δ̂⟩\) is then defined as the trace semantics of its lowering \(⟨S, δ⟩\).

\begin{remark}[Canonicity]
    Notice that symbolic GKAT coalgebra is not canonical, i.e. there exists two different symbolic GKAT colagebra with the same lowering, consider the state set \(S ≜ \{*\}\):
    \[{δ̂}₁(*) ≜ \{b ↦ (*, p), ¬ b ↦ (*, p)\} \qquad 
    {δ̂}₂(*) ≜ \{⊤ ↦ (*, p)\},\] 
    and both \(ϵ̂\) will return constant \(⊥\).
    These two symbolic GKAT coalgebra obviously have the same lowering hence behavior, yet, they are different.
    There are other symbolic representation that will satisfy canonicity, yet we opt to use our current representation for ease of construction and computational efficiency.
\end{remark}

\begin{theorem}[Functoriality]
    The lowering operation is a functor, given a symbolic GKAT coalgebra homomorphism \(h: Ŝ → Û\), then \(h\) is also a homomorphism \(h: S → U\).
\end{theorem}

\begin{proof}
    
\end{proof}

We can then migrate the normalized bisimulation algorithm to the symbolic setting, we will first prove an inductive construction theorem like~\cref{thm:recursive-construction}.

\begin{theorem}[Symbolic Inductive Construction]\label{thm:symb-recursive-construction}
    Given two symbolic GKAT coalgebra \(Ŝ = ⟨S, ϵ̂_S, δ̂_S⟩\) and \(T̂ = ⟨T, ϵ̂_T, δ̂_T⟩\) and two states \(s ∈ S\) and \(t ∈ T\), there exists a normalized bisimulation on the lowered coalgebra \({∼} ⊆ S × T\) s.t. \(s ∼ t\) if and only if all the following holds:
    \begin{itemize}
        \item \(⋁ ϵ̂_S(s) ≡ ⋁ ϵ̂_T(t)\);
        \item for all \((b, s', p) ∈ δ̂_S(s)\) and \((c, t', q) ∈ δ̂_T(t)\), if \(b ∧ c ≢ 0\) and \(p = q\) then here exists a normalized bisimulation \({∼_{s',t'}} ⊆ S × T\) s.t. \(s' ∼_{s',t'} t'\);
        \item for all \((b, s', p) ∈ δ̂_S(s)\) and \((c, t', q) ∈ δ̂_T(t)\), if \(b ∧ c ≢ 0\) and \(p ≠ q\) then both \(s'\) and \(t'\) is dead;  
        \item for all \((b, s', p) ∈ δ̂_S(s)\) and \(c ∈ ρ̂_T(t)\), if \(b ∧ c ≢ 0\), then \(s'\) is dead;
        \item for all \(b ∈ ρ̂_S(s)\) and \((c, t', q) ∈ δ̂_T(t)\), if \(b ∧ c ≢ 0\), then \(t'\) is dead;
    \end{itemize}
\end{theorem}

\begin{proof}
    Reduces to~\cref{thm:recursive-construction} i.e. all the above condition holds if and only if all the condition in~\cref{thm:recursive-construction} holds in the lowered coalgebra.
\end{proof}

Then for the algorithm, we can just recursively check all the conditions in~\cref{thm:symb-recursive-construction}.

\begin{algorithm*}
    \caption{Symbolic On-the-fly bisimulation algorithm}\label{alg:symb-bisim}
    \begin{algorithmic}
        \Function{equiv}{$s ∈ S, t ∈ T$}
        \If {\Call{eq}{$s, t$}} {\Return true}
        \ElsIf {\(s ∈\) deadStates\(_S\)} {\Return \Call{isDead\(_T\)}{$t$}} 
        \ElsIf {\(t ∈\) deadStates\(_T\)} {\Return \Call{isDead\(_S\)}{$s$}} 
        \Else {}
        \Return {
            \Comment{conditions of ~\cref{thm:symb-recursive-construction}}
            \\\vspace{5px}
            \(\qquad
            \begin{aligned}
                & ⋁ ϵ̂_S(s) ≡ ⋁ ϵ̂_T(t) \mathrel{\&\!\&} \\  
                & \forall (b, s', p) ∈ δ̂_S(s), (c, t', q) ∈ δ̂_T(t), (b ∧ c) ≢ ⊥ ⟹ 
                \begin{cases}
                    \text{\Call{IsDead$_S$}{$s$}} ∧ \text{\Call{IsDead$_T$}{$t$}} & \text{if \(p ≠ q\)} \\
                    \text{\Call{Union}{$s$, $t$}}; \text{\Call{Equiv}{$s', t'$}} & \text{if \(p = q\)}
                \end{cases} \mathrel{\&\!\&}\\
                & \forall (b, s', p) ∈ δ̂_S(s), c ∈ ρ̂_T(t), (b ∧ c) ≢ ⊥ ⟹ \text{\Call{IsDead$_S$}{$s'$}} \mathrel{\&\!\&}\\
                & \forall b ∈ ρ̂_S(s), (c, t', q) ∈ δ̂_T(t), (b ∧ c) ≢ ⊥ ⟹ \text{\Call{IsDead$_T$}{$t'$}}
            \end{aligned}\)
        }
        \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm*}

Inspired by the syntax of Ocaml, the \(\mathrel{\&\!\&}\) is the logical-and operator on the language level, specifying that all four conditions in the return statements must be satisfied to return true. 
Notice just like the non-symbolic case, this algorithm can be modified to perform symbolic bisimulation of (non-normalized) GKAT automaton, which coincides with infinite trace equivalence~\cite{schmid_GuardedKleeneAlgebra_2021}, by letting \textsc{IsDead} always return false and keep deadStates empty.


\section{Construction of Symbolic GKAT Automata}

The final piece of the puzzle is to convert any given expression into an ``equivalent'' Symbolic GKAT Automata. This goal can be achieved by lifting existent constructions like derivatives and Thompson's construction~\cite{schmid_GuardedKleeneAlgebra_2021,smolka_GuardedKleeneAlgebra_2020}.
The correctness of these conversions is a consequence of  correctness of their non-symbolic counter-part, i.e. we will prove that the lowering as shown in~\labelcref{cons:lowering} of these constructions will yield the conventional derivative and Thompson's construction. 

\begin{figure*}
    \begin{mathpar}
        \inferrule[]{\\}
        {p \transvia{1 ∣ p}_{D̂} 1} \and  
        \inferrule[]{\\}
        {b \transAcc{D̂}{b}} \and  
        \inferrule[]
        {e \transvia{c ∣ p}_{D̂} e'}
        {e +_b f \transvia{b ∧ c ∣ p}_{D̂} e'} 
        \and
        \inferrule[]
        {e \transAcc{D̂}{c}}
        {e +_b f \transAcc{D̂}{b ∧ c}}
        \and
        \inferrule[]
        {f \transvia{c ∣ p}_{D̂} f'}
        {e +_b f \transvia{\overline{b} ∧ c ∣ p}_{D̂} f'}
        \and
        \inferrule[]
        {f \transAcc{D̂}{c}}
        {e +_b f \transAcc{D̂}{\overline{b} ∧ c}}
        \and
        \inferrule[]
        {e \transAcc{D̂}{b} \\ f \transAcc{D̂}{c}}
        {e; f \transAcc{D̂}{b ∧ c}}
        \and 
        \inferrule[]
        {e \transvia{b ∣ p}_{D̂} e'}
        {e; f \transvia{b ∣ p}_{D̂} e'; f}
        \and
        \inferrule[]
        {e \transAcc{D̂}{b} \\ f \transvia{c ∣ p}_{D̂} f'}
        {e; f \transvia{b ∧ c ∣ p}_{D̂} f'}
        \and  
        \inferrule[]
        {\\}
        {e^{(b)} \transAcc{D̂}{\overline{b}}}  
        \and  
        \inferrule[]
        {e \transvia{c ∣ p} e'}
        {e^{(b)} \transvia{b ∧ c ∣ p}_{D̂} e'; e^{(b)}}
    \end{mathpar}
    \caption{Symbolic Derivative of GKAT Automata.}\label{fig:derivatives-rules}
\end{figure*}

The symbolic derivative coalgebra \(D̂\), with expressions as states, is the least symbolic GKAT coalgebra (ordered by point-wise subset ordering on \(ϵ̂\) and \(δ̂\)) that satisfy the rules in~\Cref{fig:derivatives-rules}.
Notice that the rules listed on~\Cref{fig:derivatives-rules} is very close to that of Schmid et al.~\cite{schmid_GuardedKleeneAlgebra_2021}.
This is no coincidence, as our definition exactly lowers to the definition of theirs.
This fact can be proven by case analysis on the shape of the source expression, and forms a basis on our correctness argument.

\begin{theorem}[Correctness]
    The lowering of \(D̂\) is exactly the derivative defined by Schmid et. al.~\cite{schmid_GuardedKleeneAlgebra_2021}.
    TODO: unfold the statement.
\end{theorem}

% \begin{table*}
%     \centering
%     \begin{tabular}{c||c|c|c|c|c|c}
%         Exp & \(S\) 
%         & \(ϵ^*\) & \(δ^*\) 
%         & \(ϵ̂(s)\) & \(δ̂(s)\) \\\hline
%         \(b\) & \(∅\) 
%         & \{b\} & \(∅\) 
%         & N/A & N/A \\  
%         \(p\) & \(\{*\}\) 
%         & \(∅\) & \(\{(1, *, p)\}\) 
%         & \(\{1\}\) & \(∅\)\\  
%         \(e₁ +_b e₂\) & \(S₁ + S₂\) &
%         \(⟨\{b\}|~ϵ₁^* ∪ ⟨\{\overline{b}\}|~ϵ₂^*\) &
%         \(⟨\{b\}|~δ₁^* + ⟨\{\overline{b}\}|~δ₂^*\) & 
%         \(\begin{cases}
%             ϵ̂₁(s) & s ∈ S₁\\
%             ϵ̂₂(s) & s ∈ S₂\\
%         \end{cases}\) & 
%         \(\begin{cases}
%             δ̂₁(s) & s ∈ S₁\\
%             δ̂₂(s) & s ∈ S₂\\
%         \end{cases}\) \\  
%         \(e₁ ; e₂\) & \(S₁ + S₂\) & 
%         \(⟨ϵ₁^*|~ϵ₂^*\) & \(δ₁^* + ⟨ϵ₁^*|~δ₂^*\) & 
%         \(\begin{cases}
%             ⟨ϵ̂₁(s)| ~ ϵ₂^*& s ∈ S₁ \\  
%             ϵ̂₂(s) & s ∈ S₂
%         \end{cases}\)& 
%         \(\begin{cases}
%             δ̂₁(s) ∪ ⟨ϵ̂(s)|~δ₂^* & s ∈ S₁ \\  
%             δ̂2(s) & s ∈ S₂
%         \end{cases}\) \\  
%         \(e₁^{(b)}\) & \(S₁\) & 
%         \(\{\overline{b}\}\) & \(⟨\{b\}|~δ₁^*\) & 
%         \(⟨\{\overline{b}\}|~ϵ̂1(s)\) & 
%         \( ⟨\{b\}|⟨ϵ̂1(s)|~δ₁^* ∪ δ₁(s)\)
%     \end{tabular}
%     \caption{Symbolic Thompson's Construction}\label{tab:symb-thomposon-construction}
% \end{table*}


\begin{table*}
    \centering
    \begin{tabular}{c||c|c|l|l}
        Exp & \(S\) & \(s^*\)  
        & \(ϵ̂(s)\) & \(δ̂(s)\) \\\hline
        \(b\) & \(\{s^*\}\) 
        & \(s^*\)
        & \(\{b\}\) & \(∅\) \\  
        \(p\) & \(\{s^*, s₁\}\) 
        & \(s^*\)
        & \(\begin{cases}
           ∅ & s = s^* \\  
           \{1\} & s = s₁ 
        \end{cases}\) 
        & \(\begin{cases}
            \{(1, s₁, 0)\} & s = s^* \\  
            ∅ & s = s₁
        \end{cases}\)\\  
        \(e₁ +_b e₂\) & \(\{s^*\} + S₁ + S₂\) &
        \(s^*\) &
        \(\begin{cases}
            ⟨\{b\}| ϵ̂₁(s₁^*) ∪ ⟨\{b\}| ϵ̂₂(s₂^*) & s = s^* \\
            ϵ̂₁(s) & s ∈ S₁\\
            ϵ̂₂(s) & s ∈ S₂\\
        \end{cases}\) & 
        \(\begin{cases}
            ⟨\{b\}| δ̂₁(s₁^*) + ⟨\{b\}| δ̂₂(s₂^*) & s = s^* \\
            δ̂₁(s) & s ∈ S₁\\
            δ̂₂(s) & s ∈ S₂\\
        \end{cases}\) \\  
        \(e₁ ; e₂\) & \(S₁ + S₂\) & 
        \(s₁^*\) & 
        \(\begin{cases}
            ⟨ϵ̂₁(s)| ϵ̂₂(s₂^*)& s ∈ S₁ \\  
            ϵ̂₂(s) & s ∈ S₂
        \end{cases}\)& 
        \(\begin{cases}
            δ̂₁(s) + ⟨ϵ̂(s)| δ̂₂(s₂^*) & s ∈ S₁ \\  
            δ̂2(s) & s ∈ S₂
        \end{cases}\) \\  
        \(e₁^{(b)}\) & \(\{s^*\} + S₁\) & 
        \(s^*\) &
        \(\begin{cases}
            \{\overline{b}\} & s = s^*\\
            ⟨\{\overline{b}\}| ϵ̂1(s) & s ∈ S₁
        \end{cases}\) & 
        \(\begin{cases}
            ⟨\{b\}| δ̂₁(s₁^*) & s = s^* \\
            δ₁(s) ∪ ⟨\{b\}|⟨ϵ̂1(s)| δ̂₁(s₁^*) & s ∈ S₁
        \end{cases}\)
    \end{tabular}
    \caption{Symbolic Thompson's Construction}\label{tab:symb-thomposon-construction}
\end{table*}

Another way to construct an automaton is via Thompson's construction, we lift the original construction to the symbolic setting.
A common expression to construct is a guard operation, denoted by \(⟨B|\), where \(B\) is a set of boolean expressions.
TODO: define transition dynamics and accepting dynamics earlier.
Concretely, this guard can be defined on both accepting dynamics and transition dynamics:
\begin{align*}
    ⟨B|ϵ̂(s) & ≜ \{b ∧ c ∣ b ∈ B, c ∈ ϵ(s)\}; \\
    ⟨B|δ̂(s) & ≜ \{(b ∧ c, s', p) ∣ b ∈ B, (c, s', p) ∈ δ(s)\}.
\end{align*}
Notably, besides guarding transition and acceptance with different conditions, like in if statements, the guard expression can be used to simulate uniform continuation. Specifically, we can use \(⟨ϵ̂(s)|~δ(s)\) to connecting all the accepting state of \(s\) to the dynamic \(δ(s)\).

With these definitions in mind, we can define symbolic Thompson's construction inductively as in~\Cref{tab:symb-thomposon-construction}, where we let \((S₁, ϵ̂₁, δ̂₁)\) and \((S₂, ϵ̂₂, δ̂₂)\) to be result of Thompson's construction for \(e₁\) and \(e₂\) respectively.
The \(S₁ + S₂\) is the disjoint union of \(S₁\) and \(S₂\), and for any two transition dynamics \(δ₁(s₁): 𝒫(\Bool × S₁ × K)\) and \(δ₂(s₂): 𝒫(\Bool × S₂ × K)\), then \(δ₁^*(s₁) + δ₂^*(s₂): 𝒫(\Bool × (S₁ + S₂) × K)\) is the bifunctorial lift via \(+\).

One notable difference between the original construction~\cite{smolka_GuardedKleeneAlgebra_2020} and our construction is that we use a start state \(s^* ∈ S\), instead of a start dynamics (or pseudo-state).
This choice will make the proof slightly easier. 
However, in~\Cref{sec:optimization-implementation}, we will explain that our implementation uses start dynamics instead of start state, to avoid unnecessary lookups and unreachable states.

We would like to explore several desirable theoretical properties of both derivatives and Thompson's construction.
Specifically, the correctness, i.e. the semantics of the ``start state'' the both construction have the same preserves the trace semantics of the expression; finiteness, i.e. the coalgebra generated is always finite, which means that our equivalence algorithm will eventually terminate; and finally, how does the number of reachable state relate to the size of the expression, so that we can estimate the complexity of the equivalence checking algorithm.
Turns out all of these questions can be answered by a connection by a homomorphism from symbolic Thompson's construction to the symbolic derivatives.

\begin{theorem}\label{thm:hom-thompson-derivative}
    Given any GKAT expression \(e\), the resulting symbolic GKAT coalgebra from Thompson's construction \(Ŝ_e\) have a homomorphism to derivatives \(h: Ŝ_e → D̂\), s.t. for the start state \(s^* ∈ S, h(s^*) = e\).
\end{theorem}

\begin{proof}
    By induction on the structure of \(e\). We will recall that \(h: Ŝ_e → ⟨e⟩_D\) is a symbolic GKAT coalgebra homomorphism when the following two conditions are true: \(s \transAcc{Sₑ}{b}\) if and only if \(h(s) \transAcc{D̂}{b}\); and \(s \transvia{b ∣ p}_{Sₑ} s'\) if and only if \(h(s) \transvia{b ∣ p}_{D̂} h(s')\).

    When \(e ≜ b\) for some tests \(b\), then the function \(h\) is defined as \(\{s^* ↦ b\}\).
    When \(e ≜ p\) for some primitive action \(p\), then the function \(h\) is defined as \(\{s^* ↦ p, * ↦ 1\}\).
    The homomorphism condition can then be verified by unfolding the definition.

    When \(e ≜ e₁ +_b e₂\), by induction hypothesis, we have homomorphisms \(h₁: Ŝ_{e₁} → ⟨e₁⟩_D\) and \(h₂: Ŝ_{e₂} → ⟨e₂⟩_D\).
    Then we define the homomorphism 
    \[h(s) ≜ \begin{cases}
        e₁ +_b e₂ & s = s^* \\  
        h₁(s) & s ∈ Ŝ_{e₁} \\  
        h₂(s) & s ∈ Ŝ_{e₂}
    \end{cases}\]
    We show that \(h\) is a homomorphism. Because \(Ŝₑ\) preserves the transition and acceptance of \(Ŝ_{e₁}\) and \(Ŝ_{e₂}\), then for all \(s ∈ Ŝ_{e₁} ∩ Ŝ_{e}\), we have
    \begin{align*}
        & s \transAcc{Ŝ_{e}}{c} \text{ iff }
        s \transAcc{Ŝ_{e₁}}{c} \text{ iff }
        h₁(s) \transAcc{D̂}{c} \text{ iff }
        h(s) \transAcc{D̂}{c} \\ 
        & s \transvia{c ∣ p}_{Ŝ_{e}} s' \text{ iff }
        s \transvia{c ∣ p}_{Ŝ_{e₁}} s' \text{ iff }
        h₁(s) \transvia{c ∣ p}_{D̂} h₁(s') \text{ iff }
        h(s) \transvia{c ∣ p}_{D̂} h(s')  
    \end{align*}
    And similarly for \(s ∈ Ŝ_{e₂} ∩ Ŝ_{e}\).
    So we only need to show the homomorphic condition for the start state \(s^*\):
    \begin{align*}
        & s^* \transAcc{Ŝ_{e}}{c} \\
        \text{iff } &
        (∃ a, b ∧ a = c \text{ and } 
        s₁^* \transAcc{Ŝ_{e₁}}{a})
        \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        s₂^* \transAcc{Ŝ_{e₂}}{a}) \\
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        h₁(s₁^*) \transAcc{D̂}{a})
        \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        h₂(s₂^*) \transAcc{D̂}{a}) \\  
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        e₁ \transAcc{D̂}{a})
        \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        e₂ \transAcc{D̂}{a}) \\  
        \text{iff }& e₁ +_b e₂ \transAcc{D̂}{c}\\
        \text{iff }& h(s^*) \transAcc{D̂}{c}. \\[5px]
        & s^* \transvia{a ∣ p}_{Ŝ_{e}} s' \\
        \text{iff } &
        (∃ a, b ∧ a = c \text{ and } 
        s₁^* \transvia{a ∣ p}_{Ŝ_{e₁}} s')
        \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        s₂^* \transvia{a ∣ p}_{Ŝ_{e₂}} s') \\
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        h₁(s₁^*) \transvia{a ∣ p}_{D̂} h(s'))
        \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        h₂(s₂^*) \transvia{a ∣ p}_{D̂} h(s')) \\  
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        e₁ \transvia{a ∣ p}_{D̂} h(s'))
        \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        e₂ \transvia{a ∣ p}_{D̂} h(s')) \\  
        \text{iff }& e₁ +_b e₂ \transvia{a ∣ p}_{D̂} h(s') \\ 
        \text{iff }& h(s^*) \transvia{a ∣ p}_{D̂} h(s')
    \end{align*}

    When \(e ≜ e₁; e₂\), by induction hypothesis, we have two homomorphisms \(h₁: Ŝ_{e₁} → D̂\) and \(h₂: Ŝ_{e₂} → D̂\).
    We define \(h\) as follows:
    \[h(s) ≜ \begin{cases}
        h₁(s); e₂ & s ∈ Ŝ_{e₁} \\  
        h₂(s) & s ∈ Ŝ_{e₂}
    \end{cases}\]
    Then we can prove that \(h\) is a homomorphism by case analysis on \(s\). 
    First case is that \(s ∈ Ŝ_{e₁}\):
    \begin{align*}
        s \transAcc{Ŝ_{e}}{c} 
        \text{ iff } & 
            ∃ a, b, a ∧ b = c, 
                s \transAcc{Ŝ_{e₁}}{a} 
                \text{ and } 
                s₂^* \transAcc{Ŝ_{e₂}}{b} \\
        \text{ iff } & 
            ∃ a, b, a ∧ b = c, 
                h₁(s) \transAcc{D̂}{a} 
                \text{ and } 
                f \transAcc{D̂}{b} \\
        \text{ iff } & h₁(s); f \transAcc{D̂}{c} 
        \text{ iff } h(s) \transAcc{D̂}{c}.\\[5px]
        s \transvia{c ∣ p}_{S_{e}} s'
        \text{ iff } & 
            (s \transvia{c ∣ p}_{Ŝ_{e₁}} s')
            \text{ or }
            (∃ a, b, a ∧ b = c
                \text{ and }
                s \transAcc{Ŝ_{e₁}}{a}
                \text{ and }
                s₂^* \transvia{b ∣ p}_{Ŝ_{e₂}} s') \\  
        \text{ iff } & 
            (h₁(s) \transvia{c ∣ p}_{D̂} h₁(s')) 
            \text{ or }
            (∃ a, b, a ∧ b = c
                \text{ and }
                h₁(s) \transAcc{D̂}{a}
                \text{ and }
                e₂ \transvia{b ∣ p}_{D̂} h₂(s')) \\
        \text{ iff } & h₁(s) \transvia{c ∣ p} h(s') 
        \text{ iff } h(s) \transvia{c ∣ p} h(s').
    \end{align*}
    The case where \(s₂ ∈ Ŝ_{e₂}\) is straightforward, as \(Ŝ_{e}\) preserves the transitions of \(Ŝ_{e₂}\):
    \begin{align*}
        & s \transAcc{Ŝ_{e}}{c} \text{ iff }
        s \transAcc{Ŝ_{e₂}}{c} \text{ iff }
        h₂(s) \transAcc{D̂}{c} \text{ iff }
        h(s) \transAcc{D̂}{c}, \\ 
        & s \transvia{c ∣ p}_{Ŝ_{e}} s' \text{ iff }
        s \transvia{c ∣ p}_{Ŝ_{e₂}} s' \text{ iff }
        h₂(s) \transvia{c ∣ p}_{D̂} h₂(s') \text{ iff }
        h(s) \transvia{c ∣ p}_{D̂} h(s').
    \end{align*}
    

    When \(e ≜ {e₁}^{(b)}\), by induction hypothesis, we have a homomorphism \(h₁: Ŝ_{e₁} → D̂\); the homomorphism \(h\) can be defined as follows: 
    \[h(s) ≜ \begin{cases}
        {e₁}^{(b)} & s ≜ s^* \\  
        h₁(s); e₁^{(b)} & s ∈ Ŝ_{e₁}
    \end{cases}\]
    We prove the homomorphism condition by case analysis on \(s\). First case is that \(s = s^*\), then:
    \begin{align*}
        (s^* \transAcc{Ŝ_e}{c})
        \text{ iff } & (s^* \transAcc{Ŝ_e}{c} \text{ and } c = \overline{b})
        \text{ iff } ({e₁}^{(b)} \transAcc{D̂}{c} \text{ and } c = \overline{b})
        \text{ iff } (h(s^*) \transAcc{D̂}{c}) \\
        (s^* \transvia{c ∣ p}_{Ŝ_e} s')
        \text{ iff } & (∃ a, b ∧ a = c \text{ and } s₁^* \transvia{a ∣ p}_{Ŝ_{e₁}} s') \\
        \text{ iff } & (∃ a, b ∧ a = c \text{ and } e₁ \transvia{a ∣ p}_{D̂} h₁(s')) \\ 
        \text{ iff } & {e₁}^{(b)} \transvia{a ∣ p}_{D̂} h₁(s') 
        \text{ iff } h(s^*) \transvia{a ∣ p}_{D̂} h(s')
    \end{align*}
    The second case is when \(s ∈ Ŝ_{e₁}\), then:
    \begin{align*}
        & s \transAcc{Ŝ_e}{c}\\
        \text{ iff } & (∃ a, \overline{b} ∧ a = c \text{ and } s \transAcc{Ŝ_{e₁}}{a}) \\  
        \text{ iff } & (∃ a, \overline{b} ∧ a = c \text{ and } h₁(s) \transAcc{D̂}{a}) \\
        \text{ iff } & h₁(s);e₁^{(b)} \transAcc{D̂}{c} 
        \text{ iff } h(s) \transAcc{D̂}{c} \\[5px]
        & s \transvia{c ∣ p}_{Ŝ_e} s' \\
        \text{ iff } & 
            (s \transvia{c ∣ p}_{Ŝ_{e₁}} s' 
            \text{ or } 
            ∃ a₁, a₂, 
                b ∧ a₁ ∧ a₂ = c 
                \text{ and } 
                s \transAcc{Ŝ_{e₁}}{a₁} 
                \text{ and } 
                s₁^* \transvia{a₂ ∣ p}_{Ŝ_{e₁}} s') \\
        \text{ iff } & 
            (h₁(s) \transvia{c ∣ p}_{D̂} h₁(s') 
            \text{ or } 
            ∃ a₁, a₂, b ∧ a₁ ∧ a₂ = c 
                \text{ and } 
                h₁(s) \transAcc{D̂}{a₁} 
                \text{ and } 
                e₁ \transvia{a₂ ∣ p}_{D̂} h₁(s'))\\
        \text{ iff } & 
            (h₁(s) \transvia{c ∣ p}_{D̂} h₁(s') 
            \text{ or } 
            ∃ a₁, a₂, b ∧ a₁ ∧ a₂ = c 
                \text{ and } 
                h₁(s) \transAcc{D̂}{a₁} 
                \text{ and } 
                e₁^{(b)} \transvia{b ∧ a₂ ∣ p}_{D̂} h₁(s');e₁^{(b)}) \\ 
        \text{ iff } &
            (h₁(s); e₁^{(b)} \transvia{c ∣ p}_{D̂} h₁(s'); e₁^{(b)}) 
        \text{ iff } h(s) \transvia{c ∣ p}_{D̂} h(s')
        \qedhere
    \end{align*}
\end{proof}

\Cref{thm:hom-thompson-derivative} have several consequences, one of the more obvious one is that we can use the functoriality of the lowering operation to show the semantic equivalence of the start state in the thompson's construction and the expression in derivative. 

\begin{corollary}[Correctness]
    Given any expression \(e\) and its Thompson's coalgebra \(Ŝ_{e}\) with a start state \(s^* ∈ Ŝ_{e}\), then the semantics of the start state is equivalent to the semantics of \(e\): \(⟦s^*⟧_{Ŝ_{e}} = ⟦e⟧.\)
\end{corollary}

A not so obvious consequence of the homomorphism in~\Cref{thm:hom-thompson-derivative}, is the complexity of the algorithm based on derivatives.
Our bisimulation algorithm (\Cref{alg:symb-bisim}) only explores the principle sub-coalgebra of the start state, i.e. \(s^*\) in the Thompson's construction \(Ŝ_{e}\) or \(e\) in the derivative \(D̂\); thus, deducing an upper bound on the size of the principle sub-coalgebras \(⟨s^*⟩_{Ŝ_{e}}\) and \(⟨e⟩_{D̂}\) are crucial to our complexity analysis.
An upper bound on \(⟨s^*⟩_{Ŝ_{e}}\) is easy to obtain, as the size of \(Ŝ_{e}\), which subsumes the states of \(⟨s^*⟩_{Ŝ_{e}}\), is linear to the size of expression \(e\); therefore \(⟨s^*⟩_{Ŝ_{e}}\) is at most linear to the size of the expression \(e\).
On the other hand the size of \(⟨e⟩_{D̂}\) can, again, be derived from the homomorphism in~\cref{thm:hom-thompson-derivative}.

\begin{corollary}\label{thm:suj-hom-thompson-derivative}
    There exists a surjective homomorphism \(h': ⟨s^*⟩_{Ŝ_{e}} → ⟨e⟩_{D̂}\). 
    Because the size of \(⟨s^*⟩_{Ŝ_{e}}\) is linear to \(e\), the size of \(⟨e⟩_{D̂}\) is at most linear to the size of expression \(e\).
\end{corollary}

\begin{proof}
    We define \(h'\) to be point-wise equal to \(h\), i.e. \(h'(s) ≜ h(s)\). 
    Then we need to show that \(h'\) is well-defined ans surjective, which is a consequence of homomorphic image preserves principle sub-coalgebra (\Cref{thm:homo-img-preserve-principle-sub-coalg}): \(h(⟨s^*⟩_{Ŝ_{e}}) = ⟨h(s)⟩_{D̂} = ⟨e⟩_{D̂}.\)
    In other words, the image of \(h\) on \(⟨s^*⟩_{Ŝ_{e}}\) is equal to \(⟨e⟩_{D̂}\); thus, because \(h'\) is point-wise equal to \(h\) and is defined on \(⟨s^*⟩_{Ŝ_{e}}\), the range of \(h'\) contains its codomain \(⟨e⟩_{D̂}\), showing that \(h'\) is surjective.
\end{proof}

An important consequence of~\cref{thm:suj-hom-thompson-derivative} is that \(⟨s^*⟩_{Ŝ_{e}}\) will have no less state than \(⟨e⟩_{D̂}\).
However, it is important to notice that this does not mean running bisimulation algorithm on 

\section{Implementation}

\subsection{Optimization}\label{sec:optimization-implementation}

\subsection{Performance}\label{sec:performance-implementation}


\section{Future Work}

Can weak symbolic coalgebra leads to a simpler completeness proof.



\printbibliography

\newpage
\appendix


\end{document}
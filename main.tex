% Please compile this document using LuaLaTeX.
% because of the use of unicode-math
% XeLaTeX and PDFLaTeX will result in error.

\newif\iffull\fulltrue
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

% page number
\pagestyle{plain}

% \usepackage{cite}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%%%%% PACKAGES
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[margin=2.5cm]{geometry}
\usepackage{tabularx}

% move proofs to the end
\usepackage[conf={restate,no link to proof}]{proof-at-the-end}

% allow page break in align environment
\allowdisplaybreaks

% Biblatex
\usepackage[style=numeric]{biblatex}
\addbibresource{refs.bib}

% For adding inline comments in the text.
\usepackage[margin=false,inline=true]{fixme}
\FXRegisterAuthor{aaa}{anaaa}{\color{cyan}AAA}
\FXRegisterAuthor{mg}{anmg}{\color{red}MG}
\FXRegisterAuthor{cz}{ancz}{\color{orange}CZ}
% \newcommand{\aaa}[1]{\aaanote{#1}}
% \newcommand{\mg}[1]{\mgnote{#1}}
% \newcommand{\cz}[1]{\cznote{#1}}
\newcommand{\aaa}[1]{}
\newcommand{\mg}[1]{}
\newcommand{\cz}[1]{}

\usepackage{stmaryrd}

\usepackage{stackengine}
\usepackage{mathrsfs}
\usepackage{braket}
\usepackage{annotate-equations}
\usepackage{scalerel}

% graphs
\usepackage{tikz}
\usetikzlibrary{arrows,automata,positioning}

% commutative diagram
\usepackage{tikz-cd}

% ref
\usepackage{hyperref}
\usepackage{cleveref}

% inference rule
\usepackage{mathpartir}
% cross-referencing infer rule
% based on https://tex.stackexchange.com/questions/340788/cross-referencing-inference-rules
\makeatletter
\let\originferrule\inferrule
\DeclareDocumentCommand \inferrule { s O {} m m}{%
  \IfBooleanTF{#1}%
  {%
    \mpr@inferstar[#2]{#3}{#4}%
  }{%
    \mpr@inferrule[#2]{#3}{#4}%
  }%
  \IfValueT{#2}%
  {%
    \my@name@inferrule{#2}%
  }%
}
\NewDocumentCommand \my@name@inferrule { m }{%
  \def\@currentlabelname{\textsc{#1}}%
}
\makeatother

% item spacing
\usepackage{enumitem}

% for code
\usepackage{listings}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{framextopmargin=50pt,frame=bottomline}
\lstset{language=caml, escapeinside={[*}{*]}}
% Clever ref names
\crefname{lstlisting}{listing}{listings}
\Crefname{lstlisting}{Listing}{Listings}

% for algorithm
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
% switch statement for pattern matching
\algnewcommand\algorithmicmatch{\textbf{match}}
\algnewcommand\algorithmicwith{\textbf{with}}
\algnewcommand\algorithmiccase{\textbf{case}}
\algnewcommand\algorithmicdefault{\textbf{default}}
\algnewcommand\Continue{\textbf{continue}}
\algdef{SE}[MATCH]{Match}{EndMatch}[1]{\algorithmicmatch\ #1\ \algorithmicwith}{\algorithmicend\ \algorithmicmatch}%
\algdef{SE}[CASE]{Case}{EndCase}[1]{\algorithmiccase\ #1 \algorithmicthen}{\algorithmicend\ \algorithmiccase}%
\algdef{SE}[DEFAULT]{Default}{EndDefault}{\hskip\algorithmicindent\algorithmicdefault}{\algorithmicend\algorithmicdefault}%
\algtext*{EndMatch}%
\algtext*{EndCase}%
\algtext*{EndDefault}%

% for better table
\usepackage{booktabs}

% subcaption
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption

% unicode math symbols
\usepackage{unicode-math}
% support for hat, overline, underline, vec, and sim combining charactors
\protected\def\afteracc{\directlua{
    local nest = tex.nest[tex.nest.ptr]
    local last = nest.tail
    if not (last and last.id == 18) then
      error'I can only put accents on simple noads.'
    end
    if last.sub or last.sup then
      error'If you want accents on a superscript or subscript, please use braces.'
    end
    local acc = node.new(21, 1)
    acc.nucleus = last.nucleus
    last.nucleus = nil
    local is_bottom = token.scan_keyword'bot' and 'bot_accent' or 'accent'
    acc[is_bottom] = node.new(23)
    acc[is_bottom].fam, acc[is_bottom].char = 0, token.scan_int()
    nest.head = node.insert_after(node.remove(nest.head, last), nil, acc)
    nest.tail = acc
    node.flush_node(last)
  }}
\AtBeginDocument{
\begingroup
  \def\UnicodeMathSymbol#1#2#3#4{%
    \ifx#3\mathaccent
      \def\mytmpmacro{\afteracc#1 }%
      \global\letcharcode#1=\mytmpmacro
      \global\mathcode#1="8000
    \else\ifx#3\mathbotaccentwide
      \def\mytmpmacro{\afteracc bot#1 }%
      \global\letcharcode#1=\mytmpmacro
      \global\mathcode#1="8000
    \fi\fi
  }
  \input{unicode-math-table}
\endgroup
}

% math font, this is needed to render \setminus command
\setmathfont{latinmodern-math}
\setmathfont[range=\setminus]{STIX Two Math}
\setmathfont[range=\similarrightarrow]{STIX Two Math}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}

\newtheorem{definition}{Definition}
\newtheorem*{definition*}{Definition}
\newtheorem{example}[definition]{Example}
\newtheorem{remark}[definition]{Remark}

%%%% Macros %%%%%

% Math?
\newcommand{\true}{\mathrm{true}}
\newcommand{\false}{\mathrm{false}}
\newcommand{\At}{\mathbf{At}}


% operators
\newcommand{\dom}[1]{\mathrm{dom}(#1)}
\newcommand{\cod}[1]{\mathrm{cod}(#1)}
\DeclareMathOperator{\post}{\mathrm{post}}
\newcommand{\reject}{\mathinner{\mathrm{rej}}}
\newcommand{\accept}{\mathinner{\mathrm{acc}}}
\DeclareMathOperator*{\bigplus}{\scalerel*{+}{\sum}}
\newcommand{\clos}[1]{\mathrel{\overline{#1}}}
\DeclareMathOperator{\norm}{\mathrm{norm}}
\DeclareMathOperator{\dead}{\mathrm{dead}}
\DeclareMathOperator{\symb}{\mathrm{symb}}
\DeclareMathOperator{\unsymb}{\symb^{-1}}


% commands 
\newcommand{\command}[1]{{\mathtt{#1}}}
\newcommand{\comAssume}[1]{\command{assume}~#1}
\newcommand{\comITE}[3]{\command{if}~#1~\command{then}~#2~\command{else}~#3}
\newcommand{\comWhile}[2]{\command{while}~#1~\command{do}~#2}

% set of models
\newcommand{\theoryOf}[1]{\ensuremath{\mathsf{#1}}}
\newcommand{\Exp}{\theoryOf{Exp}}
\newcommand{\BExp}{\theoryOf{BExp}}
\DeclareMathOperator{\GS}{\mathrm{GS}}

\newcommand\altxrightarrow[2][0pt]{\mathrel{\ensurestackMath{\stackengine%
  {\dimexpr#1-7.5pt}{\xrightarrow{\phantom{#2}}}{\scriptstyle\!#2\,}%
  {O}{c}{F}{F}{S}}}}
\newcommand{\transvia}[1]{
    \mathrel{\raisebox{-2px}{\(\altxrightarrow[-2px]{#1}\)}}
}
\newcommand{\transAcc}[2]{⇒_{#1} #2}
\newcommand{\transRej}[2]{↓_{#1} #2}

 

\begin{document}

\title{On-the-fly Algorithms for GKAT Equivalences
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

% anonymize authors
%
\author{\IEEEauthorblockN{Cheng Zhang\textsuperscript{\textsection}}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{University College London}\\
London, United Kingdom \\
0000-0002-8197-6181}
\and
\IEEEauthorblockN{Qiancheng Fu}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
qcfu@bu.edu}
\and
\IEEEauthorblockN{Hang Ji\textsuperscript{\textsection}}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Cornell University}\\
Ithaca, USA \\
hj476@cornell.edu}
\and
\IEEEauthorblockN{Ines Santacruz}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
inessdv15@gmail.com}
\and
\IEEEauthorblockN{Marco Gaboardi}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
gaboardi@bu.edu}
}

\maketitle
\begingroup\renewcommand\thefootnote{\textsection}
\footnotetext{Work largely performed at Boston University}
\endgroup

\begin{abstract}
We present several new algorithms to efficiently decide Guarded Kleene Algebra with Tests (GKAT) equivalences.
Although the current algorithm scales nearly-linearly with the size of the expression, its performance is hindered by the need to compute the entire coalgebra for the sake of normalization and exponential performance degradation when expanding the set of primitive tests.
To address these problems, we introduce two new algorithms for bisimulation checking: an on-the-fly bisimulation algorithm based on greedy bisimulation, which enables immediate termination upon discovering a counter-example and supports on-the-fly coalgebra generation with derivatives; and a symbolic algorithm that builds upon the on-the-fly algorithm while leveraging efficient SAT solvers to speed up boolean comparisons and compress the transition structures of GKAT coalgebra.
Additionally, to utilize the symbolic bisimulation checking, we provide two methods to generate symbolic GKAT coalgebras from GKAT expressions, analogous to derivative and Thompson's construction in previous works.
We have proven the correctness of our algorithms and conducted brief complexity analyses.
Finally, we implemented the symbolic algorithms in Rust, demonstrating orders-of-magnitude performance improvements over existing tools.
\end{abstract}

\begin{IEEEkeywords}
Guarded Kleene Algebra With Tests, Kleene Algebra, Coalgebra, Program Verification, Algorithm
\end{IEEEkeywords}



\section{Introduction}

Kleene Algebra with Tests (KAT) is a staple algebra system to reason about program equivalence. It has found applications in numerous fields including but not limited to software-defined networks~\cite{anderson_NetKATSemanticFoundations_2014a,moeller_KATchFastSymbolic_2024,smolka_ScalableVerificationProbabilistic_2019,smolka_CantorMeetsScott_2017}, program transformations~\cite{kozen_BohmJacopiniTheorem_2008,angus_KleeneAlgebraTests_2001,kozen_NonlocalFlowControl_2008,grathwohl_KAT_2014,kozen_KleeneAlgebraTests_1996}, and program logics~\cite{antonopoulos_AlgebraAlignmentRelational_2023,zhang_DomainReasoningTopKAT_2024,zhang_IncorrectnessLogicKleene_2022c,kozen_HoareLogicKleene_2000,desharnais_ModalKleeneAlgebra_2004}.
Beyond its theoretical achievements, Kleene Algebra with Tests has also inspired several practical tools for specialized domains.
Interestingly, despite the PSPACE-complete complexity of KAT, many tools developed are able to match, or even exceed, the performance of specialized state-of-the-art tools in their respective domains~\cite{moeller_KATchFastSymbolic_2024,smolka_ScalableVerificationProbabilistic_2019}.

This surprising fact leads to the development of Guarded Kleene Algebra with Tests (GKAT)~\cite{smolka_GuardedKleeneAlgebra_2020}, a (co)algebraic system that seeks to characterize this efficient fragment of KAT.
The determinism of GKAT gives it a different coalgebra structure from KAT, and allows for a GKAT expression to be directly converted to a linear-sized coalgebra, as opposed to the exponential sized coalgebra produced by KAT expressions~\cite{smolka_GuardedKleeneAlgebra_2020,kozen_CoalgebraicTheoryKleene_2017}.

However, as it currently stands, the desirable theoretical properties of GKAT do not translate to tangible real-world performance. 
The decision procedures for GKAT can often be slower than that of KAT; primarily due to the following two reasons.

First, the natural semantics of GKAT coalgebra is not the finite-trace model, but the infinite-trace model~\cite{smolka_GuardedKleeneAlgebra_2020,schmid_GuardedKleeneAlgebra_2021}.
Thus, to determine finite-trace equivalence, it is necessary to \emph{normalize} the coalgebra first. 
This requires one to compute the coalgebra ahead-of-time and iterate through its entirety.
Without the need to normalize prior to computing bisimulation, the decision procedure of KAT equivalence can terminate as soon as a counter-example has been found, potentially leading to better performance than the original GKAT decision algorithm~\cite{smolka_GuardedKleeneAlgebra_2020}.

Secondly, the lack of symbolic algorithms makes GKAT a less desirable foundation for performance-critical applications.
Although the generated coalgebra from GKAT expression have a linear number of states, the number of transitions is exponential to \emph{all} the primitive tests appearing in the entire expression.
This inefficiency is noted in previous literature, and poses a real limitation on practical systems~\cite{smolka_GuardedKleeneAlgebra_2020,zhang_CFGKATEfficientValidation_2025}.
On the other hand, symbolic algorithms for KAT have been proven to be effective for generic equality solving and domain-specific applications~\cite{pous_SymbolicAlgorithmsLanguage_2015,moeller_KATchFastSymbolic_2024}.

In this paper, we present two novel algorithms that address both shortcomings, and demonstrate how the coalgebra theory of GKAT serves as the foundation for the design and correctness of these algorithms.

The first algorithm supports on-the-fly coalgebra generation by performing bisimulation in a greedy manner.
Specifically, the liveness checking of states only kicks in when the bisimulation algorithm encounters a disparity.
We show the correctness of this algorithm through a novel coalgebraic concept which we named \emph{greedy bisimulation}.

The second algorithm lifts the first into a symbolic setting, where our development is based on a generalization of GKAT coalgebra, which we named \emph{symbolic GKAT coalgebra}.
In contrast to the symbolic KAT derivative~\cite{pous_SymbolicAlgorithmsLanguage_2015}, the deterministic nature of GKAT coalgebra allows its symbolic variant to transition via boolean expressions instead of primitive tests. 
This optimization not only allows more compact coalgebra representation, but also allows the usage of arbitrary off-the-shelf SAT solvers to resolve boolean constraints, instead of being limited to binary decision diagrams as utilized by the symbolic algorithm of KAT~\cite{pous_SymbolicAlgorithmsLanguage_2015}. 
Finally, we lift both the derivative construction~\cite{schmid_GuardedKleeneAlgebra_2021} and Thompson's construction~\cite{smolka_GuardedKleeneAlgebra_2020} to the symbolic setting, and prove their correspondence, correctness, and finiteness.

To evaluate the practical impact of our work, we implement our algorithms in Rust. We show through experiments that
our implementation performs orders of magnitude better than existing implementation for KAT~\cite{pous_SymbolicAlgorithmsLanguage_2015}, confirming that the theoretical elegance of GKAT indeed leads to significant real-world performance improvements.

We list our contributions and structures of this paper as follows:
\begin{itemize}
    \item In~\Cref{sec:background}, we outline the necessary concepts in universal coalgebra, and the coalgebraic theory of GKAT. In particular, we do not define the semantics of GKAT either algebraically or coalgebraically, as we can avoid directly interacting with the semantics by leveraging the coalgebraic theory of GKAT.
    \item In~\Cref{sec:greedy-bisim}, we define \emph{greedy bisimulation} and develop its coalgebraic theory to serve as the ground for the correctness of our algorithm.
    We also perform a brief complexity analysis and argue that our algorithm is almost always more efficient than the original normalize-bisimulate approach~\cite{smolka_GuardedKleeneAlgebra_2020}.
    \item In~\Cref{sec:symb-gkat-greedy-bisim}, we introduce \emph{symbolic GKAT coalgebra}, and lift the greedy bisimulation algorithm to the symbolic setting.
    \item In~\Cref{sec:symb-gkat-construction}, we lift the Thompson's construction and derivative construction to the symbolic setting.
    Then we establish their connection via a coalgebra homomorphism, which allows us to derive the correctness and finiteness of one construction from the other.
    \item Finally, in~\Cref{sec:implementation}, we describe our implementation of symbolic derivative, symbolic Thompson's construction, and symbolic greedy bisimulation.
    We analyze algorithm performance under different satisfiability solver backends, and compare our work to previous implementations.
    When using MiniSat~\cite{een_MINISAT_2004} as the backend, our implementation can solve most test cases in less than a second with minimal memory usage, whereas previous implementation~\cite{pous_SymbolicAlgorithmsLanguage_2015} almost always result in a timeout.
\end{itemize}

\section{Background on Coalgebra and GKAT}\label{sec:background}

\subsection{Concepts in Universal Coalgebra}

In this paper, we will make heavy use of coalgebraic theory, thus we will recall useful notions and theorems in universal coalgebra.
Given a functor \(F\) on the category of set and functions, a \emph{coalgebra over \(F\)} or \emph{\(F\)-coalgebra} consists of a set \(S\) and a function \(σ_S: S → F(S)\).
We typically call elements in \(S\) the \emph{states} of the coalgebra, and \(σ_S(s)\) the \emph{dynamic} of state \(s\).
We will sometimes overload \(S\) to denote both the state set and the coalgebra, when no ambiguity can arise. 

A homomorphism between two \(F\)-coalgebra \(S\) and \(U\) is a map \(h: S → U\) that preserves the function \(σ\):
\[
    \begin{tikzcd}
        S \ar{r}{h} \ar[swap]{d}{σ_S} & U \ar{d}{σ_U} \\  
        F(S) \ar{r}{F(h)} & F(U)
    \end{tikzcd}    
\]

When we can restrict the homomorphism map into an inclusion map \(i: S' → S\) for \(S' ⊆ S\) then we say that \(S'\) is a \emph{sub-coalgebra} of \(S\), denoted as \(S' ⊑ S\).
In fact, the function \(σ_{S'}\) is uniquely determined by the states \(S'\)~\cite[Proposition 6.1]{rutten_UniversalCoalgebraTheory_2000}.
Sub-coalgebras are also preserved under homomorphic images and pre-images: 
\begin{theoremEnd}{lemma}[Theorem 6.3~\cite{rutten_UniversalCoalgebraTheory_2000}]\label{thm:hom-(pre)img-preserve-sub-coalg}
    given a homomorphism \(h: S → U\), and sub-coalgebras \(S' ⊑ S\) and \(U' ⊑ U\), then 
    \(h(S') ⊑ U \text{ and } h^{-1}(U') ⊑ S.\)
\end{theoremEnd}

One particularly important sub-coalgebra of a coalgebra \(S\) is the least sub-coalgebra that contains a state \(s\). 
We will denote this sub-coalgebra as \(⟨s⟩_{S}\), and call it \emph{principle sub-coalgebra} generated by \(s\). 
We sometimes omit the subscript \(S\) when it can be inferred from context or irrelevant.
Intuitively, we usually think of principle sub-coalgebra \(⟨s⟩_S\) as the sub-coalgebra that is formed by all the ``reachable states'' from \(s\).
For all coalgebra \(S\) and a state \(s ∈ S\), principle sub-coalgebra \(⟨s⟩_S\) always exists and is unique, because sub-coalgebra of any coalgebra forms a complete lattice~\cite[theorem 6.4]{rutten_UniversalCoalgebraTheory_2000}; thus taking the meet of all the sub-coalgebra that contains \(s\) will yield \(⟨s⟩_S\).

Similar to sub-coalgebra, principle sub-coalgebra is also preserved under homomorphic image:
\begin{theoremEnd}{theorem}\label{thm:homo-img-preserve-principle-sub-coalg}
    Homomorphic image preserves principle sub-GKAT coalgebra. Specifically, given a homomorphism \(h: S → U\):
    \(h(⟨s⟩_S) = ⟨h(s)⟩_U\)
\end{theoremEnd}

\begin{proofEnd}
    We will need to show that \(h(⟨s⟩_{S})\) is the smallest sub-GKAT coalgebra of \(S\) that contain \(h(s)\). 
    By definition of image, \(h(⟨s⟩_{S})\) indeed contain \(h(s)\). 
    By \Cref{thm:hom-(pre)img-preserve-sub-coalg}, \(h(⟨s⟩_S) ⊑ U\), i.e. \(h(⟨s⟩_S)\) is a sub-coalgebra of \(U\).
    Finally, take any sub-coalgebra \(U' ⊑ U\) s.t. \(h(s) ∈ U'\): 
    \begin{align*}
        h(s) ∈ U' 
        & ⟹ s ∈ h^{-1}(U') \\  
        & ⟹ ⟨s⟩_S ⊑ h^{-1}(U') & \text{definition of \(⟨s⟩_S\)}\\  
        & ⟹ h(⟨s⟩_S) ⊑ U' & \text{\Cref{thm:hom-(pre)img-preserve-sub-coalg}}
    \end{align*}
    Hence \(h(⟨s⟩_S)\) is the smallest sub-GKAT coalgebra of \(U\) that contains \(h(s)\).
\end{proofEnd}

% bisimulation

A \emph{final coalgebra} \(ℱ\) over a signature \(F\), sometimes called the \emph{behavior} or \emph{semantics} of coalgebras over \(F\), is an \(F\)-coalgebra s.t. for all \(F\)-coalgebra \(S\), there exists a unique homomorphism \(\mathrm{beh}_S: S → ℱ\).

Given two \(F\)-coalgebra \(S\) and \(U\), the \emph{behavioral equivalence} between states in \(S\) and \(U\) can be computed by a notion called \emph{bisimulation}.
A relation \({∼} ⊆ S × U\) is called a \emph{bisimulation relation} if it forms an \(F\)-coalgebra: \[σ_{∼}: {∼} → F(∼),\] 
and its projections \(π₁: {∼} → S\) and \(π₂: {∼} → U\) are both homomorphisms:
\[
    \begin{tikzcd}[column sep=1.25cm]
        S \ar[swap]{d}{σ_S}
            & ∼ \ar[swap]{l}{π₁} \ar{r}{π₂} \ar{d}{σ_{∼}} 
            & U \ar{d}{σ_T}\\  
        F(S) & F(∼) \ar{l}{F(π₁)} \ar[swap]{r}{F(π₂)} & F(T)
    \end{tikzcd}
\]
We sometimes abuse the notation \({∼} ⊆ S × U\) to specify that \(∼\) is a bisimulation between \(S\) and \(U\).

When given a homomorphism \(h: S → U\), we can construct a bisimulation \({∼} ⊆ S × U\) where it relates all the states \(s ∈ S\) with \(h(s) ∈ U\). Each component of the pair will transition individually via their respective transition function \(δ_∼ ≜ δ_S × δ_U\).
\begin{theoremEnd}{corollary}\label{thm:hom-preserves-semantics}
    Given a homomorphism \(h: S → U\) between two \(F\)-coalgebra \(S, U\), then for all states \(s ∈ S\), \(\mathrm{beh}_S(s) = \mathrm{beh}_U(h(s)).\)
\end{theoremEnd}

\subsection{GKAT and Its Coalgebra}

Guarded Kleene Algebra with Tests, or GKAT~\cite{smolka_GuardedKleeneAlgebra_2020}, is a deterministic fragment of Kleene Algebra with Tests (KAT). 
The syntax of GKAT over a set of primitive actions \(K\) and a set of primitive tests \(T\) can be defined in two sorts, boolean expressions \(\BExp\) and GKAT expressions \(\Exp\):
\begin{align*}
    a, b, c ∈ \BExp 
        & ≜ 1 ∣ 0 ∣ t ∈ T ∣ b ∧ c ∣ b ∨ c ∣ \overline{b} \\  
    e, f ∈ \Exp 
        & ≜ p ∈ K ∣ b ∈ \BExp ∣ e +_b f ∣ e ; f ∣ e^{(b)} 
\end{align*}
where \(e +_b f\) is the if statement with condition \(b\), \(e;f\) is the sequencing of expression \(e\) and \(f\), and \(e^{(b)}\) is the while loop with body \(e\) and condition \(b\).
We use notation like \(b ≤ c\), \(b ≡ c\), and \(b ≢ c\) for the usual order, equivalence, and inequivalence in Boolean Algebra.
A GKAT expression can be unfolded into a KAT expression in the usual manner~\cite{kozen_KleeneAlgebraTests_1997c}:
\begin{align*}
    e +_b f & ≜ b; e + \overline{b}; f &
    e^{(b)} & ≜ (b; e)^*; \overline{b}.
\end{align*}
Then the semantics of each expression \(⟦e⟧\) can be computed by the semantics of Kleene Algebra with Tests~\cite{kozen_KleeneAlgebraTests_1997c}.
An important construct in the semantics is \emph{atoms}, which are conjunctions of all the primitive tests either in its positive or negative form: for \(T ≜ \{t₁, t₂, …, tₙ\}\),
\[\At_T ≜ \{t₁' ∧ t₂' ∧ ⋯ ∧ tₙ' ∣ tᵢ' ∈ \{tᵢ, \overline{tᵢ}\}\}.\]
Following the conventional notation, we denote an atom using \(α, β\); and we sometimes omit the subscript \(T\) when no confusion can arise.
Alternatively, atoms can also be thought of as truth assignments to each primitive test, indicating which primitive tests are satisfied in the current program state; and \(α ≤ b\) in Boolean Algebra if and only if the truth assignment represented by \(α\) satisfies \(b\).

For the sake of brevity, we omit the complete definition of GKAT and KAT semantics, we refer the reader to previous works~\cite{smolka_GuardedKleeneAlgebra_2020,schmid_GuardedKleeneAlgebra_2021,kozen_KleeneAlgebraTests_1997c}, which explain these semantics in detail.
Our work avoids direct interaction with the semantics by leveraging prior results in the coalgebraic theory of GKAT, which we summarize below.

Formally, GKAT coalgebras over primitive actions \(K\) and primitive tests \(T\) are coalgebras over the following functor:
\[G(S) ≜ (\{\accept, \reject\} + S × K)^{\At_T}.\] 
Intuitively, given a state \(s ∈ S\) and an atom \(α ∈ \At\), \(δ_S(s, α)\) will deterministically execute one of the following: reject \(α\) when \(δ(s, α) = \reject\), denoted as \(s \transRej{S}{α}\); accept \(α\) when \(δ(s, α) = \accept\), denoted \(s \transAcc{S}{α}\); or transition to a state \(s' ∈ S\) and execute action \(p ∈ K\) when \(δ(s, α) = (s', p)\), denoted \(s \transvia{α ∣ p}_S s'\).

In particular, \(G\) is a simple polynomial functor~\cite{jacobs_IntroductionCoalgebraMathematics_2016}, allowing the sub-coalgebra to preserve and reflect bisimulations.
\begin{theoremEnd}{theorem}[sub-coalgebras perserve and reflect bisimulation]\label{thm:sub-coalg-preserve-bisim}
    Given two states in sub-coalgebra \(s ∈ S' ⊑ S\) and \(u ∈ U' ⊑ U\), there exists a bisimulation \({∼'} ⊆ S' × U'\) s.t. \(s ∼' u\) if and only if there exists a bisimulation \({∼} ⊆ S × U\) s.t. \(s ∼ u\).
\end{theoremEnd}

\begin{proofEnd}
    For \(⟹\) direction, we will show that if \({∼'} ⊆ S' × U'\) is a bisimulation, then \({∼'}\) is also a bisimulation between \(S\) and \(U\):
    \[
        \begin{tikzcd}
            S \ar{d}{δ_S} & S' \ar[hook',swap]{l}{i} \ar{d}{δ_S}
            & ∼ \ar[swap]{l}{π₁} \ar{r}{π₂} \ar{d}{δ_∼}
            & U' \ar[hook]{r}{i} \ar{d}{δ_S} & U \ar{d}{δ_T}\\  
            G(S) & G(S') \ar[hook',swap]{l}{G(i)} 
            & G(∼) \ar[swap]{l}{G(π₁)} \ar{r}{G(π₂)} & U' \ar[hook]{r}{G(i)} & U 
        \end{tikzcd}
    \]
    Because the inclusion homomorphism \(i\) doesn't change the input, we have:
    \begin{align*}
        & {∼} \xrightarrow{π₁} S' \xrightarrow{i} S = {∼} \xrightarrow{π₁} S; \\
        & {∼} \xrightarrow{π₂} U' \xrightarrow{i} U = {∼} \xrightarrow{π₂} U.
    \end{align*}

    To prove the \(⟸\) we will show that if \({∼} ⊆ S × U\) is a bisimulation, then the restriction \({∼}' ≜ \{(s, u) ∈ {∼} ∣ s ∈ S', u ∈ U'\}\) is a bisimulation between \(S'\) and \(U'\).
    We first realize that \(∼_{S', U'}\) is a pre-image of the maximal bisimulation \(≣_{S', U'}\) along the inclusion homomorphism \(i: {∼} → {≡_{S, U}}\).
    This means that \(∼_{S', U'}\) can be formed by a pullback square:
    \[
        \begin{tikzcd}
            ∼_{S', U'} \ar{r}{i} \ar[swap]{d}{i} \ar[phantom, very near start]{dr}{\scalebox{1.5}{\(\lrcorner\)}} & ≣_{S', U'} \ar{d}{i}\\ 
            {∼} \ar[swap]{r}{i} & {≡_{S, U}}
        \end{tikzcd}
    \]
    Recall that elementary polynomial functor~\cite{jacobs_IntroductionCoalgebraMathematics_2016} like \(G\) preserves pullback, hence the pullback also uniquely generates a GKAT coalgebra~\cite{rutten_UniversalCoalgebraTheory_2000}
\end{proofEnd}

Besides the nice property of bisimulation, GKAT coalgebra is also deterministic, unlike Kleene Coalgebra with Tests (KCT)~\cite{kozen_CoalgebraicTheoryKleene_2017}.
For each atom, states in a KCT can non-deterministically accept, reject (but not both), or transition to multiple different states while executing different actions.
As we will see later, while the deterministic behavior of GKAT coalgebra enables a more versatile symbolic algorithm than KCT~\cite{pous_SymbolicAlgorithmsLanguage_2015}, it also comes with unique challenges. 
Specifically, GKAT coalgebra requires normalization to compute finite trace equivalences~\cite{smolka_GuardedKleeneAlgebra_2020}, where we reroute all the transitions that cannot lead to acceptance immediately into rejection; and states that can never lead to acceptance are called \emph{dead states}.

In previous works, these dead states are detected after the necessary coalgebra is computed and stored.
This approach requires storing all the transitions and states of coalgebra in memory, meaning that the algorithm does not short circuit even if a mismatch can be detected in the starting state; for example, when for some \(α ∈ \At\), \(δ_S(s, α) = \reject\) and \(δ_U(u, α) = \accept\), then we can conclude \(s, u\) have different trace semantics, without needing to compute the rest of the coalgebra.

Our algorithm is lazy in the dead-state detection i.e. the dead states are only checked when a mismatch requires it; even then, our dead state detection algorithm will only check the necessary states to compute the liveness of the states causing the mismatch.
This not only avoids unnecessarily checking the liveness of a state, but also enables on-the-fly generation of the coalgebra, where we can remove irrelevant states from memory after they have been explored. 

However, to truly understand our on-the-fly algorithms, we will first need to rigorously define ``dead states'', and their roles in defining the coalgebraic semantic of GKAT. 

\subsection{Liveness and Sub-GKAT coalgebras}

Traditionally, live and dead states are defined by whether they can reach an accepting state~\cite{smolka_GuardedKleeneAlgebra_2020}.
\begin{definition}[Liveness]\label{def:liveness-of-states}
    A state \(s\) is \emph{accepting} if there exists an atom \(α ∈ \At\) s.t. \(δ(s, α) = \accept\).
    A state \(s₀\) is \emph{live} if there exists an accepting state \(sₙ\) s.t. there is a liveness chain:
    \[s₀ \transvia{α₁ ∣ p₁} s₁ \transvia{α₂ ∣ p₂} s₂ ⋯ \transvia{αₙ ∣ pₙ} sₙ.\]
    A state is \emph{dead} if it is not live.
\end{definition}
Besides directly working with this explicit definition, we can also give a coalgebraic characterization of liveness.
Specifically, by induction on the length of the path from state \(s\) to an arbitrary reachable state from \(s\), we can show that the principle sub-coalgebra \(⟨s⟩_S\) exactly contains all the reachable states of \(s\) in the GKAT coalgebra \(S\).
Thus, we obtain the following coalgebraic characterization of liveness:
\begin{theoremEnd}{theorem}\label{thm:liveness-principle-sub-coalg}
    A state \(s\) is \emph{live} if there exists an accepting state \(s' ∈ ⟨s⟩\); and a state \(s\) is \emph{dead} if there is no accepting state in \(⟨s⟩\).
\end{theoremEnd}
This alternative liveness definition can help us prove important theorems regarding reachability and liveness without explicitly performing induction on the liveness chain. 
We show the following theorems as examples:
\begin{theoremEnd}[normal]{lemma}\label{thm:dead-iff-all-reachable-dead}
    A state \(s\) is dead if and only if all elements in \(⟨s⟩\) are dead.
\end{theoremEnd}
\begin{proofEnd}
    \(⟸\) direction is true, because \(s ∈ ⟨s⟩\): if all \(⟨s⟩\) is dead, then \(s\) is dead. 
    \(⟹\) direction can be proven by taking any \(s' ∈ ⟨s⟩\); then we notice \(⟨s'⟩ ⊑ ⟨s⟩\) because \(s'\) is the minimal sub-coalgebra that contains \(s'\). 
    Since there is no accepting state in \(⟨s⟩\), there cannot be any accepting state in \(⟨s'⟩\), hence \(s'\) is also dead.
\end{proofEnd}

\begin{theoremEnd}[normal]{theorem}[homomorphism perserves liveness]\label{thm:hom-preserve-liveness}
    Given a homomorphism \(h: S → U\) and a state \(s ∈ S\):
    \[\text{\(s\) is live} ⟺ \text{\(h(s)\) is live}\]
\end{theoremEnd}

\begin{proofEnd}
    Because homomorphic image preserves principle sub-GKAT coalgebra (\Cref{thm:homo-img-preserve-principle-sub-coalg}) \(h(⟨s⟩_S) = ⟨h(s)⟩_U;\) therefore for any state \(s' ∈ S\):
    \[s' ∈ ⟨s⟩_S ⟺ h(s') ∈ h(⟨s⟩_S) ⟺ h(s') ∈ ⟨h(s)⟩_U.\]
    And because \(s'\) is accepting if and only if \(h(s')\) accepting by definition of homomorphism; then \(⟨s⟩_S\) contains an accepting state if and only if \(⟨h(s)⟩_U\) contains an accepting state. 
    Therefore, \(s\) is live in \(S\) if and only if \(h(s)\) is live in \(U\).
\end{proofEnd}

The above theorem then leads to several interesting liveness preservation properties for structures on coalgebras, like sub-coalgebra and bisimulation.

\begin{theoremEnd}{corollary}[sub-coalgebra perserves liveness]\label{thm:sub-coalg-preserve-liveness}
    For a sub-coalgebra \(S' ⊑ S\) and a state \(s ∈ S'\), \(s\) is live in \(S'\) if and only if \(s\) is live in \(S\).
\end{theoremEnd}

\begin{proofEnd}
    Let the homomorphism \(h\) in \cref{thm:hom-preserve-liveness} be the inclusion homomorphism \(i: S' → S\).
\end{proofEnd}

\begin{theoremEnd}{corollary}[bisimulation preserves liveness]\label{thm:bisim-preserve-liveness}
    For two states in GKAT coalgebras \(s ∈ S\) and \(u ∈ U\), with a bisimulation \({∼} ⊆ S × U\) s.t. \(s ∼ u\), then \(s\), \(u\), and \((s, u)\) has the same liveness, i.e. needs to be all accepting, all live, or all dead, in \(S\), \(U\), and \(∼\) respectively.
\end{theoremEnd}

\begin{proofEnd}
    Because for a \(∼\) is a bisimulation when both \(π₁: {∼} → S\) and \(π₂: {∼} → U\) are homomorphisms.
    Therefore, 
    \begin{align*}
        s \text{ is live in } S 
        & ⟺ π₁((s, u)) \text{ is live in } S \\        
        & ⟺ (s, u) \text{ is live in } {∼} \\  
        & ⟺ π₂((s, u)) \text{ is live in } U  \\
        & ⟺ u \text{ is live in } U. 
        \qedhere
    \end{align*}
\end{proofEnd}


\subsection{Normalization And Semantics}

Infinite-trace model \(𝒢_ω\) is the final coalgebra of GKAT coalgebras~\cite{schmid_GuardedKleeneAlgebra_2021}.
Thus, the unique homomorphism from any coalgebra \(S\) to \(𝒢_ω\) will assign each states in \(S\) a semantics.
We denote this unique homomorphism as \(⟦-⟧^{ω}_{S}: S → 𝒢_ω\), where the semantic equivalences can be identified by bisimulation:
\(⟦s⟧^{ω}_{S} = ⟦t⟧^{ω}_{T}\) if and only if there exists a bisimulation \({∼} ⊆ S × T\), s.t. \(s ∼ t\).

Infinite-trace equivalences can be directly computed with bisimulation on derivatives, which supports on-the-fly algorithms as demonstrated by similar systems~\cite{kozen_CoalgebraicTheoryKleene_2017,almeida_DecidingKATHoare_2012,pous_SymbolicAlgorithmsLanguage_2015}. 
However, the \emph{finite} trace model \(\mathscr{G}\) is the final coalgebra of \emph{normal GKAT coalgebras}, where no state can transition to a dead state~\cite{smolka_GuardedKleeneAlgebra_2020}. 
Fortunately, every GKAT coalgebra can be normalized by rerouting all the transitions from dead states to rejection.
Given a GKAT coalgebra \(S ≜ (S, δ_S)\), the transition of its normalized coalgebra \(δ_{\norm(S)} : S → G(S)\) is defined as \(δ_{\norm(S)}(s, α) ≜ \reject\) when \(δ_S(s, α) = (s', p)\) and \(s'\) is dead in \(S\); and \(δ_{\norm(S)}(s, α) ≜ δ_S(s, α)\) otherwise. 
We call \(\norm(S) ≜ (S, δ_{\norm(S)})\) the \emph{normalized} coalgebra of \(S\).

We use \(⟦-⟧_S: \norm(S) → \mathscr{G}\) to denote the finite trace semantics of GKAT coalgebra, which is the unique homomorphism into the final coalgebra \(\mathscr{G}\). 
The finite trace equivalence between \(s ∈ S\) and \(u ∈ U\) can be decided by first normalizing \(S\) and \(U\) then deciding whether there is a bisimulation \({∼} ⊆ \norm(S) × \norm(U)\) s.t. \(s ∼ t\).
For a more detailed explanation on the finite trace semantics, we refer the reader to the work of \Citeauthor{smolka_GuardedKleeneAlgebra_2020}~\cite{smolka_GuardedKleeneAlgebra_2020}, we will only recall the correctness theorem here.

\begin{theoremEnd}{theorem}[Correctness~\cite{smolka_GuardedKleeneAlgebra_2020}]\label{thm:norm-bisim-correctness}
    Given two states in two GKAT coalgebra \(s ∈ S\) and \(u ∈ U\), then there exists a bisimulation between normalized coalgebras \({∼} ⊆ \norm(S) × \norm(U)\) s.t. \(s ∼ u\) if and only if \(s\) and \(u\) are trace equivalent \(⟦s⟧_S = ⟦u⟧_U\)
\end{theoremEnd}

Besides giving us the finite-trace semantics, the normalization operation also satisfies several nice properties.

First, normalization preserves liveness, i.e. a state \(s ∈ S\) is dead (live) if and only if it is dead (live) in \(\norm(S)\), allowing us to perform liveness analysis in \(\norm(S)\) to obtain the liveness in \(S\), and vise versa.

\begin{theoremEnd}{theorem}[normalization perserves liveness]\label{thm:norm-perserve-liveness}
    A state \(s ∈ S\) is dead (live) in \(S\) if and only if it is dead (live) in \(\norm(S)\).
\end{theoremEnd}

\begin{proofEnd}
    By definition, every state is either dead of live, thus we will only need to show the \(⟹\) direction, and 

    This proof unfortunately requires us unfolding the path to accepting states.

    If \(s\) is dead in \(S\), then by~\Cref{thm:dead-iff-all-reachable-dead}, for all \(α, p\), \(s \transvia{α ∣ p}_S s'\) implies \(s'\) is dead. 
    Because \(s\) cannot accept any atom in \(S\), \(s\) will reject all atom in \(\norm(S)\); which implies that \(s\) is dead in \(\norm(S)\).

    If \(s\) is live in \(S\), then there exists a walk \(s \transvia{b₁ ∣ p₁}_S s₁ \transvia{b₂ ∣ p₂}_S ⋯ \transvia{bₙ ∣ pₙ}_S sₙ\) s.t. \(sₙ\) is accepting, which implies every state \(sᵢ\) on the walk is live. 
    Hence, this walk also exists in \(\norm(S)\), and \(s\) is live in \(\norm(S)\).
\end{proofEnd}

Second, normalization is an endofunctor in the category of GKAT coalgebra. This result can help us obtain sub-coalgebras of normalized coalgebra, and also connect the finite trace with infinite trace semantics.

\begin{theoremEnd}{theorem}\label{thm:norm-functor}
    \(\norm\) is an endofunctor in the category GKAT coalgebra.
    More specifically, if \(h: S → U\) is a GKAT homomorphism, then \(h: \norm(S) → \norm(U)\) is also a homomorphism.
\end{theoremEnd}

\begin{proofEnd}
    Recall that \(h\) is a homomorphism if and only if for all \(s ∈ S\) and \(α ∈ \At\):
    \begin{itemize}[nosep]
        \item for a result \(r ∈ \{\reject, \accept\}\), 
        \[δ_S(s, α) = r ⟺ δ_U(h(s), α) = r;\]
        \item for any \(s' ∈ S\) and \(p ∈ K\), 
        \[δ_S(s, α) = (s', p) ⟺ δ_{U}(h(s), α) = (h(s'), p).\]
    \end{itemize}

    Then we show that \(h: \norm(S) → \norm(U)\) is a homomorphism, this is a consequence of homomorphism preserves liveness (\Cref{thm:hom-preserve-liveness}): for all \(s ∈ \norm(S)\) and \(α ∈ \At\):
    \begin{align*}
        & δ_{\norm(S)}(s, α) = \accept \\*
        ⟺{}& δ_S(s, α) = \accept \\*  
        ⟺{}& δ_U(h(s), α) = \accept \\*  
        ⟺{}& δ_{\norm(U)}(h(s), α) = \accept;\\
        & δ_{\norm(S)}(s, α) = \reject \\*  
        ⟺{}& δ_{S}(s, α) = \reject 
        \text{ or } δ_{S}(s, α) = (s', p), s' \text{ is dead} \\*
        ⟺{}& δ_{U}(h(s), α) = \reject \\*
        & \text{ or } δ_{U}(h(s), α) = (h(s'), p), h(s') \text{ is dead} \\*  
        ⟺{}& δ_{\norm(U)}(h(s), α) = \reject;\\
        & δ_{\norm(S)}(s, α) = (s', p) \\*
        ⟺{}& δ_{S}(s, α) = (s', p), s' \text{ is live} \\*  
        ⟺{}& δ_{U}(h(s), α) = (h(s'), p), h(s') \text{ is live}\\* 
        ⟺{}& δ_{\norm(U)}(h(s), α) = (h(s'), p).
        \qedhere
    \end{align*}
\end{proofEnd}

\begin{theoremEnd}{corollary}\label{thm:norm-sub-coalg}
    Normalization preserves sub-coalgebra, i.e. if \(S' ⊑ S\) then \(\norm(S') ⊑ \norm(S)\).
\end{theoremEnd}

\begin{proofEnd}
    By letting the homomorphism in~\Cref{thm:norm-functor} to be the inclusion homomorphism \(i: S' → S\)
\end{proofEnd}

Because of the functoriality, we can show that two states that are infinite-trace equivalent implies these two states are finite-trace equivalent.
This gives us more tool in proving semantic equivalence between two states in GKAT coalgebras: proving bisimulation of these two states in the \emph{non-normalized} GKAT coalgebra is sufficient (but not necessary) to obtain finite-trance equivalence for the two states.

\begin{theoremEnd}{corollary}\label{thm:inf-trace-equiv-implies-fin-trace-equiv}
    Given two states in two GKAT coalgebra \(s ∈ S, u ∈ U\), \(⟦s⟧^{ω}_{S} = ⟦u⟧^{ω}_{U} ⟹ ⟦s⟧_{S} = ⟦u⟧_{U}\).
\end{theoremEnd}

\begin{proofEnd}
    Because \(⟦s⟧^{ω}_{S} = ⟦u⟧^{ω}_{U}\), there exists a bisimulation \({∼} ⊆ S × U\) s.t. \(s ∼ u\)~\cite{schmid_GuardedKleeneAlgebra_2021}.
    Therefore, we have the following span in the category of GKAT coalgebra:
    \[\begin{tikzcd}
        S & ∼ \ar{r}{π₂} \ar[swap]{l}{π₁} & U
    \end{tikzcd}\]
    Then by~\Cref{thm:norm-functor}, \(\norm(∼)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\):
    \[\begin{tikzcd}
        \norm(S) 
        & \norm(∼) \ar{r}{π₂} \ar[swap]{l}{π₁} 
        & \norm(U)
    \end{tikzcd}\]
    Because \(s ∼ u\) and normalization operation preserves states in \(∼\), therefore \((s, u) ∈ \norm(∼)\), and because \(\norm(∼)\) is a bisimulation between the normalization of \(S\) and \(U\), therefore \(⟦s⟧_{S} = ⟦u⟧_{U}\) (\Cref{thm:norm-bisim-correctness}).
\end{proofEnd}

\section{On-The-Fly Bisimulation}\label{sec:greedy-bisim}

The original algorithm for deciding GKAT equivalences~\cite{smolka_GuardedKleeneAlgebra_2020} requires the entire coalgebra to be known prior to the execution of the bisimulation algorithm; specifically, it is necessary to iterate through the entire coalgebra in order to identify the liveness of every single states and perform the normalization operation.
This limitation poses challenges to the design of an efficient on-the-fly algorithm for GKAT.
To make the equivalence-checking procedure scalable, we propose to greedily run the bisimulation algorithm and only invoke liveness detection when a discrepancy is found. 
For example, when constructing the bisimulation between \(s ∈ S\) and \(u ∈ U\), if \(s \transvia{α ∣ p}_S s'\) and \(u \transvia{α ∣ p}_U u'\), we will proceed to recurse on \(s'\) and \(u'\), without checking the liveness of \(s'\) and \(u'\).
Whereas, if \(s \transRej{S}{α}\) but \(u \transvia{α ∣ p}_U u'\), then the transition of \(s\) and \(u\) disagree, requiring \(u'\) to be dead.

This approach offers several benefits: first, the liveness detection is only called when necessary, thus unlikely to iterate through the entire coalgebra; second, the algorithm can short-circuit on a counter-example, allowing termination before all the states are explored; finally, the algorithm supports on-the-fly coalgebra constructions like derivatives, as the coalgebra is no longer required to be fully constructed before bisimulation.

The intuition of this on-the-fly algorithm can be expressed as the following coalgebraic structure, which we named ``greedy bisimulation''.

\begin{definition}[Greedy Bisimulation]\label{def:greedy-bisim}
    Given two GKAT coalgebra \(S\) and \(U\), and a bisimulation \({∼} ⊆ \norm(S) × \norm(U)\), a greedy bisimulation \({≈} ⊆ S × U\) is the least GKAT coalgebra that contains \(∼\) with the following transition function:
    \[
        δ_≈((s, u), α) ≜ \begin{cases}
            \accept & \begin{aligned}
                & \text{if } δ_S(s, α) = \accept \\[-3px]
                & \text{and } δ_U(s, α) = \accept
            \end{aligned}\\[5px]
            ((s', u'), p) & 
                \begin{aligned}
                    & \text{if } s \transvia{α ∣ p}_{S} s' \\[-3px]
                    & \text{and } u \transvia{α ∣ p}_{U} u' 
                \end{aligned}\\[5px]
            \reject & \text{otherwise}
        \end{cases}
    \]
    We call \(∼\) \emph{the underlying bisimulation} of \(≈\).
\end{definition}

Given any bisimulation \({∼} ⊆ S × U\), we can construct a GKAT coalgebra \(∼_d\) s.t. it is \emph{almost} a greedy bisimulation, i.e. it is closed under \(δ_≈\) in~\Cref{def:greedy-bisim}, and contains \(∼\), except it is not necessarily the least GKAT coalgebra that satisfies these properties.

\begin{theoremEnd}{lemma}\label{thm:dead-construction-greedy-bisim}
    For any greedy bisimulation \({≈} ⊆ S × U\) with the underlying bisimulation \({∼}\), the following relation \({∼_d} ⊆ S × U\):
    \[{∼_d} ≜ {∼} ∪ \{(s, u) ∣ \text{\(s\) and \(u\) are dead in \(S\) and \(U\)}\},\]
    forms a GKAT coalgebra with the transition function \(δ_≈\) in~\Cref{def:greedy-bisim}, which implies \({≈} ⊑ {∼_d}\).
    Furthermore, \(\norm(∼_d)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\).
\end{theoremEnd}

\begin{proofEnd}
    We will need to show \(∼_d\) is closed under the transition of \(δ_{≈}\) in~\Cref{def:greedy-bisim}.
    Specifically, if \(s ∼_d u\) and \(s \transvia{α ∣ p}_{S} s'\) and \(u \transvia{α ∣ p} u'\), then \(s' ∼_d u'\).
    By case analysis on the liveness of \(s'\) and \(u'\) in \(S\) and \(U\):
    \begin{itemize}
        \item If both \(s'\) and \(u'\) are dead, then \(s ∼_d u\) by definition of \(∼_d\).
        \item If both \(s'\) and \(u'\) are live, then \(s, s'\) and \(u, u'\) are live in \(S\) and \(U\) respectively. 
        Thus, \(s \transvia{α ∣ p}_{\norm(S)} s'\), and \(u \transvia{α ∣ p}_{\norm(U)} u'\).
        Therefore, \(s' ∼ u'\) by definition of bisimulation on \(\norm(S)\) and \(\norm(U)\); and \(s ∼_d u'\).
        \item If \(s'\) is live but \(u'\) is dead, then \(s\) is live, and \(s ∼ u\).
        However, \(s \transvia{α ∣ p}_{\norm(S)} s'\) yet \(u \transRej{\norm(U)}{u}\), which violates the condition for bisimulation, thus this case cannot happen.
        \item If \(u'\) is live but \(s'\) is dead, this case also cannot happen similar to the previous case.
    \end{itemize}

    By~\Cref{thm:bisim-between-dead}, we can construct a bisimulation \(∼_{s,u} ≜ \{(s, u)\} ⊆ \norm(S) × \norm(U)\) between any two dead states \(s ∈ S\) and \(u ∈ U\).
    Then because bisimulation is closed under union~\cite[Theorem 5.5]{rutten_UniversalCoalgebraTheory_2000}, \(\norm(∼_d) = {∼} ∪ ⋃ \{∼_{s, u} ∣ \text{\(s, u\) are dead}\}\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\).
    % TODO: bisimulation are uniquely determined by its carrier.
\end{proofEnd}

Although \Cref{thm:dead-construction-greedy-bisim} can be proved by unfolding the definition, \(∼_d\) turns out to be an important structure. 
Specifically, \(∼_d\) serves a connection between \(≈\) and its underlying bisimulation \(∼\), and the order \({∼} ⊆ {≈} ⊑ {∼_d}\) will reveal many structural properties of \(≈\).

\begin{theoremEnd}[normal]{corollary}\label{thm:greedy-bisim-exists-unique}
    For any bisimulation \({∼} ⊆ \norm(S) × \norm(U)\), a greedy bisimulation \(≈\) with \(∼\) as underlying bisimulation exists and is unique.
\end{theoremEnd}

\begin{proofEnd}
    Because sub-coalgebra form a complete lattice under union and intersection~\cite[Theorem 6.4]{rutten_UniversalCoalgebraTheory_2000}, then \(≈\) is the intersection of all the sub-coalgebra of \(∼_d\) (\Cref{thm:dead-construction-greedy-bisim}) containing \(∼\).
\end{proofEnd}

\begin{theoremEnd}[normal]{corollary}\label{thm:greedy-bisim-dead-or-bisim}
    For every pair of states \((s, u)\) in a greedy bisimulation \({≈} ⊆ S × U\), either \((s, u)\) is in the underlying bisimulation \(∼\), or both \(s\) and \(u\) are dead in \(S\) and \(U\).
\end{theoremEnd}

\begin{proofEnd}
    Because \({≈} ⊑ {∼_d}\), and \(∼_d\) only contains pairs in \(∼\) or pairs of dead-state, then so is \(≈\).
\end{proofEnd}

\begin{theoremEnd}[normal]{corollary}[Perservation Of Liveness]\label{thm:greedy-bisim-perserve-liveness}
    Given a pair of states in a greedy bisimulation \(s ≈ u\) where \({≈} ⊆ S × U\), then \((s, u)\), \(s\), \(u\) all have the same liveness in \(≈\), \(S\), and \(U\) respectively.
\end{theoremEnd}

\begin{proofEnd}
    Let \(∼\) by the underlying bisimulation of \(≈\), consider the \(∼_d\) in~\Cref{thm:dead-construction-greedy-bisim}.
    Because \({≈} ⊑ {∼_d}\) and \(\norm(∼_d) ⊆ \norm(S) × \norm(U)\) is a bisimulation,
    \begin{align*}
        & \text{\(s\) is live in \(S\)} \\
        \text{ iff } & \text{\(s\) is live in \(\norm(S)\)} 
            & \text{\Cref{thm:norm-perserve-liveness}}\\  
        \text{ iff } & \text{\((s, u)\) is live in \(\norm(∼_d)\)} 
            & \text{\Cref{thm:bisim-preserve-liveness}}\\
        \text{ iff } & \text{\((s, u)\) is live in \(∼_d\)} 
            & \text{\Cref{thm:norm-perserve-liveness}}\\
        \text{ iff } & \text{\((s, u)\) is live in \(≈\)} 
            & \text{\Cref{thm:sub-coalg-preserve-liveness}}
    \end{align*}
\end{proofEnd}

\begin{theoremEnd}[normal]{corollary}\label{thm:greedy-bisim-iff-norm-bisim}
    Given two GKAT coalgebras \(S\) and \(U\), and a relational GKAT coalgebra \({≈} ⊆ S × U\) with the transition \(δ_≈\) as in~\Cref{def:greedy-bisim}.
    \(≈\) is a greedy bisimulation if and only if \(\norm(≈)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\)
\end{theoremEnd}

\begin{proofEnd}
    The \(⟹\) direction, we let \(∼_d\) be defined as in~\Cref{thm:dead-construction-greedy-bisim}.
    Because \({≈} ⊑ {∼_d}\) and~\Cref{thm:norm-sub-coalg}, \(\norm(≈) ⊑ \norm(∼_d)\). 
    Because \(\norm(∼_d)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\), therefore \(\norm(≈)\) is also a bisimulation (\Cref{thm:greedy-bisim-iff-norm-bisim}).

    The \(⟸\) direction can be proven by simply letting \(\norm(≈)\) be the underlying bisimulation of \(≈\), because \(\norm(≈)\) has the same carrier set of \(≈\), then \(≈\) has to be the least GKAT coalgebra with \(δ_≈\) that contains \(\norm(≈)\), satisfying all the conditions for greedy bisimulation.
\end{proofEnd}

Finally, with all the structural properties in place, we can derive that the maximal greedy bisimulation actually corresponds to maximal bisimulation.

\begin{theoremEnd}{theorem}\label{thm:max-bisim-is-max-greedy-bisim}
    Given two GKAT coalgebra \(S\) and \(U\), carrier of the maximal bisimulation \({≡} ⊆ \norm(S) × \norm(U)\) is also the carrier of maximal greedy bisimulation, which we will denote as \({≊} ⊆ S × U\).
\end{theoremEnd}

\begin{proofEnd}
    By definition of greedy bisimulation, the carrier of \({≡}\) is a subset of the carrier of \({≊}\). 
    Then because of~\Cref{thm:greedy-bisim-iff-norm-bisim}, \(\norm(≊)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\), which implies \(\norm(≊) ⊑ {≡}\).
    Finally, because \(\norm\) do not change the carrier set, therefore the carrier of \(≡\) and \(≊\) necessarily coincide.
\end{proofEnd}

Similarly to the correctness of bisimulation between normalized coalgebra (\Cref{thm:norm-bisim-correctness}), we also establish a correctness theorem for greedy bisimulation, i.e. there exists a greedy bisimulation between two states if and only if they are semantically equivalent.

\begin{theoremEnd}{theorem}[Correctness]\label{thm:greedy-bisim-correctness}
    Given two states in GKAT coalgebras \(s ∈ S\) and \(u ∈ ,U\), there exists a greedy bisimulation \({≈} ⊆ S × U\) s.t. \(s ≈ u\), if and only if \(⟦s⟧_S = ⟦u⟧_U\).
\end{theoremEnd}

\begin{proofEnd}
    \(⟹\) direction, because normalization preserves carrier set, therefore \((s, u) ∈ \norm(≈)\).
    And because \(\norm(≈)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\) (\Cref{thm:greedy-bisim-iff-norm-bisim}), \(⟦s⟧_S = ⟦u⟧_U\).

    \(⟸\) direction, because \(⟦s⟧_S = ⟦u⟧_U\), then by~\cref{thm:norm-bisim-correctness}, then there a bisimulation \(∼ ⊆ \norm(S) × \norm(U)\) s.t. \(s ∼ u\). 
    Because~\Cref{thm:greedy-bisim-exists-unique}, therefore there exists a greedy bisimulation \(≈\) with \(∼\) as its underlying bisimulation. 
    Because the carriers \({∼} ⊆ {≈}\), therefore \(s ∼ u\) implies \(s ≈ u\).
\end{proofEnd}

Despite satisfying many desirable structural properties, the definition of greedy bisimulation is still coalgebraic.
To aid the design of an algorithm based on greedy bisimulation, we establish the correspondence of its definition with a set of concrete criteria, that can be checked by an algorithm.
In the following theorem, we will present such necessary and sufficient conditions to construct a greedy bisimulation.
\begin{theoremEnd}{theorem}[Recursive Construction]\label{thm:recursive-construction}
    Given a relation \({≈} ⊆ S × U\), \(≈\) is a carrier of some greedy bisimulation if and only if all the following condition are satisfied: for all pair of states \(s ≈ u\),
    \begin{enumerate}
        \item\label{itm:acc-condition} for all \(α ∈ \At\), 
        \(s \transAcc{S}{α}\) if and only if \(u \transAcc{U}{α}\);
        \item\label{itm:rej-or-dead} if \(s\) reject \(α\) but \(u\) transitions to \(u'\), then \(u'\) is dead; similarly when \(u\) rejects \(α\);  
        \item\label{itm:transition-bisim} If \(δ_{S}(s, α) = (s', p)\) and \(δ_{U}(u, α) = (u', p)\), then \(s' ≈ u'\).
        \item\label{itm:transition-dead} If \(δ_{S}(s, α) = (s', p)\) and \(δ_{U}(u, α) = (u', q)\), s.t. \(p ≠ q\), then both \(s'\) and \(u'\) are dead.
    \end{enumerate}
\end{theoremEnd}

\begin{proof}[Sketch]
    The \(⟹\) direction is by unfolding the definitions; and the \(⟸\) direction is proven by demonstrating \(\norm(≈) ⊆ \norm(S) × \norm(U)\) is a bisimulation, then \(≈\) is a greedy bisimulation because of~\Cref{thm:greedy-bisim-iff-norm-bisim}.
\end{proof}

\begin{proofEnd}
    To show the \(⟹\) direction, assuming there exists a greedy bisimulation \(≈\), then it would satisfy all the conditions. 
    We first note that the pairs \((s, u) ∈ {≈}\) can satisfy one of two properties (or both), either \(s\) and \(u\) are both dead in \(S\) and \(U\), or \((s, u)\) is in the underlying bisimulation \(∼\).
    Then we will verify that all the condition is satisfied both when \(s ∼ u\) and when \(s, u\) are dead.

    The condition~\labelcref{itm:acc-condition} holds if \(s ∼ u\):
    \begin{align*}
        s \transAcc{S}{α}
        \text{ iff }& s \transAcc{\norm(S)}{α} \\
        \text{ iff }& (s, u) \transAcc{∼}{α} \\ 
        \text{ iff }& u \transAcc{\norm(U)}{α} \\
        \text{ iff }& u \transAcc{U}{α};
    \end{align*}
    condition~\labelcref{itm:acc-condition} also holds if \(s\) and \(u\) are dead in \(S\) and \(U\), because both can never accept any atoms.

    The condition~\labelcref{itm:rej-or-dead} holds when \(s ∼ u\): 
    \begin{align*}
        & δ_S(s, α) = \reject \text{ and } δ_U(u, α) = (u', p)\\
        ⟹{}& δ_{\norm(S)}(s, α) = \reject \text{ and } δ_U(u, α) = (u', p) \\
        ⟹{}& δ_{\norm(U)}(u, α) = \reject \text{ and } δ_U(u, α) = (u', p)\\
        ⟹{}& \text{\(u'\) is dead}
    \end{align*}
    Similarly, when \(u'\) reject \(α\) and \(s\) transitions to \(s'\).
    The condition~\labelcref{itm:rej-or-dead} holds when \(s, u\) are both dead because both of them can only transition to dead state by~\Cref{thm:dead-iff-all-reachable-dead}.

    The condition~\labelcref{itm:transition-bisim} holds because when \(s \transvia{α ∣ p}_S s'\) and \(u \transvia{α ∣ p}_U u'\), then \((s, u) \transvia{α ∣ p}_{≈} (s', u')\) by definition of greedy bisimulation (\Cref{def:greedy-bisim}).

    The condition~\labelcref{itm:transition-dead} holds, let \(δ_S(s, α) = (s', p)\) and \(δ_U(u, α) = (u', q)\) and \(p ≠ q\). 
    \begin{itemize}
        \item If \(s ∼ u\), then we can show condition~\labelcref{itm:transition-dead} by case analysis on the liveness of \(s'\) and \(u'\):
        if \(s', u'\) are not both dead, then the condition of \({∼} ⊆ \norm(S) × \norm(U)\) cannot be satisfied, hence \((s', u') ∉ {∼}\), which by~\Cref{thm:greedy-bisim-dead-or-bisim} means that \(s'\) and \(u'\) are both dead, contradicting the premise.
        Therefore, \(s'\) and \(u'\) needs to be both dead.
        \item If \(s\) and \(u\) are both dead, then by~\Cref{thm:dead-iff-all-reachable-dead}, both \(s\) and \(u\) can only transition to dead state, hence \(s'\) and \(u'\) are both dead.
    \end{itemize}

    Then we prove the \(⟸\) direction.
    If \(≈\) satisfy all the conditions, then by condition~\labelcref{itm:transition-bisim}, \(≈\) is closed under \(δ_≈\), hence is a GKAT coalgebra.
    We let its underlying bisimulation be \(\norm(≈)\), which is sound if \(\norm(≈)\) is a bisimulation (\cref{thm:greedy-bisim-iff-norm-bisim}).
    Thus, we only need to show that \(\norm(≈)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\). Without loss of generality, we will show that \(π₁: \norm(≈) → \norm(S)\) is a bisimulation.

    For all pair of states \((s, u) ∈ \norm(≈)\) and any atom \(α ∈ \At\), we will verify all the conditions of a homomorphism.
    For the accepting case, by the definition of \(δ_{≈}\):
    \begin{align*}
        (s, u) \transAcc{\norm(≈)}{α}
        & \text{ iff } (s, u) \transAcc{≈}{α} \\  
        & \text{ implies } s \transAcc{≈}{α}
    \end{align*}
    For the transition case, because \((s, u)\) is live in \(≈\) implies that \(s\) is live in \(S\) (\Cref{thm:greedy-bisim-perserve-liveness}):
    \begin{align*}
        & (s, u) \transvia{α ∣ p}_{\norm(S)} (s', u') \\
        & \text{iff }
            (s, u) \transvia{α ∣ p}_{S} (s', u')
            \text{ and \(s, u\) is live in \(≈\)} \\  
        & \text{implies }
            s \transvia{α ∣ p}_S s' 
            \text{ and \(s\) is live in \(S\)} \\  
        & \text{implies }
            s \transvia{α ∣ p}_{\norm(S)} s' 
    \end{align*}
    Finally, the rejection case: \((s, u) \transRej{\norm(≈)}{α}\) either when \((s, u) \transvia{α ∣ p}_{≈} (s', u')\) and \((s', u')\) is dead, or \((s, u) \transRej{≈}{α}\).
    We will show that both cases will lead to \(s \transRej{\norm(S)}{α}\).
    The \((s, u) \transvia{α ∣ p}_{≈} (s', u')\) case can be proven by the definition of \(δ_≈\) and~\Cref{thm:greedy-bisim-perserve-liveness}:
    \begin{align*}
        & (s, u) \transvia{α ∣ p}_{≈} (s', u') 
            \text{ and \((s', u')\) is dead in \(≈\)} \\
        \text{implies } & s \transvia{α ∣ p}_{S} s' 
            \text{ and \(s'\) is dead in \(S\)} \\  
        \text{implies } & s \transRej{\norm(S)}{α}.
    \end{align*}
    Then the hardest case is when \((s, u) \transRej{≈}{α}\) because by definition of \(δ_≈\) (\Cref{def:greedy-bisim}) there are many cases that can lead to rejection.
    We will need to eliminate irrelevant cases using the condition provided by the construction.
    We proceed by case analysis on dynamics of \(s\) and \(u\) on \(α\); first listing some cases that contradicts the premise \((s, u) \transRej{≈}{α}\):
    \begin{itemize}
        \item If \(s\) accepts \(α\) in \(S\), then \(u\) necessarily accepts \(α\) in \(U\) by condition~\labelcref{itm:acc-condition}, which implies \((s, u) \transAcc{≈}{α}\), contradicting the premise.
        \item Similarly, if \(u\) accepts \(α\) in \(U\), then \((s, u) \transAcc{≈}{α}\), contradicting the premise.
        \item If \(s \transvia{α ∣ p}_S s'\) and \(u \transvia{α ∣ p}_U u'\) then \((s, u) \transvia{α ∣ p}_≈ (s', u;)\), contradicting the premise.
    \end{itemize}
    Then we list the other cases, which indeed satisfy \((s, u) \transRej{≈}{α}\), and show that all of them leads to \(s \transRej{\norm(S)}{α}\), using the condition in the theorem:
    \begin{itemize}
        \item If \(s \transRej{S}{α}\), then \(s \transRej{\norm(S)}{α}\).
        \item If \(s \transvia{α ∣ p}_S s'\) and \(u \transRej{U}{α}\), then by condition~\labelcref{itm:rej-or-dead}, \(s'\) is dead, and \(s \transRej{\norm(S)}{α}\).
        \item If \(s \transvia{α ∣ p}_S s'\) and \(u \transvia{α ∣ q}_U u'\), then by condition~\labelcref{itm:transition-dead}, both \(s'\) and \(u'\) are dead, and \(s \transRej{\norm(S)}{α}\).
        \qedhere
    \end{itemize}
\end{proofEnd}



\begin{theoremEnd}[all end]{lemma}\label{thm:bisim-between-dead}
    Given any two dead states \(s ∈ S\) and \(u ∈ U\), we can construct a bisimulation \(∼\) between \(\norm(S)\) and \(\norm(U)\):
    \begin{align*}
        ∼ &≜ \{(s, u)\}, & δ_∼((s, u)) & ≜ \reject.
    \end{align*}
    Therefore, for the maximal bisimulation \({≡} ⊆ \norm(S) × \norm(U)\), \(s ≡ u\).
\end{theoremEnd}

Besides the construction, a classical optimization of the bisimulation-construction algorithm is to look for an equivalence relation instead of a relation~\cite{smolka_GuardedKleeneAlgebra_2020,hopcroft_LinearAlgorithmTesting_1971}.
This technique also applies to greedy bisimulation.

\begin{theoremEnd}{theorem}[Equivalence Relation]\label{thm:greedy-bisim-iff-greedy-bisim-equiv}
    Given two states in GKAT coalgebra \(s ∈ S\) and \(u ∈ U\), there exists a greedy bisimulation \(≈\) between \(S\) and \(U\) s.t. \(s ≈ u\), if and only if there exists a greedy bisimulation that is an \emph{equivalence relation} \(≃\) in \(S + U\) s.t. \(s ≃ u\).
\end{theoremEnd}

\begin{proofEnd}
    First, recall that by~\Cref{thm:greedy-bisim-iff-norm-bisim} \(\norm(≈)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\).
    Because \(S ⊑ S + U\) and \(U ⊑ S + U\) and normalization preserves sub-coalgebra (\Cref{thm:norm-sub-coalg}), we have \(\norm(S) ⊑ \norm(S + U)\) and \(\norm(U) ⊑ \norm(S + U)\); and by~\Cref{thm:sub-coalg-preserve-liveness}, \(s ∈ S\) is dead in \(S + U\) if and only if it is dead in \(S\), and similarly for \(u ∈ U\).

    The \(⟹\) direction. 
    Because sub-coalgebra preserves bisimulation (\Cref{thm:sub-coalg-preserve-bisim}) and \(\norm(S) ⊑ \norm(S + U)\) and \(\norm(U) ⊑ \norm(S + U)\), we have \(\norm(≈)\) is a bisimulation between \(\norm(S + U)\) and itself.
    Let \(≡\) to be the maximal bisimulation within \(\norm(S + U)\), then \(≡\) is an equivalence relation~\cite[Corollary 5.6]{rutten_UniversalCoalgebraTheory_2000}, and so is the maximal greedy bisimulation \(≊\) (\Cref{thm:max-bisim-is-max-greedy-bisim}).
    This gives us the following chain of set inclusions:
    \[(s, u) ∈ {≈} = \norm(≈) ⊑ {≡} ⊆ {≊}.\]
    Thus, \(≊\) is an equivalence relation~\cite[Corollary 5.6]{rutten_UniversalCoalgebraTheory_2000} and a greedy bisimulation s.t. \(s ≡ u\).

    The \(⟸\) direction can be shown by considering the maximal greedy bisimulation \(≊\) within \(S + U\) and maximal bisimulation \(≡\) within \(\norm(S + U)\).
    By maximality and the carrier of \(≡\) coincide with \(≊\), we have the following set inclusions:
    \[(s, u) ∈ {≃} ⊑ {≊} = {≡}.\]
    Finally, we reflect \(≡\) through the inclusion homomorphisms \(\norm(S) ⊑ \norm(S + U)\) and \(\norm(U) ⊑ \norm(S + U)\) as in~\Cref{thm:sub-coalg-preserve-bisim} to obtain \({∼} ⊆ \norm(S) × \norm(U)\) and \(s ∼ u\).
    Therefore, \((s, u)\) is also in the greedy bisimulation \({≈} ⊆ \norm(S) × \norm(U)\), generated by \(∼\).
\end{proofEnd}

Before we introduce the algorithm, we will first briefly sketch the liveness-check algorithm. It uses depth-first search to check whether there exists any accepting states in the reachable states of \(s ∈ S\): if there exists an accepting state in \(⟨s⟩_S\), then the algorithm terminates immediately, and returns that \(s\) is live, otherwise it would return all the states in \(⟨s⟩\), and by~\cref{thm:dead-iff-all-reachable-dead}, all the states in \(⟨s⟩\) are dead. Note that we use depth-first search here for its simplicity, other search algorithms will also be sound.

We cache all the known dead states from the previous searches; whether a state \(s\) is in this cache can be checked by the function call \Call{knownDead\(_S\)}{$s$}. 
On the other hand, the function \Call{isDead\(_S\)}{$s$} will first check if \(s\) is known to be dead, and invoke the search algorithm in the coalgebra \(⟨s⟩\) if \(s\) is not in the cached dead states.
Because the non-symbolic version of liveness checking is similar to the symbolic version, we only present the symbolic version of this algorithm in~\cref{lst:dead-state-detection}.

\begin{algorithm*}
    \caption{On-the-fly bisimulation algorithm}\label{alg:bisim}
    \begin{algorithmic}
        \Function{equiv}{$s ∈ S, u ∈ U$}
        \If {\Call{eq}{$s, u$}} {\Return true} \EndIf
        \If {\Call{knownDead\(_S\)}{$s$}} 
            \State\Return \Call{isDead\(_U\)}{$u$}
        \EndIf
        \If {\Call{knownDead\(_U\)}{$u$}} 
            \State\Return \Call{isDead\(_S\)}{$s$}
        \EndIf
        \State\Return \(∀ α ∈ \At\) \Comment{conditions in~\Cref{thm:recursive-construction}}
        \\\vspace{5px}
        \(\qquad\begin{aligned}
            & s \transAcc{S}{α} \text{ iff } u \transAcc{U}{α} \mathrel{\&\!\&}\\  
            & s \transRej{S}{α} \text{ and } u \transvia{α ∣ p}_U u' \text{ implies } \Call{isDead\(_U\)}{u'} \mathrel{\&\!\&}\\  
            & s \transvia{α ∣ p}_S s' \text{ and } u \transRej{U}{α} \text{ implies } \Call{isDead\(_U\)}{s'} \mathrel{\&\!\&}\\  
            & s \transvia{α ∣ p}_S s' \text{ and } u \transvia{α ∣ p}_U u' \text{ implies } \Call{union}{s, u}; \Call{equiv}{s', u'} \mathrel{\&\!\&}\\
            & s \transvia{α ∣ p}_S s' \text{ and } u \transvia{α ∣ q}_U u' \text{ and } p ≠ q \text{ implies } \Call{isDead\(_S\)}{s'} \text{ and } \Call{isDead\(_U\)}{u'}
        \end{aligned}\)
        \EndFunction
    \end{algorithmic}
\end{algorithm*}

Finally, we present our equivalence-checking algorithm as~\Cref{alg:bisim} which consists of recursively checking the conditions in~\Cref{thm:recursive-construction} with two optimization: early termination with known dead states, and the use of union-find data structure; both of which we will justify later. 
In the algorithm, ``implies'', ``iff'', ``and'' are logical operators that take booleans (i.e. true or false) and return a boolean. We use \(\&\!\&\), instead of ``and'', to separate the conditions for clarity; logically \(\&\!\&\) is the same function as ``and'', but syntactically \(\&\!\&\) binds less tightly than ``and''.

Intuitively, the algorithm returns true if and only if a greedy bisimulation \(≈\) can be constructed s.t. \(s ≈ u\).
Note the existence of such \(≈\) is not only equivalent to the trace equivalence of \(s\) and \(u\) (\Cref{thm:greedy-bisim-correctness}), but is also equivalent to \(s ≊ u\), where \({≊} ⊆ S × U\) is the maximal greedy bisimulation.
Our algorithm deviates from the ideal scenario by implementing two algorithm mentioned above. 
First optimization is to terminate early when one state is known to be dead. 
If the other state is live, then the algorithm return false, because \(s ≈ u\) implies \(s\) and \(u\) have the same liveness (\Cref{thm:greedy-bisim-perserve-liveness}); if both input state are dead, because all dead state pairs are in \(≊\) (\Cref{thm:bisim-between-dead,thm:max-bisim-is-max-greedy-bisim}).
Then, we use a union-find data structure to organize the explored states into equivalence classes, which is justified by finding a equivalence relation (\Cref{thm:greedy-bisim-iff-greedy-bisim-equiv}).

Concerning the complexity, our algorithm has similar worst case complexity as the original algorithm~\cite{smolka_GuardedKleeneAlgebra_2020}.
In particular, when deciding the trace equivalence of two states \(s ∈ S\) and \(u ∈ U\), the original algorithm requires one pass of \(⟨s⟩\) and \(⟨u⟩\) to normalize them, and then visits each pair of states in \(⟨s⟩ × ⟨u⟩\) at most once during bisimulation. This means that the algorithm will visit at most \(|⟨s⟩ + ⟨u⟩ + ⟨s⟩ × ⟨u⟩|\) number of states in total.

Our equivalence-checking algorithm attempts to find a bisimulation first, which visits each pair in \(⟨s⟩ × ⟨u⟩\) at most once. When a mismatch is found, the liveness-checking algorithm is invoked.
Crucially, our algorithm satisfies the following property: if the liveness-checking algorithm finds a live state, the entire equivalence-checking algorithm will halt and return false.
Thus, by caching all the known dead states, the liveness-checking only visits states in \(⟨s⟩\) and \(⟨u⟩\) at most once.
Therefore, the worst case of our algorithm is also \(|⟨s⟩ × ⟨u⟩ + ⟨s⟩ + ⟨u⟩|\).

Although both algorithms have similar worst case complexity, the on-the-fly algorithm will almost always out-perform the original algorithm. 
This is because the on-the-fly algorithm only invokes liveness checking when necessary. 
In the extreme case when the two input states are infinite-trace equivalent, the on-the-fly algorithm can even skip liveness checking entirely.


\section{Symbolic Coalgebra and Algorithm}\label{sec:symb-gkat-greedy-bisim}

Although \Cref{alg:bisim} is on-the-fly, it still uses GKAT coalgebra, which contains exponentially many transitions with respect to the number of primitive tests \(|T|\).
Concretely, the transition function \(δ: S → \At_T → \{\accept, \reject\} + S × K\) requires computing the transition result for each \emph{atom}, and the number of atoms \(\At_T ≅ 2^{T}\) is exponential to the size of primitive tests in \(T\).
This is also why several GKAT-related complexity results only consider constant sized \(T\).

Symbolic GKAT coalgebra, instead of computing the behavior of each atom individually, groups atoms into boolean expressions. 
This optimization leads to space-efficient coalgebras and an equivalence checking algorithm that can make use of off-the-shelf SAT solvers.
Specifically, given a set of primitive actions \(K\) and primitive tests \(T\), a \emph{symbolic GKAT coalgebra} \(Ŝ ≜ (S, ϵ̂, δ̂)\) consists of a state set \(S\), an accepting function \(ϵ̂\), and a transition function \(δ̂\):
\begin{mathpar}
    ϵ̂: S → \mathscr{P}(\BExp_T) \and
    δ̂: S → \mathscr{P}(\BExp_T × S × K).
\end{mathpar}
This coalgebra is required to satisfy the \emph{disjointedness condition}, i.e. for all states \(s ∈ S\), the boolean expressions that \(s\) accepts \(ϵ̂(s)\) and the boolean expressions that enables \(s\) to transition \(\{b ∣ ∃ (b, s', p) ∈ δ(s)\}\) are disjoint.
Formally, the disjointedness condition states the conjunction of any two distinct expressions from the set \(ϵ̂(s) ∪ \{b ∣ ∃ (b, s', p) ∈ δ(s)\}\) is equivalent to \(0\) under Boolean Algebra.
Similar to the non-symbolic case, we use \(s \transAcc{Ŝ}{b}\) to indicate \(b ∈ ϵ̂(s)\) and \(s \transvia{b ∣ p}_{Ŝ} s'\) to indicate \((b, p, s') ∈ δ̂(s)\).

We name \(ϵ̂\) the accepting function because intuitively a state \(s\) accepts an atom \(α\) when there exists a \(b ∈ ϵ̂(s)\), s.t. \(α ≤ b\); similarly, \(δ̂\) is called the transition function because a state \(s\) transitions to \(s'\) via atom \(α\) while executing \(p\) when there exists \((b, s', p) ∈ δ̂(s)\) and \(α ≤ b\).
With the above intuition in mind, a symbolic GKAT coalgebra \(Ŝ ≜ (S, ϵ̂, δ̂)\) can be lowered into a GKAT coalgebra \(S ≜ (S, δ)\) in the following manner:
\begin{align}\label{cons:lowering}
δ(s, α) ≜ \begin{cases}
    \accept & ∃ b ∈ ϵ̂(s), α ≤ b \\[5px]
    (s', p) & 
        \begin{aligned}
            & ∃ b ∈ \BExp_T, α ≤ b \\
            & \text{ and } δ̂(s, b) = (s', p)  
        \end{aligned}\\[5px]
    \reject & \text{otherwise}
\end{cases}
\end{align}
The function \(δ\) is well-defined, i.e. exactly one clause can be satisfied for any \(s ∈ S\) and \(α ∈ \At\), because of the disjointedness condition.
We usually use \(S\) to denote the lowering of \(Ŝ\); and the semantics of a state \(s ∈ Ŝ\) is defined as its semantics in the lowering \(⟦s⟧_{Ŝ} ≜ ⟦s⟧_{S}.\)

We will then use \(ρ̂(s): \BExp_T\) to represent all the atoms that the state \(s\) rejects in the lowering, and \(ρ̂(s)\) is computed as follows:
\begin{align*}
    ρ̂(s) ≜ ⋀ \{\overline{b} ∣{}
        & ∃ s' ∈ S, p ∈ K, (b, s', p) ∈ δ̂(s) \\
        & \text{ or } b ∈ ϵ̂(s)\}.
\end{align*}

\begin{remark}[Canonicity]\label{rem:canonicity}
    Symbolic GKAT coalgebra is not canonical, i.e. there exists two different symbolic GKAT coalgebra with the same lowering, consider the state set \(S ≜ \{s\}\):
    \begin{align*}
        {δ̂}₁(s) & ≜ \{b ↦ (s, p), \overline{b} ↦ (s, p)\} \\
        {δ̂}₂(s) & ≜ \{⊤ ↦ (s, p)\},
    \end{align*} 
    and both \(ϵ̂₁, ϵ̂₂\) return constant \(0\).
    These two symbolic GKAT coalgebra \(Ŝ₁ ≜ (S, δ̂₁, ϵ̂₁)\) and \(Ŝ₂ ≜ (S, δ̂₂, ϵ̂₂)\) have the same lowering and semantics, yet, they are different.
    It is possible to construct symbolic representations that satisfy canonicity, yet we opt to use our current representation for ease of construction and computational efficiency.
\end{remark}

\begin{theoremEnd}{theorem}[Functoriality]\label{thm:lowering-functor}
    The lowering operation is a functor, every symbolic GKAT coalgebra homomorphism \(h: Ŝ → Û\), is also a GKAT coalgebra homomorphism \(h: S → U\).
\end{theoremEnd}

% \begin{proofEnd}
%     Since \(Ŝ\) and \(Û\) have the same states as their lowering, therefore \(h: S → U\) is indeed a function, then we only need to verify the homomorphism condition on \(h\).
%     TODO: finish.
% \end{proofEnd}

Functoriality states that a homomorphism on two symbolic coalgebras induces a homomorphism of on their lowering; similarly, a bisimulation on symbolic GKAT coalgebra also induces a bisimulation on their lowering.
However, the converse is not true, precisely because of the canonicity problem noted in~\Cref{rem:canonicity}: take the \(Ŝ₁\) and \(Ŝ₂\) in~\Cref{rem:canonicity}, because they have the same lowering, therefore the identity homomorphism is a homomorphism on their lowerings, but there is no homomorphism from \(Ŝ₁\) to \(Ŝ₂\).

We can then define the symbolic equivalence algorithm, with its correctness stated in~\cref{thm:recursive-construction}.

\begin{theoremEnd}{theorem}[Symbolic Recursive Construction]\label{thm:symb-recursive-construction}
    Given two symbolic GKAT coalgebra \(Ŝ = (S, ϵ̂_S, δ̂_S)\) and \(Û = (U, ϵ̂_U, δ̂_U)\) and two states \(s ∈ S\) and \(u ∈ U\), \(s\) and \(u\) are trace equivalent \(⟦s⟧_{Ŝ} = ⟦u⟧_{Û}\), if and only there exists a relation \({≈} ⊆ Ŝ × Û\) s.t. the following holds:
    \begin{itemize}
        \item \(⋁ ϵ̂_S(s) ≡ ⋁ ϵ̂_U(u)\); 
        \item if \(s \transvia{b ∣ p}_{Ŝ} s'\) and \(b ∧ ρ̂_U(u) ≢ 0\), then \(s'\) is dead;
        \item if \(u \transvia{c ∣ p}_{Û} u'\) and \(ρ̂_S(s) ∧ c ≢ 0\), then \(u'\) is dead;
        \item if \(s \transvia{b ∣ p}_{Ŝ} s'\), \(u \transvia{c ∣ p}_{Û} u'\), and \(b ∧ c ≢ 0\), then \(s' ≈ u'\); 
        \item if \(s \transvia{b ∣ p}_{Ŝ} s'\), \(u \transvia{c ∣ q}_{Û} u'\), \(p ≠ q\), and \(b ∧ c ≢ 0\), then \(s'\) and \(u'\) are both dead.
    \end{itemize}
\end{theoremEnd}

\begin{proofEnd}
    Reduces to~\Cref{thm:recursive-construction} i.e. all the above condition holds if and only if all the condition in~\Cref{thm:recursive-construction} holds in the lowering.
\end{proofEnd}

\begin{algorithm*}
    \caption{Symbolic On-the-fly Bisimulation Algorithm}\label{alg:symb-bisim}
    \begin{algorithmic}
        \Function{equiv}{$s ∈ S, u ∈ U$}
        \If {\Call{eq}{$s, u$}} {\Return true}
        \ElsIf {\Call{knownDead\(_S\)}{$s$}} {\Return \Call{isDead\(_U\)}{$u$}} 
        \ElsIf {\Call{knownDead\(_U\)}{$u$}} {\Return \Call{isDead\(_S\)}{$s$}} 
        \Else {}
        \Return {
            \Comment{conditions of ~\Cref{thm:symb-recursive-construction}}
            \\\vspace{5px}
            \(\qquad
            \begin{aligned}
                & ⋁ ϵ̂_S(s) ≡ ⋁ ϵ̂_U(u) \mathrel{\&\!\&} \\  
                & s \transvia{b ∣ p} s' \text{ and } (b ∧ ρ̂_U(u)) ≢ 0 \text{ implies \Call{isDead$_S$}{$s'$}} \mathrel{\&\!\&}\\
                & u \transvia{b ∣ p} u' \text{ and } (ρ̂_S(s) ∧ b) ≢ 0 \text{ implies \Call{isDead$_U$}{$u'$}} \mathrel{\&\!\&}\\
                & s \transvia{b ∣ p} s' \text{ and } u \transvia{c ∣ p} u' \text{ implies } \text{\Call{Union}{$s$, $u$}}; \text{\Call{Equiv}{$s', u'$}} \mathrel{\&\!\&}\\
                & s \transvia{b ∣ p} s' \text{ and } u \transvia{c ∣ q} u' \text{ and } p ≠ q \text{ implies \Call{isDead}{$s'$} and \Call{isDead}{$u'$}}
            \end{aligned}\)
        }
        \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm*}

Similar to the non-symbolic version of the algorithm, \Cref{alg:symb-bisim} first checks if either of the input states is dead, then recursively checks all the conditions in~\Cref{thm:symb-recursive-construction}, and organize the explored state in equivalence classes for efficiency.
The operator ``and'', ``implies'', and \(\mathrel{\&\!\&}\) denotes the corresponding logical operator as in~\Cref{alg:bisim}.
The symbolic version of the liveness checking algorithm is also based on a graph search algorithm on all the reachable state where we try to identify an accepting state by checking whether \(⋁ ϵ̂(s) ≡ 0\).
The only deviation from the non-symbolic case is that we do not follow empty transitions in the symbolic case, i.e. if \(δ̂(s) ≜ (b, s', p)\) and \(b ≡ 0\), we will not search \(s'\) to check the liveness of \(s'\).
In~\Cref{sec:optimization-implementation}, we will outline a simple modification for the symbolic GKAT coalgebra generation algorithm in~\Cref{sec:symb-gkat-construction} to avoid generating empty transitions, allowing us to skip the emptiness check when deciding whether a state is dead or not.
% TODO: I think we need to make sure that we are presenting the dead state checking algorithm ignoring the empty transition.
% After which we can comment it here, to claim that this is for generality.


\section{Symbolic GKAT Coalgebra Construction}\label{sec:symb-gkat-construction}

The final piece of the puzzle is to convert any GKAT expression into an \emph{equivalent} state in some symbolic GKAT coalgebra. 
This goal can be achieved by lifting existing constructions like derivatives and Thompson's construction~\cite{schmid_GuardedKleeneAlgebra_2021,smolka_GuardedKleeneAlgebra_2020} to the symbolic setting.
Then the correctness of the non-symbolic construction can be used to show the correctness of the symbolic construction i.e. we will prove that the lowering of symbolic derivatives will yield the conventional derivative.
Other important properties, like finiteness of derivative coalgebra and the correctness of the Thompson's construction can be established using a symbolic GKAT coalgebra homomorphism from the Thompson's construction to the derivative.

\begin{figure*}
    \begin{mathpar}
        \inferrule[]{\\}
        {p \transvia{1 ∣ p}_{D̂} 1} \and  
        \inferrule[]{\\}
        {b \transAcc{D̂}{b}} \and  
        \inferrule[]
        {e \transvia{c ∣ p}_{D̂} e'}
        {e +_b f \transvia{b ∧ c ∣ p}_{D̂} e'} 
        \and
        \inferrule[]
        {e \transAcc{D̂}{c}}
        {e +_b f \transAcc{D̂}{b ∧ c}}
        \and
        \inferrule[]
        {f \transvia{c ∣ p}_{D̂} f'}
        {e +_b f \transvia{\overline{b} ∧ c ∣ p}_{D̂} f'}
        \and
        \inferrule[]
        {f \transAcc{D̂}{c}}
        {e +_b f \transAcc{D̂}{\overline{b} ∧ c}}
        \and
        \inferrule[]
        {e \transAcc{D̂}{b} \\ f \transAcc{D̂}{c}}
        {e; f \transAcc{D̂}{b ∧ c}}
        \and 
        \inferrule[]
        {e \transvia{b ∣ p}_{D̂} e'}
        {e; f \transvia{b ∣ p}_{D̂} e'; f}
        \and
        \inferrule[]
        {e \transAcc{D̂}{b} \\ f \transvia{c ∣ p}_{D̂} f'}
        {e; f \transvia{b ∧ c ∣ p}_{D̂} f'}
        \and  
        \inferrule[]
        {\\}
        {e^{(b)} \transAcc{D̂}{\overline{b}}}  
        \and  
        \inferrule[]
        {e \transvia{c ∣ p} e'}
        {e^{(b)} \transvia{b ∧ c ∣ p}_{D̂} e'; e^{(b)}}
    \end{mathpar}
    \caption{Symbolic Derivative Coalgebra \(D̂\)}\label{fig:derivatives-rules}
\end{figure*}

The symbolic derivative coalgebra \(D̂\), with expressions as states, is the symbolic GKAT coalgebra generated by the rules in~\Cref{fig:derivatives-rules} i.e. a transition is in \(D̂\) if and only if it is derivable.
Our symbolic derivative resembles the GKAT derivative by~\citeauthor{schmid_GuardedKleeneAlgebra_2021}~\cite{schmid_GuardedKleeneAlgebra_2021}.
This similarity is no coincidence, as our definition exactly lowers to the definition of theirs.
This fact can be proven by case analysis on the shape of the source expression, and forms a basis for our correctness argument.
\begin{theoremEnd}{theorem}[Correctness]\label{thm:derivative-correctness}
    The lowering of \(D̂\), denoted \(D\), is exactly the derivative defined by~\citeauthor{schmid_GuardedKleeneAlgebra_2021}~\cite{schmid_GuardedKleeneAlgebra_2021}.
    Therefore, the semantics of the expression is equal to the semantics generated by the derivative coalgebra, \(⟦e⟧ = ⟦e⟧_{D} = ⟦e⟧_{D̂}\).
\end{theoremEnd}
% \begin{proofEnd}
%     TODO: unfold the statement.
% \end{proofEnd}


\begin{table*}
    \centering
    \setlength{\extrarowheight}{3px}
    \begin{tabular}{c||c|c|l|l}
        Exp & \(S\) & \(s^*\)  
        & \(ϵ̂(s)\) & \(δ̂(s)\) \\\hline
        \(b\) & \(\{s^*\}\) 
        & \(s^*\)
        & \(\{b\}\) & \(∅\) \\  
        \(p\) & \(\{s^*, s₁\}\) 
        & \(s^*\)
        & \(\begin{cases}
           ∅ & s = s^* \\  
           \{1\} & s = s₁ 
        \end{cases}\) 
        & \(\begin{cases}
            \{(1, s₁, 0)\} & s = s^* \\  
            ∅ & s = s₁
        \end{cases}\)\\  
        \(e₁ +_b e₂\) & \(\{s^*\} + S₁ + S₂\) &
        \(s^*\) &
        \(\begin{cases}
            ⟨\{b\}| ϵ̂₁(s₁^*) ∪ ⟨\{b\}| ϵ̂₂(s₂^*) & s = s^* \\
            ϵ̂₁(s) & s ∈ S₁\\
            ϵ̂₂(s) & s ∈ S₂\\
        \end{cases}\) & 
        \(\begin{cases}
            ⟨\{b\}| δ̂₁(s₁^*) + ⟨\{b\}| δ̂₂(s₂^*) & s = s^* \\
            δ̂₁(s) & s ∈ S₁\\
            δ̂₂(s) & s ∈ S₂\\
        \end{cases}\) \\  
        \(e₁ ; e₂\) & \(S₁ + S₂\) & 
        \(s₁^*\) & 
        \(\begin{cases}
            ⟨ϵ̂₁(s)| ϵ̂₂(s₂^*)& s ∈ S₁ \\  
            ϵ̂₂(s) & s ∈ S₂
        \end{cases}\)& 
        \(\begin{cases}
            δ̂₁(s) + ⟨ϵ̂(s)| δ̂₂(s₂^*) & s ∈ S₁ \\  
            δ̂2(s) & s ∈ S₂
        \end{cases}\) \\  
        \(e₁^{(b)}\) & \(\{s^*\} + S₁\) & 
        \(s^*\) &
        \(\begin{cases}
            \{\overline{b}\} & s = s^*\\
            ⟨\{\overline{b}\}| ϵ̂1(s) & s ∈ S₁
        \end{cases}\) & 
        \(\begin{cases}
            ⟨\{b\}| δ̂₁(s₁^*) & s = s^* \\
            δ₁(s) ∪ ⟨\{b\}|⟨ϵ̂1(s)| δ̂₁(s₁^*) & s ∈ S₁
        \end{cases}\)
    \end{tabular}
    \caption{Symbolic Thompson's Construction}\label{tab:symb-Thompson-construction}
\end{table*}

Similarly, we also lift the Thompson's construction~\cite{smolka_GuardedKleeneAlgebra_2020} to the symbolic setting.
A useful operation for Thompson's construction is the following guard operation, denoted by \(⟨B|\), where \(B\) is a set of boolean expressions: for a transition function \(δ̂\), an accepting function \(ϵ̂\), and a state \(s\),
\begin{align*}
    ⟨B|ϵ̂(s) & ≜ \{b ∧ c ∣ b ∈ B, c ∈ \hat{\epsilon}(s)\}; \\
    ⟨B|δ̂(s) & ≜ \{(b ∧ c, s', p) ∣ b ∈ B, (c, s', p) ∈ \hat{\delta}(s)\}.
\end{align*}
Besides guarding transition and acceptance with conditions for if-statements and while-loops, the guard operator can also be used to simulate uniform continuation. 
Specifically, we can use \(⟨ϵ̂(s)|δ(s')\) to connect all the accepting transition of \(s\) to the dynamic \(δ(s')\).

With these definitions in mind, we can define symbolic Thompson's construction inductively as in~\Cref{tab:symb-Thompson-construction}, where we let \((S₁, ϵ̂₁, δ̂₁)\) and \((S₂, ϵ̂₂, δ̂₂)\) to be results of Thompson's construction for \(e₁\) and \(e₂\) respectively.
In this table, \(S₁ + S₂\) denotes the disjoint union of \(S₁\) and \(S₂\), and for any two transition dynamics \(δ₁(s₁): \mathscr{P}(\BExp × S₁ × K)\) and \(δ₂(s₂): \mathscr{P}(\BExp × S₂ × K)\), we can also compose them in parallel as \(δ₁(s₁) + δ₂(s₂)\):
\begin{align*}
    δ₁(s₁) & + δ₂(s₂): \mathscr{P}(\BExp × (S₁ + S₂) × K) \\*
    δ₁(s₁) & + δ₂(s₂) ≜ \\*
        & \{(b, \mathrm{inj}ₗ(s₁'), p) ∣ (b, s₁', p) ∈ δ₁(s₁)\} ∪ \\*
        & \{(b, \mathrm{inj}ᵣ(s₂'), p) ∣ (b, s₂', p) ∈ δ₂(s₂)\},
\end{align*}
where \(\mathrm{inj}ₗ: S₁ → S₁ + S₂\) and \(\mathrm{inj}ᵣ: S₂ → S₁ + S₂\) are the canonical left/right injection of the coproduct.

Our construction deviates from the original construction~\cite{smolka_GuardedKleeneAlgebra_2020} by using a start state \(s^* ∈ S\) instead of a start dynamics (or pseudo-state).
This choice will make the proof of~\Cref{thm:hom-thompson-derivative} slightly easier. 
However, as explained in~\Cref{sec:optimization-implementation}, our implementation still utilizes start dynamics to avoid unnecessary lookups.

We would like to explore several desirable theoretical properties of both derivatives and Thompson's construction.
Specifically, \emph{correctness}, i.e. the semantics of the ``start state'' of both constructions preserve the trace semantics of the expression; \emph{finiteness}, i.e. the coalgebra generated is always finite, implying the termination of our equivalence algorithm; and finally, \emph{complexity}, i.e. the relationship between the number of reachable states and the size of the input expression, which serves as an estimated complexity of our equivalence checking algorithm.
It turns out, all of these questions can be answered by a homomorphism from symbolic Thompson's construction to the symbolic derivatives.

\begin{theoremEnd}{theorem}\label{thm:hom-thompson-derivative}
    Given any GKAT expression \(e\), the resulting symbolic GKAT coalgebra from Thompson's construction \(Ŝ_e\) have a homomorphism to derivatives \(h: Ŝ_e → D̂\), s.t. for the start state \(s^* ∈ S, h(s^*) = e\).
\end{theoremEnd}

\begin{proofEnd}
    By induction on the structure of \(e\). We will recall that \(h: Ŝ_e → ⟨e⟩_D\) is a symbolic GKAT coalgebra homomorphism when the following two conditions are true: \(s \transAcc{Sₑ}{b}\) if and only if \(h(s) \transAcc{D̂}{b}\); and \(s \transvia{b ∣ p}_{Sₑ} s'\) if and only if \(h(s) \transvia{b ∣ p}_{D̂} h(s')\).

    When \(e ≜ b\) for some tests \(b\), then the function \(h\) is defined as \(\{s^* ↦ b\}\).
    When \(e ≜ p\) for some primitive action \(p\), then the function \(h\) is defined as \(\{s^* ↦ p, * ↦ 1\}\).
    The homomorphism condition can then be verified by unfolding the definition.

    When \(e ≜ e₁ +_b e₂\), by induction hypothesis, we have homomorphisms \(h₁: Ŝ_{e₁} → ⟨e₁⟩_D\) and \(h₂: Ŝ_{e₂} → ⟨e₂⟩_D\).
    Then we define the homomorphism 
    \[h(s) ≜ \begin{cases}
        e₁ +_b e₂ & s = s^* \\  
        h₁(s) & s ∈ Ŝ_{e₁} \\  
        h₂(s) & s ∈ Ŝ_{e₂}
    \end{cases}\]
    We show that \(h\) is a homomorphism. Because \(Ŝₑ\) preserves the transition and acceptance of \(Ŝ_{e₁}\) and \(Ŝ_{e₂}\), then for all \(s ∈ Ŝ_{e₁} ∩ Ŝ_{e}\), we have
    \begin{align*}
        & s \transAcc{Ŝ_{e}}{c} 
        \text{ iff } s \transAcc{Ŝ_{e₁}}{c} \\*
        & \text{ iff } h₁(s) \transAcc{D̂}{c} 
        \text{ iff } h(s) \transAcc{D̂}{c}; \\ 
        & s \transvia{c ∣ p}_{Ŝ_{e}} s' 
        \text{ iff } s \transvia{c ∣ p}_{Ŝ_{e₁}} s' \\*
        & \text{ iff } h₁(s) \transvia{c ∣ p}_{D̂} h₁(s') 
        \text{ iff } h(s) \transvia{c ∣ p}_{D̂} h(s'). 
    \end{align*}
    And similarly for \(s ∈ Ŝ_{e₂} ∩ Ŝ_{e}\).
    So we only need to show the homomorphic condition for the start state \(s^*\):
    \begin{align*}
        & s^* \transAcc{Ŝ_{e}}{c} \\
        \text{iff } &
        (∃ a, b ∧ a = c \text{ and } 
        s₁^* \transAcc{Ŝ_{e₁}}{a}) \\*
        & \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        s₂^* \transAcc{Ŝ_{e₂}}{a}) \\
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        h₁(s₁^*) \transAcc{D̂}{a}) \\*
        & \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        h₂(s₂^*) \transAcc{D̂}{a}) \\  
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        e₁ \transAcc{D̂}{a}) \\*
        & \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        e₂ \transAcc{D̂}{a}) \\  
        \text{iff }& e₁ +_b e₂ \transAcc{D̂}{c}\\
        \text{iff }& h(s^*) \transAcc{D̂}{c}. \\[5px]
        & s^* \transvia{a ∣ p}_{Ŝ_{e}} s' \\
        \text{iff } &
        (∃ a, b ∧ a = c \text{ and } 
        s₁^* \transvia{a ∣ p}_{Ŝ_{e₁}} s') \\*
        & \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        s₂^* \transvia{a ∣ p}_{Ŝ_{e₂}} s') \\
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        h₁(s₁^*) \transvia{a ∣ p}_{D̂} h(s')) \\*
        & \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        h₂(s₂^*) \transvia{a ∣ p}_{D̂} h(s')) \\  
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        e₁ \transvia{a ∣ p}_{D̂} h(s')) \\*
        & \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        e₂ \transvia{a ∣ p}_{D̂} h(s')) \\  
        \text{iff }& e₁ +_b e₂ \transvia{a ∣ p}_{D̂} h(s') \\ 
        \text{iff }& h(s^*) \transvia{a ∣ p}_{D̂} h(s').
    \end{align*}

    When \(e ≜ e₁; e₂\), by induction hypothesis, we have two homomorphisms \(h₁: Ŝ_{e₁} → D̂\) and \(h₂: Ŝ_{e₂} → D̂\).
    We define \(h\) as follows:
    \[h(s) ≜ \begin{cases}
        h₁(s); e₂ & s ∈ Ŝ_{e₁} \\  
        h₂(s) & s ∈ Ŝ_{e₂}
    \end{cases}\]
    Then we can prove that \(h\) is a homomorphism by case analysis on \(s\). 
    First case is that \(s ∈ Ŝ_{e₁}\):
    \begin{align*}
        & s \transAcc{Ŝ_{e}}{c} \\
        \text{ iff } & 
            ∃ a, b, a ∧ b = c, 
                s \transAcc{Ŝ_{e₁}}{a} 
                \text{ and } 
                s₂^* \transAcc{Ŝ_{e₂}}{b} \\
        \text{ iff } & 
            ∃ a, b, a ∧ b = c, 
                h₁(s) \transAcc{D̂}{a} 
                \text{ and } 
                f \transAcc{D̂}{b} \\
        \text{ iff } & h₁(s); f \transAcc{D̂}{c} 
        \text{ iff } h(s) \transAcc{D̂}{c}.\\[5px]
        & s \transvia{c ∣ p}_{S_{e}} s' \\
        \text{ iff } & 
            \begin{aligned}[t]
                (∃ a, b, a ∧ b = c
                & \text{ and }
                s \transAcc{Ŝ_{e₁}}{a} \\*
                & \text{ and }
                s₂^* \transvia{b ∣ p}_{Ŝ_{e₂}} s') 
            \end{aligned} \\*
            & \text{ or }
            (s \transvia{c ∣ p}_{Ŝ_{e₁}} s') \\  
        \text{ iff } & 
            \begin{aligned}[t]
                (∃ a, b, a ∧ b = c
                & \text{ and }
                h₁(s) \transAcc{D̂}{a} \\*
                &\text{ and }
                e₂ \transvia{b ∣ p}_{D̂} h₂(s'))
            \end{aligned} \\*
            & \text{ or }
            (h₁(s) \transvia{c ∣ p}_{D̂} h₁(s')) \\
        \text{ iff } & h₁(s) \transvia{c ∣ p} h(s') 
        \text{ iff } h(s) \transvia{c ∣ p} h(s').
    \end{align*}
    The case where \(s₂ ∈ Ŝ_{e₂}\) is straightforward, as \(Ŝ_{e}\) preserves the transitions of \(Ŝ_{e₂}\):
    \begin{align*}
        & s \transAcc{Ŝ_{e}}{c} 
        \text{ iff } s \transAcc{Ŝ_{e₂}}{c} \\*
        & \text{ iff } h₂(s) \transAcc{D̂}{c} 
        \text{ iff } h(s) \transAcc{D̂}{c}, \\ 
        & s \transvia{c ∣ p}_{Ŝ_{e}} s' 
        \text{ iff } s \transvia{c ∣ p}_{Ŝ_{e₂}} s' \\*
        & \text{ iff } h₂(s) \transvia{c ∣ p}_{D̂} h₂(s') 
        \text{ iff } h(s) \transvia{c ∣ p}_{D̂} h(s').
    \end{align*}
    

    When \(e ≜ {e₁}^{(b)}\), by induction hypothesis, we have a homomorphism \(h₁: Ŝ_{e₁} → D̂\); the homomorphism \(h\) can be defined as follows: 
    \[h(s) ≜ \begin{cases}
        {e₁}^{(b)} & s ≜ s^* \\  
        h₁(s); e₁^{(b)} & s ∈ Ŝ_{e₁}
    \end{cases}\]
    We prove the homomorphism condition by case analysis on \(s\). First case is that \(s = s^*\), then:
    \begin{align*}
        (s^* \transAcc{Ŝ_e}{c})
        \text{ iff } & (s^* \transAcc{Ŝ_e}{c} \text{ and } c = \overline{b}) \\*
        \text{ iff } & ({e₁}^{(b)} \transAcc{D̂}{c} \text{ and } c = \overline{b}) \\*
        \text{ iff } & (h(s^*) \transAcc{D̂}{c}); \\
        (s^* \transvia{c ∣ p}_{Ŝ_e} s')
        \text{ iff } & (∃ a, b ∧ a = c \text{ and } s₁^* \transvia{a ∣ p}_{Ŝ_{e₁}} s') \\
        \text{ iff } & (∃ a, b ∧ a = c \text{ and } e₁ \transvia{a ∣ p}_{D̂} h₁(s')) \\ 
        \text{ iff } & {e₁}^{(b)} \transvia{a ∣ p}_{D̂} h₁(s') 
        \text{ iff } h(s^*) \transvia{a ∣ p}_{D̂} h(s')
    \end{align*}
    The second case is when \(s ∈ Ŝ_{e₁}\), then:
    \begin{align*}
        & s \transAcc{Ŝ_e}{c}\\
        \text{ iff } & (∃ a, \overline{b} ∧ a = c \text{ and } s \transAcc{Ŝ_{e₁}}{a}) \\  
        \text{ iff } & (∃ a, \overline{b} ∧ a = c \text{ and } h₁(s) \transAcc{D̂}{a}) \\
        \text{ iff } & h₁(s);e₁^{(b)} \transAcc{D̂}{c} 
        \text{ iff } h(s) \transAcc{D̂}{c} \\[5px]
        & s \transvia{c ∣ p}_{Ŝ_e} s' \\
        \text{ iff } & 
            (s \transvia{c ∣ p}_{Ŝ_{e₁}} s' \\
            & \text{ or } 
            \begin{aligned}[t]
                ∃ a₁, a₂, {}
                & b ∧ a₁ ∧ a₂ = c \\*
                & \text{ and } 
                s \transAcc{Ŝ_{e₁}}{a₁} \\*
                & \text{ and } 
                s₁^* \transvia{a₂ ∣ p}_{Ŝ_{e₁}} s')
            \end{aligned} \\
        \text{ iff } & 
            (h₁(s) \transvia{c ∣ p}_{D̂} h₁(s') \\*
            & \text{ or } 
            \begin{aligned}[t]
                ∃ a₁, a₂, &{} b ∧ a₁ ∧ a₂ = c \\*
                & \text{ and } 
                h₁(s) \transAcc{D̂}{a₁} \\*
                & \text{ and } 
                e₁ \transvia{a₂ ∣ p}_{D̂} h₁(s')) 
            \end{aligned}\\
        \text{ iff } & 
            (h₁(s) \transvia{c ∣ p}_{D̂} h₁(s')  \\
            & \text{ or } 
            \begin{aligned}[t]
                ∃ a₁, a₂, &{} b ∧ a₁ ∧ a₂ = c \\
                & \text{ and } 
                h₁(s) \transAcc{D̂}{a₁} \\
                &{} \text{ and } 
                e₁^{(b)} \transvia{b ∧ a₂ ∣ p}_{D̂} h₁(s');e₁^{(b)}) \\ 
            \end{aligned}\\
        \text{ iff } &
            (h₁(s); e₁^{(b)} \transvia{c ∣ p}_{D̂} h₁(s'); e₁^{(b)}) \\
        \text{ iff } & h(s) \transvia{c ∣ p}_{D̂} h(s').
        \qedhere
    \end{align*}
\end{proofEnd}

\Cref{thm:hom-thompson-derivative} can be proven by unfolding the definition, and implies several properties we desire. 
One of the more obvious ones is the semantic equivalence of the start state in the Thompson's construction and the expression in the derivative coalgebra. 
% TODO: what is "in derivative"?

\begin{theoremEnd}{corollary}[Correctness]
    Given any expression \(e\) and its Thompson's coalgebra \(Ŝ_{e}\) with the start state \(s^* ∈ Ŝ_{e}\), then the semantics of the start state is equivalent to the semantics of \(e\): \(⟦s^*⟧_{Ŝ_{e}} = ⟦e⟧.\)
\end{theoremEnd}

\begin{proofEnd}
    By functoriality of lowering~\Cref{thm:lowering-functor}, then \(h: S_{e} → D\) is a homomorphism on their lowerings. 
    Because homomorphism preserves semantics (\Cref{thm:hom-preserves-semantics}) \(⟦s^*⟧^{ω}_{S_{e}} = ⟦h(s^*)⟧^{ω}_{D} = ⟦e⟧^{ω}_{D}\), where \(⟦-⟧^{ω}\) is the infinite trace semantics i.e. the unique map into the final GKAT coalgebra \(\mathscr{G}_ω\).

    Finally, because infinite trace equivalence implies finite trace equivalence (\Cref{thm:inf-trace-equiv-implies-fin-trace-equiv}) and the correctness of derivative (\Cref{thm:derivative-correctness}), we obtain the following chain of equalities:
    \(⟦s^*⟧_{S_{e}} = ⟦e⟧_{D} = ⟦e⟧\).
\end{proofEnd}

A not so obvious consequence of the homomorphism in~\Cref{thm:hom-thompson-derivative} is the complexity of the algorithm based on derivatives.
Our bisimulation algorithm (\Cref{alg:symb-bisim}) only explores the principle sub-coalgebra of the start state, i.e. \(s^*\) in the Thompson's construction \(Ŝ_{e}\) or \(e\) in the derivative \(D̂\); thus, deducing upper bounds on the size of the principle sub-coalgebras \(⟨s^*⟩_{Ŝ_{e}}\) and \(⟨e⟩_{D̂}\) is crucial to our complexity analysis.
An upper bound on \(⟨s^*⟩_{Ŝ_{e}}\) is easy to obtain, as the size of \(Ŝ_{e}\), which subsumes the states of \(⟨s^*⟩_{Ŝ_{e}}\), is linear to the size of expression \(e\); therefore \(⟨s^*⟩_{Ŝ_{e}}\) is at most linear to the size of the expression \(e\).
On the other hand the size of \(⟨e⟩_{D̂}\) can, again, be derived from the homomorphism in~\Cref{thm:hom-thompson-derivative}.

\begin{theoremEnd}{corollary}\label{thm:suj-hom-thompson-derivative}
    There exists a surjective homomorphism \(h': ⟨s^*⟩_{Ŝ_{e}} → ⟨e⟩_{D̂}\). 
    Because the size of \(⟨s^*⟩_{Ŝ_{e}}\) is linear to \(e\), the size of \(⟨e⟩_{D̂}\) is at most linear to the size of expression \(e\).
\end{theoremEnd}

\begin{proofEnd}
    We define \(h'\) to be point-wise equal to \(h\), i.e. \(h'(s) ≜ h(s)\), i.e. \(h'\) is \(h\) restricted on the domain \(⟨s^*⟩_{Ŝ_{e}}\). 
    We need to show that \(h'\) is well-defined and surjective, which is a consequence of homomorphic image preserves principle sub-coalgebra (\Cref{thm:homo-img-preserve-principle-sub-coalg}): \(h(⟨s^*⟩_{Ŝ_{e}}) = ⟨h(s)⟩_{D̂} = ⟨e⟩_{D̂}.\)
    In other words, the image of \(h\) on \(⟨s^*⟩_{Ŝ_{e}}\) is equal to \(⟨e⟩_{D̂}\); thus, because \(h'\) the restriction of \(h\) on \(⟨s^*⟩_{Ŝ_{e}}\), the range of \(h'\) contains its codomain \(⟨e⟩_{D̂}\), showing that \(h'\) is surjective.
\end{proofEnd}

Because of the surjectivity of \(h'\), \(⟨s^*⟩_{Ŝ_{e}}\) has no fewer states than \(⟨e⟩_{D̂}\).
Because the number of states in \(⟨s^*⟩_{Ŝ_{e}}\) is bounded by \(Ŝ_{e}\), and \(|Ŝ_{e}|\) is linear to the size of the input expression, the number of states in \(⟨e⟩_{D̂}\) is at most linear to the size of the expression \(e\). 

% Although the size of the coalgebra generated by the derivative is smaller than Thompson's construction, the states in the derivative coalgebra \(D̂\) are expressions which can be more expensive to store. Computing the next transition of the derivative coalgebra can also take longer. On the other hand, the states of Thompson's construction only requires inductively going through the expression once to construct the entire coalgebra \(Ŝ_{e}\), and its states can be represented by more efficient constructs, like integers. In practice, we utilize techniques such as hashconsing and memoization to address the memory and time usage problems of derivative construction respectively.

\section{Implementation and Evaluation}\label{sec:implementation}
We implement our symbolic on-the-fly bisimulation algorithm~(\Cref{alg:symb-bisim}) in Rust along with derivative-based~(\Cref{fig:derivatives-rules}) and Thompson's-construction-based~(\Cref{tab:symb-Thompson-construction}) algorithms for constructing symbolic GKAT coalgebras. The source code and benchmark suite is freely available in our repository \url{https://anonymous.4open.science/r/rust-gkat-071E}.

\subsection{Optimization}\label{sec:optimization-implementation}
\begin{definition}[blocked transition]
    For any \(s \in S\), a transition \((b, s', p) \in \hat{\delta}_S(s)\) is blocked if \(b \equiv 0\).
\end{definition}

Notice that during bisimulation~(\Cref{alg:symb-bisim}) for some \(s \in S\) and \(u \in U\), if \(s \transvia{b ∣ p} s'\) and \(b\) is equivalent to \(0\), then implications with premises of the form \((b \land \cdot) \not{\equiv} 0\) are trivially satisfied. 
In other words, \textit{blocked} transitions in \(S\) do not contribute to the result of bisimulation. The same observation holds true symmetrically for \(U\). So blocked transitions of a coalgebra can be safely pruned without impacting the result of bisimulation.

While blocked transitions are irrelevant regarding the result of bisimulation, they can negatively impact the performance of bisimulation as the algorithm may perform many unneeded satisfiability checks. To prevent performance degradation due to blocked transitions, we remove them through an \textit{eager-pruning} optimization. Basically, in our coalgebra construction algorithms, \(\hat{\delta}\) is constructed with only transitions \((b, s, p)\) where \(b \not{\equiv} 0\). The coalgebras produced in this manner essentially have their blocked transitions pruned at the time of construction (eager). 

Eager-pruning also improves the efficiency of the Thompson's construction algorithm~(\Cref{tab:symb-Thompson-construction}) significantly. This is due to the fact that eager-pruning can reduce the size of \(\hat{\delta}'\) computed recursively for sub-expressions, which in turn makes computing \(\hat{\delta}\) for the overall expression faster.

Our implementation of Thompson's construction also uses start dynamics to reduces the number of lookups when accessing the dynamics \(\hat{\epsilon}(s^*) \) and \(\hat{\delta}(s^*)\) of start state \(s^*\). 
Concretely, we use two extra fields \(\epsilon^*\) and \(\delta^*\) to track \(\hat{\epsilon}(s^*) \) and \(\hat{\delta}(s^*)\) respectively. 
By tracking \(ϵ^*\) and \(δ^*\), we can remove \(s^*\) from the coalgebra construction, only adding \(s^*\) back as a fresh state after the construction terminates. 
The resulting coalgebra is equivalent \(⟨s^*⟩_{Ŝ_{e}}\) from \Cref{tab:symb-Thompson-construction}.

The final optimization that we perform is collapsing the boolean expression set \(\hat{\epsilon}(s)\) into a single boolean expression. Basically, each \(\hat{\epsilon}(s)\) is represented by the boolean disjunction of its elements. Consider the guard operation defined in \Cref{sec:symb-gkat-construction}.
\begin{align*}
    ⟨\hat{\epsilon_1}(s_1)|\hat{\epsilon_2}(s_2) & ≜ \{b ∧ c ∣ b ∈ \hat{\epsilon_1}(s_1), c ∈ \hat{\epsilon_2}(s_2)\}
\end{align*}
If \(\hat{\epsilon_1}(s_1)\) and \(\hat{\epsilon_2}(s_2)\) are sets of boolean expressions, then the guard operation would produce a set whose size is proportional to \(|\hat{\epsilon_1}(s_1)| \times |\hat{\epsilon_2}(s_2)|\). However, if the collapsed representation is used, \(⟨\hat{\epsilon_1}(s_1)|\hat{\epsilon_2}(s_2)\) will simply be a conjunction of \(\hat{\epsilon_1}(s_1)\) and \(\hat{\epsilon_2}(s_2)\). 
This optimization is indeed sound because of the distributivity of boolean algebra.

\subsection{Evaluation}\label{sec:performance-implementation}
Due to the fact that our symbolic algorithms are generic regarding the choice of solvers for boolean satisfiability, we compare their performance using BDD to using a SAT solver (MiniSat~\cite{een_MINISAT_2004}). Our experiments are performed on a laptop with an Apple M4 Pro CPU and 24 GB RAM. \Cref{tab:benchmark-time,tab:benchmark-memory} present the experimental results of equivalence checking based on derivatives (DV), Thompson's construction (TC) and SymKAT~\cite{pous_SymbolicAlgorithmsLanguage_2015} (SK). Each benchmark consists of 50 expression pairs with properties described by the benchmark's name. For example, expressions in `e250b5p10eq' have approximately 250 primitive actions in total (e250), a maximum boolean expression size of 5 (b5), 10 possible unique primitive tests (p10) and are known to be equivalent (eq). Benchmarks with the suffix `ne' have expression pairs that are known to be non-equivalent. 

\begin{table}
\footnotesize
\centering
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l r r r r r}
    Benchmark & DV$_{\textsf{BDD}}$ & TC$_{\textsf{BDD}}$ & DV$_{\textsf{SAT}}$ & TC$_{\textsf{SAT}}$ & SK \\
    \hline
    e250b5p10ne    & 0.18   & 0.18   & 0.16 & 0.17 &    5.82 \\
    e250b5p10eq    & 0.19   & 0.18   & 0.15 & 0.18 &    2.83 \\
    e500b5p50ne    & 0.21   & 0.21   & 0.21 & 0.22 &   37.28 \\
    e500b5p50eq    & 0.22   & 0.21   & 0.20 & 0.27 &   14.06 \\
    e1000b10p100ne & 0.26   & 0.28   & 0.34 & 0.38 & timeout \\
    e1000b10p100eq & 0.28   & 0.26   & 0.28 & 0.41 &   77.83 \\
    e2000b20p200ne & 1.32   & 2.60   & 0.68 & 0.79 & timeout \\
    e2000b20p200eq & 1.92   & 2.95   & 0.41 & 1.00 & timeout \\
    e3000b30p200ne & 1.57   & 24.86  & 1.30 & 1.60 & timeout \\
    e3000b30p200eq & 10.82  & 22.83  & 0.66 & 1.54 & timeout \\
    degenerate     & 99.48  & 228.16 & 0.24 & 0.30 & timeout
\end{tabular}
\caption{Total Time Usage (seconds)}\label{tab:benchmark-time}
\end{table}
\begin{table}
\footnotesize
\centering
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l r r r r r}
    Benchmark & DV$_{\textsf{BDD}}$ & TC$_{\textsf{BDD}}$ & DV$_{\textsf{SAT}}$ & TC$_{\textsf{SAT}}$ & SK \\
    \hline
    e250b5p10ne    & 15.17  & 14.54   & 7.07  & 7.01  &  114.06 \\
    e250b5p10eq    & 15.69  & 14.69   & 7.06  & 7.29  &  100.48 \\
    e500b5p50ne    & 15.99  & 15.33   & 7.02  & 7.29  &  524.89 \\
    e500b5p50eq    & 16.85  & 15.26   & 6.99  & 7.01  &  546.91 \\
    e1000b10p100ne & 17.51  & 21.31   & 7.78  & 7.50  & timeout \\
    e1000b10p100eq & 19.52  & 17.48   & 8.17  & 7.04  & 5822.66 \\
    e2000b20p200ne & 237.17 & 280.70  & 12.95 & 11.43 & timeout \\
    e2000b20p200eq & 107.61 & 102.18  & 12.76 & 13.25 & timeout \\
    e3000b30p200ne & 112.06 & 1232.24 & 21.00 & 19.30 & timeout \\
    e3000b30p200eq & 245.23 & 228.86  & 21.18 & 17.64 & timeout \\
    degenerate     & 632.70 & 1225.24 & 19.13 & 17.94 & timeout
\end{tabular}
\caption{Peak Memory Usage (megabytes)}\label{tab:benchmark-memory}
\vspace{-1em}
\end{table}

We observe that DV performs better than TC consistently when both algorithms use the same solver backend and all variants of DV and TC perform better than SymKAT. When comparing the solver backends, BDD and SAT perform similarly for smaller benchmarks, however, BDD's time and memory usage increase considerably for larger benchmarks. On the other hand, the performance of SAT is stable for all benchmarks. Furthermore, we have encountered degenerative expression pairs (the `degenerate' benchmark) which greatly reduce BDD performance but can be efficiently checked by SAT. We did not encounter any expression pairs where BDD performed significantly better than SAT.

\section{Related Works and Discussions}

\subsection{Generic Symbolic Techniques}

There are many studies that utilizes symbolic techniques to solve various problems surrounding (extended) regular expressions, and have found a wide range of real-world applications, including but not limited to low level program analysis~\cite{dallapreda_AbstractSymbolicAutomata_2015a}, list comprehension~\cite{saarikivi_FusingEffectfulComprehensions_2017}, constraint solving~\cite{stanford_SymbolicBooleanDerivatives_2021}, HTML decoding, malware fingerprinting, image blurring, location privacy~\cite{veanes_SymbolicFiniteState_2012}, regex processing, and string sanitizer~\cite{veanes_ApplicationsSymbolicFinite_2013}.

Our study, on the other hand,  focus on GKAT and GKAT automata (represented as coalgebra throughout the paper). 
Although previous works on deterministic symbolic transducer~\cite{saarikivi_FusingEffectfulComprehensions_2017,veanes_SymbolicFiniteState_2012} might seem similar to that of GKAT, the automata shape are subtly different, specifically, instead of having accept and rejecting states, a state in GKAT automata can accept or reject based on the input atoms.

Another difference between aforementioned works and ours is that we utilize the coalgebraic theory of GKAT to streamline some of our proofs.
In fact, we are not the first to look at symbolic transition system through the lens of coalgebra, \citeauthor{bonchi_CoalgebraicSymbolicSemantics_2009}~\cite{bonchi_CoalgebraicSymbolicSemantics_2009} have an elegant coalgebra theory surrounding symbolic transition system.
However, instead of defining the symbolic semantics via the coalgebra theory, we opt to use the notion of lowering to connect symbolic GKAT coalgebra and GKAT coalgebra.
This approach allows us to leverage previous correctness proofs like in~\Cref{thm:derivative-correctness}, and also enables a non-symbolic equivalence-checking algorithm as in~\cref{alg:bisim}.
Another notable difference is that our equivalence checking also need to handle normalization, which is a unique property of GKAT not found in general coalgebra or automata.

\subsection{KAT and GKAT}

GKAT is a guarded fragment of Kleene Algebra with Tests (KAT), which enjoys a symbolic algorithm~\cite{pous_SymbolicAlgorithmsLanguage_2015}.
This algorithm by~\citeauthor{pous_SymbolicAlgorithmsLanguage_2015} generalizes the notion of derivatives, and utilizes binary decision diagram (BDD) to form the desired coalgebras.
Although some of our examples leverage BDD to solve equivalence and inequivalence of boolean expressions, our algorithm is not bounded by a particular boolean representation or solvers.
In fact, we have demonstrated that in many scenarios, SAT solvers like MiniSat~\cite{een_MINISAT_2004} can achieve better performance than BDD.

In similar veins, KATch~\cite{moeller_KATchFastSymbolic_2024} is a symbolic solver for NetKAT~\cite{anderson_NetKATSemanticFoundations_2014} based on forwarding decision diagram, and achieved outstanding performance among state-of-the-art tools for reasoning about software-defined networks.
The system of GKAT also stem from the research of the research of software-defined networks~\cite{smolka_ScalableVerificationProbabilistic_2019,smolka_GuardedKleeneAlgebra_2020}, and quickly found applications in probabilistic verification~\cite{ro.zowski_ProbabilisticGuardedKAT_2023}, probabilistic program logic~\cite{gomes_KleeneAlgebraTests_2024}, networks~\cite{wasserstein_GUARDEDNETKATSOUNDNESS_2023}, and control-flow validation~\cite{zhang_CFGKATEfficientValidation_2025}.
It would be interesting to see how our symbolic algorithm can be used to speed up these variants of GKAT.

\printbibliography

\clearpage
\appendix

\section{Detailed Proof}
\printProofs

\begin{onecolumn}
\begin{lstlisting}[
    caption=Dead State Detection Implementation,
    label=lst:dead-state-detection
]
pub type Deriv<B> = Vec<(B, Exp<B>, u64)>;

pub struct Solver<B> {
    // search states
    dead_states: HashSet<Exp<B>>,
    explored: HashSet<Exp<B>>,
    uf_table: HashMap<Exp<B>, UnionFindNode<()>>,
    // caching
    eps_cache: HashMap<Exp<B>, B>,
    drv_cache: HashMap<Exp<B>, Deriv<B>>,
}

impl<B: BExp> Solver<B> {
    #[inline]
    pub fn known_dead(&self, exp: &Exp<B>) -> bool {
        self.dead_states.contains(&exp)
    }

    pub fn is_dead<G: Gkat<B>>(&mut self, gkat: &mut G, exp: &Exp<B>) -> bool {
        let mut stack = Vec::new();
        stack.push(exp.clone());
        self.explored.clear();
        while let Some(exp) = stack.pop() {
            if self.known_dead(&exp) || self.explored.contains(&exp) {
                continue;
            }
            self.explored.insert(exp.clone());
            let eps = self.epsilon(gkat, &exp);
            if gkat.is_false(&eps) {
                for (_, e, _) in self.derivative(gkat, &exp) {
                    stack.push(e);
                }
            } else {
                return false;
            }
        }
        self.dead_states.extend(self.explored.iter().cloned());
        return true;
    }
}
\end{lstlisting}
\end{onecolumn}


\end{document}
% Please compile this document using LuaLaTeX.
% because of the use of unicode-math
% XeLaTeX and PDFLaTeX will result in error.

\newif\iffull\fulltrue
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

% page number
\pagestyle{plain}

% \usepackage{cite}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%%%%% PACKAGES
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[margin=2.5cm]{geometry}

% allow page break in align environment
\allowdisplaybreaks

% Biblatex
\usepackage[style=numeric]{biblatex}
\addbibresource{refs.bib}

% For adding inline comments in the text.
\usepackage[margin=false,inline=true]{fixme}
\FXRegisterAuthor{aaa}{anaaa}{\color{cyan}AAA}
\FXRegisterAuthor{mg}{anmg}{\color{red}MG}
\FXRegisterAuthor{cz}{ancz}{\color{orange}CZ}
% \newcommand{\aaa}[1]{\aaanote{#1}}
% \newcommand{\mg}[1]{\mgnote{#1}}
% \newcommand{\cz}[1]{\cznote{#1}}
\newcommand{\aaa}[1]{}
\newcommand{\mg}[1]{}
\newcommand{\cz}[1]{}

\usepackage{stmaryrd}

\usepackage{stackengine}
\usepackage{mathrsfs}
\usepackage{braket}
\usepackage{annotate-equations}
\usepackage{scalerel}

% graphs
\usepackage{tikz}
\usetikzlibrary{arrows,automata,positioning}

% commutative diagram
\usepackage{tikz-cd}

% ref
\usepackage{hyperref}
\usepackage{cleveref}

% inference rule
\usepackage{mathpartir}
% cross-referencing infer rule
% based on https://tex.stackexchange.com/questions/340788/cross-referencing-inference-rules
\makeatletter
\let\originferrule\inferrule
\DeclareDocumentCommand \inferrule { s O {} m m}{%
  \IfBooleanTF{#1}%
  {%
    \mpr@inferstar[#2]{#3}{#4}%
  }{%
    \mpr@inferrule[#2]{#3}{#4}%
  }%
  \IfValueT{#2}%
  {%
    \my@name@inferrule{#2}%
  }%
}
\NewDocumentCommand \my@name@inferrule { m }{%
  \def\@currentlabelname{\textsc{#1}}%
}
\makeatother

% item spacing
\usepackage{enumitem}

% for code
\usepackage{listings}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{framextopmargin=50pt,frame=bottomline}
\lstset{language=caml, escapeinside={[*}{*]}}

% for algorithm
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
% switch statement for pattern matching
\algnewcommand\algorithmicmatch{\textbf{match}}
\algnewcommand\algorithmicwith{\textbf{with}}
\algnewcommand\algorithmiccase{\textbf{case}}
\algnewcommand\algorithmicdefault{\textbf{default}}
\algnewcommand\Continue{\textbf{continue}}
\algdef{SE}[MATCH]{Match}{EndMatch}[1]{\algorithmicmatch\ #1\ \algorithmicwith}{\algorithmicend\ \algorithmicmatch}%
\algdef{SE}[CASE]{Case}{EndCase}[1]{\algorithmiccase\ #1 \algorithmicthen}{\algorithmicend\ \algorithmiccase}%
\algdef{SE}[DEFAULT]{Default}{EndDefault}{\hskip\algorithmicindent\algorithmicdefault}{\algorithmicend\algorithmicdefault}%
\algtext*{EndMatch}%
\algtext*{EndCase}%
\algtext*{EndDefault}%

% for better table
\usepackage{booktabs}

% subcaption
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption

% unicode math symbols
\usepackage{unicode-math}
% support for hat, overline, underline, vec, and sim combining charactors
\protected\def\afteracc{\directlua{
    local nest = tex.nest[tex.nest.ptr]
    local last = nest.tail
    if not (last and last.id == 18) then
      error'I can only put accents on simple noads.'
    end
    if last.sub or last.sup then
      error'If you want accents on a superscript or subscript, please use braces.'
    end
    local acc = node.new(21, 1)
    acc.nucleus = last.nucleus
    last.nucleus = nil
    local is_bottom = token.scan_keyword'bot' and 'bot_accent' or 'accent'
    acc[is_bottom] = node.new(23)
    acc[is_bottom].fam, acc[is_bottom].char = 0, token.scan_int()
    nest.head = node.insert_after(node.remove(nest.head, last), nil, acc)
    nest.tail = acc
    node.flush_node(last)
  }}
\AtBeginDocument{
\begingroup
  \def\UnicodeMathSymbol#1#2#3#4{%
    \ifx#3\mathaccent
      \def\mytmpmacro{\afteracc#1 }%
      \global\letcharcode#1=\mytmpmacro
      \global\mathcode#1="8000
    \else\ifx#3\mathbotaccentwide
      \def\mytmpmacro{\afteracc bot#1 }%
      \global\letcharcode#1=\mytmpmacro
      \global\mathcode#1="8000
    \fi\fi
  }
  \input{unicode-math-table}
\endgroup
}

% math font, this is needed to render \setminus command
\setmathfont{latinmodern-math}
\setmathfont[range=\setminus]{STIX Two Math}
\setmathfont[range=\similarrightarrow]{STIX Two Math}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}

\newtheorem{definition}{Definition}
\newtheorem*{definition*}{Definition}
\newtheorem{example}[definition]{Example}
\newtheorem{remark}[definition]{Remark}

%%%% Macros %%%%%

% Math?
\newcommand{\true}{\mathrm{true}}
\newcommand{\false}{\mathrm{false}}
\newcommand{\At}{\mathbf{At}}


% operators
\newcommand{\dom}[1]{\mathrm{dom}(#1)}
\newcommand{\cod}[1]{\mathrm{cod}(#1)}
\DeclareMathOperator{\post}{\mathrm{post}}
\newcommand{\reject}{\mathinner{\mathrm{rej}}}
\newcommand{\accept}{\mathinner{\mathrm{acc}}}
\DeclareMathOperator*{\bigplus}{\scalerel*{+}{\sum}}
\newcommand{\clos}[1]{\mathrel{\overline{#1}}}
\DeclareMathOperator{\norm}{\mathrm{norm}}
\DeclareMathOperator{\dead}{\mathrm{dead}}
\DeclareMathOperator{\symb}{\mathrm{symb}}
\DeclareMathOperator{\unsymb}{\symb^{-1}}


% commands 
\newcommand{\command}[1]{{\mathtt{#1}}}
\newcommand{\comAssume}[1]{\command{assume}~#1}
\newcommand{\comITE}[3]{\command{if}~#1~\command{then}~#2~\command{else}~#3}
\newcommand{\comWhile}[2]{\command{while}~#1~\command{do}~#2}

% set of models
\newcommand{\theoryOf}[1]{\ensuremath{\mathsf{#1}}}
\newcommand{\Exp}{\theoryOf{Exp}}
\newcommand{\BExp}{\theoryOf{BExp}}
\DeclareMathOperator{\GS}{\mathrm{GS}}

\newcommand\altxrightarrow[2][0pt]{\mathrel{\ensurestackMath{\stackengine%
  {\dimexpr#1-7.5pt}{\xrightarrow{\phantom{#2}}}{\scriptstyle\!#2\,}%
  {O}{c}{F}{F}{S}}}}
\newcommand{\transvia}[1]{
    \mathrel{\raisebox{-2px}{\(\altxrightarrow[-2px]{#1}\)}}
}
\newcommand{\transAcc}[2]{⇒_{#1} #2}

 

\begin{document}

\title{On-the-fly Algorithms for GKAT Equivalences
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Cheng Zhang\textsuperscript{\textsection}}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{University College London}\\
London, United Kingdom \\
0000-0002-8197-6181}
\and
\IEEEauthorblockN{Qiancheng Fu}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
email address or ORCID}
\and
\IEEEauthorblockN{Hang Ji}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
email address or ORCID}
\and
\IEEEauthorblockN{Ines Santacruz}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
email address or ORCID}
\and
\IEEEauthorblockN{Marco Gaboardi}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
email address or ORCID}
}

\maketitle
\begingroup\renewcommand\thefootnote{\textsection}
\footnotetext{Work largely performed at Boston University}
\endgroup

\begin{abstract}
This document is a model and instructions for \LaTeX.
This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert.
\end{IEEEkeywords}



\section{Introduction}

\paragraph{Notation:} In this paper, we will use un-curried notation to apply curried functions, for example, given a function \(δ: X → Y → Z\), we will write the function applications as \(δ(x): Y → Z\) and \(δ(x, y): Z\).

\section{Background on Coalgebra and GKAT}

\subsection{Concepts in Universal Coalgebra}

In this paper, we will make heavy use of coalgebraic theory, thus it is empirical for us to recall some notions and useful theorems in universal coalgebra.
Given a functor \(F\) on the category of set and functions, a \emph{coalgebra over \(F\)} or \emph{\(F\)-coalgebra} consists of a set \(S\) and a function \(σ_S: S → F(S)\).
We typically call elements in \(S\) the \emph{states} of the coalgebra, and \(σ_S(s)\) the \emph{dynamic} of state \(s\).
We will sometimes use the states \(S\) to denote the coalgebra, when no ambiguity can arise. 

A homomorphism between two \(F\)-coalgebra \(S\) and \(U\) is a map \(h: S → U\) that preserves the function \(σ\); diagrammatically, the following diagram commutes:
\[
    \begin{tikzcd}
        S \ar{r}{h} \ar[swap]{d}{σ_S} & U \ar{d}{σ_U} \\  
        F(S) \ar{r}{F(h)} & F(U)
    \end{tikzcd}    
\]

When we can restrict the homomorphism map into an inclusion map \(i: S' → S\) for \(S' ⊆ S\) then we say that \(S'\) is a \emph{sub-coalgebra} of \(S\), denoted as \(S' ⊑ S\). Specifically, the following diagram commutes when \(S' ⊑ S\):
\[
    \begin{tikzcd}
        S' \ar[hook]{r}{i} \ar[swap]{d}{σ_{S'}} & S \ar{d}{σ_S} \\  
        F(S') \ar[hook]{r}{F(i)} & F(S)
    \end{tikzcd}    
\]
In fact, the function \(σ_{S'}\) is uniquely determined by the states \(S'\)~\cite[Proposition 6.1]{rutten_UniversalCoalgebraTheory_2000}.
Sub-coalgebras are also preserved under homomorphic images and pre-images: 
\begin{lemma}[Theorem 6.3~\cite{rutten_UniversalCoalgebraTheory_2000}]\label{thm:hom-(pre)img-preserve-sub-coalg}
    given a homomorphism \(h: S → U\), and sub-coalgebras \(S' ⊑ S\) and \(U' ⊑ U\), then 
    \[h(S') ⊑ U \text{ and } h^{-1}(U') ⊑ S.\]
\end{lemma}

One particularly important sub-coalgebra of a coalgebra \(S\) is the least sub-coalgebra that contains a state \(s\). 
We will denote this sub-coalgebra as \(⟨s⟩_{S}\), and call it \emph{principle sub-coalgebra} generated by \(s\). 
We sometimes omit the subscript \(S\) when it can be inferred from context or irrelevant.
Intuitively, we usually think of principle sub-coalgebra \(⟨s⟩_S\) as the sub-coalgebra that is formed by all the ``reachable state'' from \(s\).
This coalgebraic characterization of reachable state can allow us to avoid induction on the length of path from \(s\) to a state in \(⟨s⟩_S\).

For all coalgebra \(S\) and a state \(s ∈ S\), principle sub-coalgebra \(⟨s⟩_S\) always exists and is unique, because sub-coalgebra of any coalgebra forms a complete lattice~\cite[theorem 6.4]{rutten_UniversalCoalgebraTheory_2000}; thus taking the meet of all the sub-coalgebra that contains \(s\) will yield \(⟨s⟩_S\).

Similar to sub-coalgebra, principle sub-coalgebra is also preserved under homomorphic image:
\begin{theorem}\label{thm:homo-img-preserve-principle-sub-coalg}
    Homomorphic image preserves principle sub-GKAT coalgebra. Specifically, given a homomorphism \(h: S → U\):
    \[h(⟨s⟩_S) = ⟨h(s)⟩_U\]
\end{theorem}

\begin{proof}
    We will need to show that \(h(⟨s⟩_{S})\) is the smallest sub-GKAT coalgebra of \(S\) that contain \(h(s)\). 
    By definition of image, \(h(⟨s⟩_{S})\) indeed contain \(h(s)\). 
    By \Cref{thm:hom-(pre)img-preserve-sub-coalg}, \(h(⟨s⟩_S) ⊑ U\), i.e. \(h(⟨s⟩_S)\) is a sub-coalgebra of\(U\).
    Finally, take any sub-coalgebra \(U' ⊑ U\) s.t. \(h(s) ∈ U'\): 
    \begin{align*}
        h(s) ∈ U' 
        & ⟹ s ∈ h^{-1}(U') \\  
        & ⟹ ⟨s⟩_S ⊑ h^{-1}(U') & \text{definition of \(⟨s⟩_S\)}\\  
        & ⟹ h(⟨s⟩_S) ⊑ U' & \text{\Cref{thm:hom-(pre)img-preserve-sub-coalg}}
    \end{align*}
    Hence \(h(⟨s⟩_S)\) is the smallest sub-GKAT coalgebra of \(U\) that contains \(h(s)\).
\end{proof}

% bisimulation

A \emph{final coalgebra} \(ℱ\) over a signature \(F\), sometimes called the \emph{behavior} or \emph{semantics} of coalgebras over \(F\), is an \(F\)-coalgebra s.t. for all \(F\)-coalgebra \(S\), there exists a unique homomorphism \(\mathrm{beh}_S: S → ℱ\).

Given two \(F\)-coalgebra \(S\) and \(U\), the \emph{behavioral equivalence} between states in \(S\) and \(U\) can be computed by a notion called \emph{bisimulation}.
A relation \({∼} ⊆ S × U\) is called a \emph{bisimulation relation} if it forms an \(F\)-coalgebra: \[σ_{∼}: {∼} → F(∼),\] 
and its projections \(π₁: {∼} → S\) and \(π₂: {∼} → U\) are both homomorphisms:
\[
    \begin{tikzcd}[column sep=1.25cm]
        S \ar[swap]{d}{σ_S}
            & ∼ \ar[swap]{l}{π₁} \ar{r}{π₂} \ar{d}{σ_{∼}} 
            & U \ar{d}{σ_T}\\  
        F(S) & F(∼) \ar{l}{F(π₁)} \ar[swap]{r}{F(π₂)} & F(T)
    \end{tikzcd}
\]
In a special case, when there exists a homomorphism \(h: S → U\), then we can simply pick \({∼} ⊆ S × U\) to be \(\{(s, h(s)) ∣ s ∈ S\}\), which gives us a bisimulation with the lift \(σ_{∼} ≜ σ_S × σ_U\).

When given a homomorphism \(h: S → U\), we can construct a bisimulation \({∼} ⊆ S × U\), where it relates all the states \(s ∈ S\) with \(h(s) ∈ U\); and each component of the pair will transition individually via their respective transition function \(δ_∼ ≜ δ_S × δ_U\), where \(×\) is the bifunctorial lift of product.
\begin{corollary}\label{thm:hom-preserves-semantics}
    Given a homomorphism \(h: S → U\) between two \(F\)-coalgebra \(S, U\), then all for all states \(s ∈ S\), \(\mathrm{beh}_S(s) = \mathrm{beh}_U(h(s)).\)
\end{corollary}

\subsection{GKAT and Its Coalgebra}

Guarded Kleene Algebra with Tests, or GKAT~\cite{smolka_GuardedKleeneAlgebra_2020}, is a deterministic fragment of Kleene Algebra with Tests. 
The syntax of GKAT over a set of primitive actions \(K\) and a set of primitive tests \(T\) can be defined in two sorts, boolean expressions \(\BExp\) and GKAT expressions \(\Exp\):
\begin{align*}
    a, b, c ∈ \BExp 
        & ≜ 1 ∣ 0 ∣ t ∈ T ∣ b ∧ c ∣ b ∨ c ∣ \overline{b} \\  
    e, f ∈ \Exp 
        & ≜ p ∈ K ∣ b ∈ \BExp ∣ e +_b f ∣ e ; f ∣ e^{(b)} 
\end{align*}
where \(e +_b f\) is the if statement with condition \(b\), \(e;f\) is the sequencing of expression \(e\) and \(f\), and \(e^{(b)}\) is the while loop with body \(e\) and condition \(b\).
We use notation like \(b ≤ c\), \(b ≡ c\), and \(b ≢ c\) for the usual order, equivalence, and inequivalence in Boolean Algebra.
A GKAT expression can be unfolded into a KAT expression in the usual manner~\cite{kozen_KleeneAlgebraTests_1997c}:
\begin{align*}
    e +_b f & ≜ b; e + \overline{b}; f &
    e^{(b)} & ≜ (b; e)^*; \overline{b}.
\end{align*}
Then the semantics of each expression \(⟦e⟧\) can be computed by the semantics of Kleene Algebra with tests~\cite{kozen_KleeneAlgebraTests_1997c}.
An important construct in the semantics is \emph{atoms}, which are conjunctions of all the primitive tests either in its positive or negative form: for \(T ≜ \{t₁, t₂, …, tₙ\}\)
\[\At_T ≜ \{t₁' ∧ t₂' ∧ ⋯ ∧ tₙ' ∣ tᵢ' ∈ \{tᵢ, \overline{tᵢ}\}\}.\]
Follow the conventional notation, we denote an atom using \(α, β\); and we sometimes omit the subscript \(T\) when no confusing can arise.
Alternatively, atoms can also be thought of as truth assignments to each primitive tests, indicating which primitive tests is satisfied in the current program states; and \(α ≤ b\) if and only if the truth assignment represented by \(α\) satisfies \(b\).

For the sake of brevity, we omit the complete definition of GKAT and KAT semantics, we refer the reader to previous works~\cite{smolka_GuardedKleeneAlgebra_2020,schmid_GuardedKleeneAlgebra_2021,kozen_KleeneAlgebraTests_1997c}, which explains these semantics in detail.
Our work avoids direct interaction with the semantics by leveraging prior results in the coalgebraic theory of GKAT, which we will recap below.

Formally, GKAT coalgebras over primitive actions \(K\) and primitive tests \(T\) are coalgebras over the following functor:
\[G(S) ≜ (\{\accept, \reject\} + S × K)^{\At_T}.\] 
Intuitively, given a state \(s ∈ S\) and an atom \(α ∈ \At\), \(δ(s, α)\) will deterministically execute one of the following: reject \(α\) when \(δ(s, α) = \reject\); accept \(α\) when \(δ(s, α) = \accept\); or transition to a state \(s' ∈ S\) and execute action \(p ∈ K\) when \(δ(s, α) = (s', p)\).

In particular, \(G\) is a simple polynomial functor~\cite{jacobs_IntroductionCoalgebraMathematics_2016}, allowing the sub-coalgebra to preserve and reflect bisimulations.
\begin{theorem}[sub-coalgebras perserve and reflect bisimulation]\label{thm:sub-coalg-preserve-bisim}
    Given two states in sub-coalgebra \(s ∈ S' ⊑ S\) and \(u ∈ U' ⊑ U\), there exists a bisimulation \({∼'} ⊆ S' × U'\) s.t. \(s ∼' u\) if and only if there exists a bisimulation \({∼} ⊆ S × U\) s.t. \(s ∼ u\).
\end{theorem}

\begin{proof}
    For \(⟹\) direction, we will show that if \({∼'} ⊆ S' × U'\) is a bisimulation, then \({∼'}\) is also a bisimulation between \(S\) and \(U\):
    \[
        \begin{tikzcd}
            S \ar{d}{δ_S} & S' \ar[hook',swap]{l}{i} \ar{d}{δ_S}
            & ∼ \ar[swap]{l}{π₁} \ar{r}{π₂} \ar{d}{δ_∼}
            & U' \ar[hook]{r}{i} \ar{d}{δ_S} & U \ar{d}{δ_T}\\  
            G(S) & G(S') \ar[hook',swap]{l}{G(i)} 
            & G(∼) \ar[swap]{l}{G(π₁)} \ar{r}{G(π₂)} & U' \ar[hook]{r}{G(i)} & U 
        \end{tikzcd}
    \]
    Because the inclusion homomorphism \(i\) doesn't change the input, we have:
    \begin{align*}
        & {∼} \xrightarrow{π₁} S' \xrightarrow{i} S = {∼} \xrightarrow{π₁} S; \\
        & {∼} \xrightarrow{π₂} U' \xrightarrow{i} U = {∼} \xrightarrow{π₂} U.
    \end{align*}

    To prove the \(⟸\) we will show that if \({∼} ⊆ S × U\) is a bisimulation, then the restriction \({∼}' ≜ \{(s, u) ∈ {∼} ∣ s ∈ S', u ∈ U'\}\) is a bisimulation between \(S'\) and \(U'\).
    We first realize that \(∼_{S', U'}\) is a pre-image of the maximal bisimulation \(≣_{S', U'}\) along the inclusion homomorphism \(i: {∼} → {≡_{S, U}}\).
    This means that \(∼_{S', U'}\) can be formed by a pullback square:
    \[
        \begin{tikzcd}
            ∼_{S', U'} \ar{r}{i} \ar[swap]{d}{i} \ar[phantom, very near start]{dr}{\scalebox{1.5}{\(\lrcorner\)}} & ≣_{S', U'} \ar{d}{i}\\ 
            {∼} \ar[swap]{r}{i} & {≡_{S, U}}
        \end{tikzcd}
    \]
    Recall that elementary polynomial functor~\cite{jacobs_IntroductionCoalgebraMathematics_2016} like \(G\) preserves pullback, hence the pullback also uniquely generates a GKAT coalgebra~\cite{rutten_UniversalCoalgebraTheory_2000}
\end{proof}

Besides nice property with bisimulation, GKAT coalgebra is also deterministic, unlike Kleene coalgebra with tests (KCT)~\cite{kozen_CoalgebraicTheoryKleene_2017}.
For each atom, states in a KCT can accept or reject the atom (but not both), yet states can also non-deterministically transition to multiple different states via the same atom, while executing different actions.
As we will see later, the deterministic behavior of GKAT coalgebra not only enables a more versatile symbolic algorithm than KCT~\cite{pous_SymbolicAlgorithmsLanguage_2015}, but also present challenges. 
Specifically, GKAT coalgebra requires normalization to compute finite trace equivalences~\cite{smolka_GuardedKleeneAlgebra_2020}, where we reroute all the transitions that cannot lead to acceptance immediately into rejection; and states that can never lead to acceptance is called \emph{dead states}.

In previous works, these dead states are detected after the necessary coalgebra is computed and stored.
This approach requires storing all the transition and states of coalgebra in memory, meaning that the algorithm does not short circuit even if a mismatch can be detected in the starting state; for example, when for some \(α ∈ \At\), \(δ_S(s, α) = \reject\) and \(δ_U(u, α) = \accept\).

Our algorithm is lazy in the dead state detection i.e. the dead state are only checked when a mismatch requires it; even then, our dead state detection algorithm will only check the necessary states to compute the liveness of the states causing the mismatch.
This not only avoids unnecessarily checking the liveness of a state, but also enables allows on-the-fly generation of the coalgebra, like using derivatives, where we can remove irrelevant states from memory after it has been explored. 

However, to truly understand our on-the-fly algorithms, we will first need to define ``dead states'', and its role in defining the coalgebraic semantic of GKAT. 

\subsection{Liveness and Sub-GKAT coalgebras}

Traditionally, live and dead states are defined by whether they can reach an accepting state~\cite{smolka_GuardedKleeneAlgebra_2020}. 
However, we can use induction on the length of the path to show that principle sub-coalgebra \(⟨s⟩_S\) contains exactly the reachable states of \(s\) in any GKAT coalgebra \(S\). 
Thus, the classical definition is equivalent to the following:
\begin{definition}[liveness of states]\label{def:liveness-of-states}
    A state \(s\) is \emph{accepting} if there exists an atom \(α ∈ \At\) s.t. \(δ(s, α) = \accept\); a state \(s'\) is \emph{live} if there exists an accepting state \(s' ∈ ⟨s⟩\); and a state \(s'\) is \emph{dead} if there is no accepting state in \(⟨s⟩\).
\end{definition}
This alternative liveness definition can help us prove important theorems regarding reachability and liveness without explicitly performing induction on traces. 
We show the following theorems as examples:
\begin{lemma}\label{thm:dead-iff-all-reachable-dead}
    A state \(s\) is dead if and only if all elements in \(⟨s⟩\) is dead.
\end{lemma}
\begin{proof}
    \(⟸\) direction is true, because \(s ∈ ⟨s⟩\): if all \(⟨s⟩\) is dead, then \(s\) is dead. 
    \(⟹\) direction can be proven as follows.
    Take any \(s' ∈ ⟨s⟩\), then \(⟨s'⟩ ⊑ ⟨s⟩\) because \(s'\) is the minimal sub-coalgebra that contains \(s'\). 
    Since there is no accepting state in \(⟨s⟩\), thus there cannot be any accepting state in \(⟨s'⟩\), hence \(s'\) is also dead.
\end{proof}

\begin{theorem}[homomorphism perserves liveness]\label{thm:hom-preserve-liveness}
    Given a homomorphism \(h: S → U\) and a state \(s ∈ S\):
    \[\text{\(s\) is live} ⟺ \text{\(h(s)\) is live}\]
\end{theorem}

\begin{proof}
    Because homomorphic image preserves principle sub-GKAT coalgebra (\Cref{thm:homo-img-preserve-principle-sub-coalg})
    \[h(⟨s⟩_S) = ⟨h(s)⟩_U;\]
    therefore for any state \(s' ∈ S\):
    \[s' ∈ ⟨s⟩_S ⟺ h(s') ∈ h(⟨s⟩_S) ⟺ h(s') ∈ ⟨h(s)⟩_U.\]
    And because \(s'\) is accepting if and only if \(h(s')\) accepting by definition of homomorphism; then \(⟨s⟩_S\) contains an accepting state if and only if \(⟨h(s)⟩_U\) contains an accepting state. 
    Therefore, \(s\) is live in \(S\) if and only if \(h(s)\) is live in \(U\).
\end{proof}

The above theorem then leads to several interesting liveness preservation properties for structures on coalgebras, like sub-coalgebra and bisimulation.

\begin{corollary}[sub-coalgebra perserves liveness]\label{thm:sub-coalg-preserve-liveness}
    For a sub-coalgebra \(S' ⊑ S\) and a state \(s ∈ S'\), \(s\) is live in \(S'\) if and only if \(s\) is live in \(S\).
\end{corollary}

\begin{proof}
    Let the homomorphism \(h\) in \cref{thm:hom-preserve-liveness} be the inclusion homomorphism \(i: S' → S\).
\end{proof}

\begin{corollary}[bisimulation preserves liveness]\label{thm:bisim-preserve-liveness}
    If there exists a bisimulation \(∼\) between GKAT coalgebra \(S\) and \(U\) s.t. \(s ∼ u\) for some states \(s ∈ S\) and \(u ∈ U\), then \(s\) and \(u\) has to be either both accepting, both live or both dead.
\end{corollary}

\begin{proof}
    Because for a \(∼\) is a bisimulation when both \(π₁: {∼} → S\) and \(π₂: {∼} → U\) are homomorphisms.
    Therefore, 
    \begin{align*}
        s \text{ is live in } S 
        & ⟺ π₁((s, u)) \text{ is live in } S \\        
        & ⟺ (s, u) \text{ is live in } {∼} \\  
        & ⟺ π₂((s, u)) \text{ is live in } U  \\
        & ⟺ u \text{ is live in } U. 
        \qedhere
    \end{align*}
\end{proof}


\subsection{Normalization And Semantics}

Infinite-trace model \(𝒢_ω\) is the final coalgebra of GKAT coalgebras~\cite{schmid_GuardedKleeneAlgebra_2021}.
The finality of the model allows us to define the semantics of each state in a given GKAT coalgebra \(S\), which we will denote as \(⟦-⟧^{ω}_{S}: S → 𝒢_ω\), where the semantic equivalences can identified by bisimulation~\cite{schmid_GuardedKleeneAlgebra_2021}:
\(⟦s⟧^{ω}_{S} = ⟦t⟧^{ω}_{T}\) if and only if there exists a bisimulation \({∼} ⊆ S × T\), s.t. \(s ∼ t\).

The infinite trace equivalences can be directly computed with bisimulation on derivative, which supports on-the-fly algorithm as demonstrated by similar systems~\cite{kozen_CoalgebraicTheoryKleene_2017,almeida_DecidingKATHoare_2012,pous_SymbolicAlgorithmsLanguage_2015}. 
However, the \emph{finite} trace model \(𝒢\) is the final coalgebra of GKAT coalgebras without dead states, which we call \emph{normal GKAT coalgebra}~\cite{smolka_GuardedKleeneAlgebra_2020}. 
Fortunately every GKAT coalgebra can be normalized by rerouting all the transition from dead states to rejection.
Concretely, given a GKAT coalgebra \(S ≜ (S, δ_S)\), \(δ_{\norm(S)} : S → G(S)\) is defined as \(δ_{\norm(S)}(s, α) ≜ \reject\) when \(δ_S(s, α) = (s', p)\) and \(s'\) is dead in \(S\); and \(δ_{\norm(S)}(s, α) ≜ δ_S(s, α)\) otherwise. 
We call \(\norm(S) ≜ (S, δ_{\norm(S)})\) the \emph{normalized} coalgebra of \(S\).

And we use \(⟦-⟧_S: \norm(S) → 𝒢\) to denote the finite trace semantics of GKAT coalgebra, which is the unique homomorphism into the final coalgebra \(𝒢\). 
The finite trace equivalence between \(s ∈ S\) and \(u ∈ U\) can be decided by first normalizing \(S\) and \(U\) then deciding whether there is a bisimulation \({∼} ⊆ \norm(S) × \norm(U)\) s.t. \(s ∼ t\).
For a more detailed explanation on the finite trace semantics, we refer the reader to the work of \Citeauthor{smolka_GuardedKleeneAlgebra_2020}~\cite{smolka_GuardedKleeneAlgebra_2020}, however we will recall the correctness theorem here.

\begin{theorem}[Correctness~\cite{smolka_GuardedKleeneAlgebra_2020}]\label{thm:norm-bisim-correctness}
    Given two states in two GKAT coalgebra \(s ∈ S\) and \(u ∈ U\), then there exists a bisimulation between normalized coalgebras \({∼} ⊆ \norm(S) × \norm(U)\) s.t. \(s ∼ u\) if and only if \(s\) and \(u\) are trace equivalent \(⟦s⟧_S = ⟦u⟧_U\)
\end{theorem}

Besides giving us the finite-trace semantics, the normalization operation also satisfy several nice properties.

First, normalization preserves liveness, i.e. a state \(s ∈ S\) is dead (live) if and only if it is dead (live) in \(\norm(S)\), allowing us to perform liveness analysis in \(\norm(S)\) to obtain the liveness in \(S\), and vise versa.

\begin{theorem}[normalization perserves liveness]
    A state \(s ∈ S\) is dead (live) in \(S\) if and only if it is dead (live) in \(\norm(S)\).
\end{theorem}

\begin{proof}
    By definition, every state is either dead of live, thus we will only need to show the \(⟹\) direction, and 

    This proof unfortunately requires us unfolding the path to accepting states.

    If \(s\) is dead in \(S\), then by~\Cref{thm:dead-iff-all-reachable-dead}, for all \(α, p\), \(s \transvia{α ∣ p}_S s'\) implies \(s'\) is dead. 
    Because \(s\) cannot accept any atom in \(S\), \(s\) will reject all atom in \(\norm(S)\); which implies that \(s\) is dead in \(\norm(S)\).

    If \(s\) is live in \(S\), then there exists a walk \(s \transvia{b₁ ∣ p₁}_S s₁ \transvia{b₂ ∣ p₂}_S ⋯ \transvia{bₙ ∣ pₙ}_S sₙ\) s.t. \(sₙ\) is accepting, which implies every state \(sᵢ\) on the walk is live. 
    Hence, this walk also exists in \(\norm(S)\), and \(s\) is live in \(\norm(S)\).
\end{proof}

Second, normalization is an endofunctor in the category of GKAT coalgebra, this result can help us obtain sub-coalgebras of normalized coalgebra, and also connect the finite trace and infinite trace semantics.

\begin{theorem}\label{thm:norm-functor}
    \(\norm\) is an endofunctor in the category GKAT coalgebra.
    More specifically, if \(h: S → U\) is a GKAT homomorphism, then \(h: \norm(S) → \norm(U)\) is also a homomorphism.
\end{theorem}

\begin{proof}
    Recall that \(h\) is a homomorphism if and only if for all \(s ∈ S\) and \(α ∈ \At\):
    \begin{itemize}[nosep]
        \item for a result \(r ∈ \{\reject, \accept\}\), 
        \[δ_S(s, α) = r ⟺ δ_U(h(s), α) = r;\]
        \item for any \(s' ∈ S\) and \(p ∈ K\), 
        \[δ_S(s, α) = (s', p) ⟺ δ_{U}(h(s), α) = (h(s'), p).\]
    \end{itemize}

    Then we show that \(h: \norm(S) → \norm(U)\) is a homomorphism, this is a consequence of homomorphism preserves liveness (\Cref{thm:hom-preserve-liveness}): for all \(s ∈ \norm(S)\) and \(α ∈ \At\):
    \begin{align*}
        & δ_{\norm(S)}(s, α) = \accept \\*
        ⟺{}& δ_S(s, α) = \accept \\*  
        ⟺{}& δ_U(h(s), α) = \accept \\*  
        ⟺{}& δ_{\norm(U)}(h(s), α) = \accept;\\
        & δ_{\norm(S)}(s, α) = \reject \\*  
        ⟺{}& δ_{S}(s, α) = \reject 
        \text{ or } δ_{S}(s, α) = (s', p), s' \text{ is dead} \\*
        ⟺{}& δ_{U}(h(s), α) = \reject \\*
        & \text{ or } δ_{U}(h(s), α) = (h(s'), p), h(s') \text{ is dead} \\*  
        ⟺{}& δ_{\norm(U)}(h(s), α) = \reject;\\
        & δ_{\norm(S)}(s, α) = (s', p) \\*
        ⟺{}& δ_{S}(s, α) = (s', p), s' \text{ is live} \\*  
        ⟺{}& δ_{U}(h(s), α) = (h(s'), p), h(s') \text{ is live}\\* 
        ⟺{}& δ_{\norm(U)}(h(s), α) = (h(s'), p).
        \qedhere
    \end{align*}
\end{proof}

\begin{corollary}\label{thm:norm-sub-coalg}
    Normalization preserves sub-coalgebra, i.e. if \(S' ⊑ S\) then \(\norm(S') ⊑ \norm(S)\).
\end{corollary}

\begin{proof}
    By letting the homomorphism in~\Cref{thm:norm-functor} to be the inclusion homomorphism \(i: S' → S\)
\end{proof}

Because of the functoriality, we can show that two states are infinite-trace equivalent implies these two states are finite-trance equivalent.
This gives us more tool in proving semantic equivalence between two states in GKAT coalgebras: proving bisimulation of these two states in the \emph{non-normalized} GKAT coalgebra can also obtain semantic equivalence for two states.

\begin{corollary}\label{thm:inf-trace-equiv-implies-fin-trace-equiv}
    Given two states in two GKAT coalgebra \(s ∈ S, u ∈ U\), \(⟦s⟧^{ω}_{S} = ⟦u⟧^{ω}_{U} ⟹ ⟦s⟧_{S} = ⟦u⟧_{U}\).
\end{corollary}

\begin{proof}
    Because \(⟦s⟧^{ω}_{S} = ⟦u⟧^{ω}_{U}\), there exists a bisimulation \({∼} ⊆ S × U\) s.t. \(s ∼ u\)~\cite{schmid_GuardedKleeneAlgebra_2021}.
    Therefore, we have the following span in the category of GKAT coalgebra:
    \[\begin{tikzcd}
        S & ∼ \ar{r}{π₂} \ar[swap]{l}{π₁} & U
    \end{tikzcd}\]
    Then by~\Cref{thm:norm-functor}, \(\norm(∼)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\):
    \[\begin{tikzcd}
        \norm(S) 
        & \norm(∼) \ar{r}{π₂} \ar[swap]{l}{π₁} 
        & \norm(U)
    \end{tikzcd}\]
    Because \(s ∼ u\) and normalization operation preserves states in \(∼\), therefore \((s, u) ∈ \norm(∼)\), and because \(\norm(∼)\) is a bisimulation between the normalization of \(S\) and \(U\), therefore \(⟦s⟧_{S} = ⟦u⟧_{U}\) (\Cref{thm:norm-bisim-correctness}).
\end{proof}

\section{On-The-Fly Bisimulation}

The original algorithm for deciding GKAT equivalences~\cite{smolka_GuardedKleeneAlgebra_2020} requires the entire coalgebra to be known prior to the execution of the bisimulation algorithm; specifically, it is necessary to iterate through the entire coalgebra in order to identify the liveness of every single states, in order to perform the normalization operation.
This limitation poses challenges to design an efficient on-the-fly algorithm for GKAT.
In order to make the decision procedure scalable, we will need to merge the normalization and bisimulation procedure, so that our algorithm can normalize the coalgebra only when we need to.

In this section, we introduce an algorithm that merges bisimulation and normalization, where we only test the liveness of states when a disparity is noticed by the bisimulation algorithm.
For example, when deciding the trace equivalence between \(s ∈ S\) and \(u ∈ U\), if \(δ_S(s, α) = (s', p)\) and \(δ_U(u, α) = (u', p)\), we will proceed to recurse on \(s'\) and \(u'\), without checking the liveness of \(s'\) and \(u'\).
However, if \(s\) rejects \(α\) instead of transitioning to \(s'\), we will then check whether \(u'\) is dead.

We present several sound and complete conditions for finite-trace equivalence between states of GKAT coalgebras in~\Cref{thm:recursive-construction}, which also serves as the core correctness theorem for our equivalence-checking algorithm i.e.~\Cref{alg:bisim}.
This algorithm mostly consists of recursively checking the conditions in~\Cref{thm:recursive-construction} with minor optimizations, which we will outline later.

\begin{definition}[normalized bisimulation]\label{def:norm-bism}
    A relation \({≃} ⊆ S × U\) on two GKAT coalgebra \(S\) and \(U\) is a normalized bisimulation if and only if for any two states \(s ≃ u\), all the following holds:
    \begin{enumerate}
        \item\label{itm:acc-condition} for all \(α ∈ \At\), \(δ_{S}(s, α) = \accept ⟺ δ_{U}(u, α) = \accept\);
        \item\label{itm:rej-or-dead} if \(s\) reject \(α\) but \(u\) transitions to \(u'\), then \(u'\) is dead; similarly when \(u\) rejects \(α\);  
        \item\label{itm:transition-bisim} If \(δ_{S}(s, α) = (s', p)\) and \(δ_{U}(u, α) = (u', p)\), then \(s' ≃ u'\).
        \item\label{itm:transition-dead} If \(δ_{S}(s, α) = (s', p)\) and \(δ_{U}(u, α) = (u', q)\), s.t. \(p ≠ q\), then both \(s'\) and \(t'\) are dead.
    \end{enumerate}
\end{definition}

\begin{lemma}[bisimulation between dead states]\label{thm:bisim-between-dead}
    Given two dead states \(s ∈ S\) and \(u ∈ U\), then the singleton bisimulation \({∼} ⊆ \norm(S) × \norm(U)\):
    \begin{mathpar}
        {∼} ≜ \{(s,u)\} \and 
        δ_{∼}((s,u), α) ≜ \reject
    \end{mathpar}
    is a bisimulation between \(\norm(S)\) and \(\norm(U)\).
\end{lemma}

\begin{proof}
    By computation
\end{proof}

\begin{lemma}\label{thm:recursive-construction-lemma}
    Given two GKAT coalgebra \(S\) and \(U\), and two of their elements \(s ∈ S\) and \(u ∈ U\),
    there exists a bisimulation \({∼} ⊆ \norm(⟨s⟩) × \norm(⟨u⟩)\) s.t. \(s ∼ u\), if and only if there exists a normalized bisimulation \({≃} ⊆ ⟨s⟩ × ⟨u⟩\) s.t. \(s ≃ u\).
\end{lemma}

\begin{proof}
    Recall that there exists a bisimulation \({∼} ⊆ \norm(⟨s⟩) × \norm(⟨u⟩)\) if and only if for all \(s₁ ∼ u₁\):
    \begin{itemize}
        \item for all results \(r ∈ \{\accept, \reject\}\): \(δ_{\norm(S)}(s₁, α) = r ⟺ δ_{\norm(U)}(u₂, α) = r\);
        \item otherwise, let \((s₂, p) ≜ δ_{\norm(S)}(s₁, α)\) and \((u₂, q) ≜ δ_{\norm(u)}(u₁, α)\), then \(p = q\) and \(s₂ ∼ u₂\)
    \end{itemize}

    We prove \(⟹\) direction by constructing the following normalized bisimulation from the bisimulation \({∼}\):
    \[{≃} ≜ {∼} ∪ \{(s, u) ∣ \text{both \(s ∈ S, u ∈ U\) are dead}\}.\]
    We then show that \({≃}\) satisfies all the condition in~\Cref{def:norm-bism}.

    The condition \labelcref{itm:acc-condition} holds if \(s ∼ u\):
    \begin{align*}
        δ_S(s, α) = \accept 
        ⟺{}& δ_{\norm(S)}(s, α) = \accept \\
        ⟺{}& δ_∼((s,u), α) = \accept \\ 
        ⟺{}& δ_{\norm(U)}(u, α) = \accept \\
        ⟺{}& δ_U(u, α) = \accept
    \end{align*}
    The condition \labelcref{itm:acc-condition} also holds if \(s, u\) are both dead, because both can never accepts any atoms.

    The condition~\labelcref{itm:rej-or-dead} holds when \(s ∼ u\): 
    \begin{align*}
        & δ_S(s, α) = \reject \text{ and } δ_U(u, α) = (u', p)\\
        ⟹{}& δ_{\norm(S)}(s, α) = \reject \text{ and } δ_U(u, α) = (u', p) \\
        ⟹{}& δ_{\norm(U)}(u, α) = \reject \text{ and } δ_U(u, α) = (u', p)\\
        ⟹{}& \text{\(u'\) is dead}
    \end{align*}
    Similarly, when \(u'\) reject \(α\) and \(s\) transitions to \(s'\).
    The condition~\labelcref{itm:rej-or-dead} holds when \(s, u\) are both dead because both of them can only transition to dead state by~\Cref{thm:dead-iff-all-reachable-dead}.

    The condition~\labelcref{itm:transition-bisim} holds when \(s ∼ u\).
    First note that \(s'\) and \(u'\) has to be both live or both dead: because \(δ_S(s, α) = (s', p)\), then \(\norm(δ_S)(s', α)\) can either be rejection or \((s',p)\), and so is \(\norm(δ_U)(u', α)\):
    \begin{align*}
        s' \text{ is live} 
        & ⟺ δ_{\norm(S)}(s, α) = (s', p) \\
        & ⟺ δ_{\norm(U)}(u, α) = (u', p) \\
        & ⟺ u' \text{ is live}.
    \end{align*}
    \begin{itemize}
        \item If both \(s'\) and \(u'\) are live, then \(s' ∼ u'\). By~\cref{thm:sub-coalg-preserve-bisim}, the bisimulation \(∼'\) is just \(∼\) restricted to \(⟨s'⟩\) and \(⟨u'⟩\).
        \item If both \(s'\) and \(u'\) are dead, then \(∼'\) can just be the singleton relation, according to~\cref{thm:bisim-between-dead}.
    \end{itemize}

    The condition~\labelcref{itm:transition-dead} holds: by the proof of condition~\labelcref{itm:transition-bisim}, \(s'\) and \(u'\) has to be either both live or both dead; if they are both live, then there cannot be a element in \(G(∼)\) that can project to \((s', p)\) under \(π₁\) but projects to \((t', q)\) under \(π₂\). Thus both \(s'\) and \(t'\) has to be dead.

    We then show the \(⟸\) direction, for arbitrary \(s' ∈ S\) and \(u' ∈ U\), we use \(≡_{s', u'}\) to denote the maximal bisimulation between \(\norm(⟨s'⟩)\) and \(\norm(⟨u'⟩)\).
    \begin{align*}
        {∼'} ≜ ⋃ \{≡_{s', u'} & ∣ ∃ α ∈ \At, p ∈ K, \\*
            & δ_{\norm(S)}(s, α) = (s', p) \\*
            & \text{ and } δ_{\norm(U)}(u, α) = (u', p)\}.
    \end{align*}
    For all the \(s'\) and \(u'\) in the above definition, \(⟨s'⟩ ⊑ ⟨s⟩\) and \(⟨u'⟩ ⊑ ⟨u⟩\), therefore by~\Cref{thm:norm-sub-coalg}, \(\norm(⟨s'⟩) ⊑ \norm(⟨s⟩)\) and \(\norm(⟨u'⟩) ⊑ \norm(⟨u⟩)\). 
    By~\Cref{thm:sub-coalg-preserve-bisim}, every \(≡_{s', u'}\) is a bisimulation between \(\norm(⟨s⟩)\) and \(\norm(⟨t⟩)\), and because bisimulation is closed under arbitrary union~\cite{rutten_UniversalCoalgebraTheory_2000}, \(∼'\) is a bisimulation between \(\norm(⟨s⟩)\) and \(\norm(⟨t⟩)\).

    To obtain the desired bisimulation \({∼}\) between \(\norm(⟨s⟩)\) and \(\norm(⟨u⟩)\), we add the pair \((s, t)\) to \(∼'\), 
    \[{∼} ≜ {∼'} ∪ \{(s, u)\},\] 
    with the following transition \(δ_{∼}\): for all \(α ∈ \At\),
    \begin{itemize}[nosep]
        \item if \(δ_{\norm(S)}(s, α) = δ_{\norm(U)}(u, α) = \accept\), then \(δ_{∼}((s, u), α) ≜ \accept\);
        \item if \(δ_{\norm(S)}(s, α) = δ_{\norm(U)}(u, α) = \reject\), then \(δ_{∼}((s, u), α) ≜ \reject\);
        \item if \(δ_{\norm(S)}(s, α) = (s', p)\) and \(δ_{\norm(U)}(u, α) = (u', p)\), then \(δ_{∼}((s, u), α) = ((s', u'), p)\);
        \item for all \((s', u') ∈ {∼}'\) that is not equal to \((s, u)\), we let \(δ_∼\) inherits the transition of \(δ_{∼'}\), i.e. \(δ_{∼}((s', u'), α) = δ_{∼'}((s', u'), α)\)
    \end{itemize}
    \emph{if \(δ_{∼}\) is well-defined}, then we can verify that \(∼\) is indeed a bisimulation between \(\norm(⟨s⟩)\) and \(\norm(⟨u⟩)\) where \(s ∼ u\). 
    We show the slightly more complicated case: \(δ_{\norm(S)}(s, α) = (s', p)\) and \(δ_{\norm(U)}(u, α) = (u', p)\) implies \(s' ∼ u'\) as an example. 
    By condition~\labelcref{itm:transition-bisim}, there exists a bisimulation \(∼_{s', u'} ⊆ \norm(⟨s'⟩) × \norm(⟨u'⟩)\), and because \(≡_{s', u'}\) is the maximal bisimulation between \(\norm(⟨s'⟩)\) and \(\norm(⟨u'⟩)\),
    \[(s', u') ∈ {∼_{s', u'}} ⊆ {≡_{s', u'}} ⊆ {∼}.\]

    Finally, we demonstrate that \(δ_∼\) is well-defined by leveraging the conditions in~\Cref{thm:recursive-construction}. 
    Specifically, we will show that the definition of \(δ_∼\) covers all the possible cases, by case analysis on the result of \(δ_S\): for all \(α ∈ \At\),
    \begin{itemize}
        \item If \(δ_S(s, α) = \accept\), then by condition~\labelcref{itm:acc-condition}, \(δ_U(u, α) = \accept\); therefore \[δ_{\norm(S)}(s, α) = δ_{\norm(U)}(u, α) = \accept.\]
        \item If \(δ_S(s, α)\) transitions to a dead state or reject, then by condition~\labelcref{itm:rej-or-dead} \(δ_U(u, α)\) will also transition to a dead state or reject, then \[δ_{\norm(S)}(s, α) = δ_{\norm(U)}(u, α) = \reject.\]
        \item If \(δ_S(s, α) = (s', p)\) and \(s'\) is live, then \(δ_U(u, α) = (u', p)\) necessarily holds, otherwise it would violate one of conditions~\labelcref{itm:acc-condition,itm:rej-or-dead,itm:transition-dead}. 

        By condition~\labelcref{itm:transition-bisim}, there exists a bisimulation \(∼_{s', u'}\) between \(\norm(⟨s'⟩)\) and \(\norm(⟨u'⟩)\) s.t. \(s' ∼_{s', u'} u'\). Because bisimulation preserves liveness (\Cref{thm:bisim-preserve-liveness}), \(s', u'\) has to be both dead or live. 
        Finally, because \(s'\) is live, therefore \(u'\) is also live, and we obtain the final case in the definition of \(δ_{∼}\): \(δ_{\norm(S)}(s, α) = (s', p)\) and \(δ_{\norm(U)}(u, α) = (u', p)\).
        \qedhere
    \end{itemize}
\end{proof}

With some slight tweak to \Cref{thm:recursive-construction-lemma}, it will be applicable to trace equivalence instead of bisimulation.
The correctness of this modification is supported by the correspondence between trace equivalence and the existence of a bisimulation between normalized GKAT coalgebras, which is stated in~\Cref{thm:norm-bisim-correctness}.

\begin{theorem}[Recursive Construction]\label{thm:recursive-construction}
    For any two states in two GKAT coalgebra \(s ∈ S, u ∈ U\), \(s\) and \(u\) are finite-trace equivalent \(⟦s⟧_{S} = ⟦u⟧_{U}\) if and only if all the following conditions hold:
    \begin{enumerate}
        \item for all \(α ∈ \At\), \(δ_{S}(s, α) = \accept ⟺ δ_{U}(u, α) = \accept\);
        \item \(s\) reject \(α\) or transition to a dead state via \(α\) if and only if \(u\) rejects \(α\) or transition to a dead state via \(α\);  
        \item if \(δ_{S}(s, α) = (s', p)\) and \(δ_{U}(u, α) = (u', p)\), then \(⟦s'⟧_{S} = ⟦u'⟧_{U}\);
        \item if \(δ_{S}(s, α) = (s', p)\) and \(δ_{U}(u, α) = (u', q)\), s.t. \(p ≠ q\), then both \(s'\) and \(t'\) are dead.
    \end{enumerate}
    Notice that above condition is similar to those in~\cref{thm:recursive-construction}, except bisimulation of \(s'\) and \(u'\) is replaced with trace equivalence.
\end{theorem}

\begin{proof}
    By the standard argument with normalization preserves sub-coalgebra (\Cref{thm:norm-sub-coalg}), we can obtain for all \(s ∈ S\) and \(u ∈ U\), \(\norm(⟨s⟩) ⊑ \norm(S)\) and \(\norm(⟨u⟩) ⊑ \norm(U)\).
    Therefore, because subcoalgebra preserve and reflect bisimulation (\Cref{thm:sub-coalg-preserve-bisim}) and the correctness of bisimulation on normalized coalgebra (\Cref{thm:norm-bisim-correctness}): for all \(s ∈ S\) and \(u ∈ U\),
    \begin{align*}
        & ∃~ {∼} ⊆ \norm(⟨s⟩) × \norm(⟨u⟩), s ∼ u \\
        ⟺{}& ∃~ {∼} ⊆ \norm(S) × \norm(U), s ∼ u \\  
        ⟺{}& ⟦s⟧_S = ⟦u⟧_U.
    \end{align*}
    We can instantiate the above equivalence to \(s', u'\) and \(s, u\) respectively: the instantiation to \(s', u'\) will give us the conditions in this theorem is equivalent to the ones in~\Cref{thm:recursive-construction-lemma}; and instantiation to \(s, u\) show that existence of bisimulation is equivalent to trace equivalence:
    \begin{align*}
        & \text{Conditions in \Cref{thm:recursive-construction-lemma} hold} \\  
        ⟺{}& \text{Conditions in the current theorem hold}\\
        ⟺{}& ∃~ {∼} ⊆ \norm(⟨s⟩) × \norm(⟨u⟩), s ∼ u\\
        ⟺{}& ⟦s⟧_{S} = ⟦u⟧_{U}. \qedhere
    \end{align*}
\end{proof}

The above construction theorem already gives us an algorithm to recursively decide whether \(⟦s⟧_{S} = ⟦u⟧_U\), when \(⟨s⟩_S\) and \(⟨u⟩_U\) is finite.
However, this algorithm can be further optimized: we will derive that a dead state is only trace equivalent to other dead states. 
This means that when checking the trace-equivalence of states \(s\) and \(u\), if we already know one of them is dead, we only need to check whether the other is dead, instead of going through all the conditions in~\Cref{thm:recursive-construction}.

\begin{theorem}\label{thm:bisim-one-dead}
    Given two states \(s ∈ S\) and \(u ∈ U\), if \(s\) is a dead state in \(S\), then \(s\) and \(u\) is trace equivalent if and only if \(u\) is also dead.
\end{theorem}

\begin{proof}
    TODO: I think we can refine \(⟹\) direction, but I am not sure...  

    For the \(⟸\) direction, we can construct the bisimulation as in~\cref{thm:bisim-between-dead}, which implies trace equivalence.
    And the \(⟹\) direction, if \(s\) is dead, then by definition of normalization, it will be all rejecting, i.e. for all \(α ∈ \At\), \(δ_{\norm(S)}(s, α) = \reject\).
    If \(s\) and \(u\) are trace equivalent, then there exists a bisimulation \({∼}: \norm(S) × \norm(U)\) s.t. \(s ∼ u\).
    By unfolding the definition of a bisimulation, \(u\) also need to be all rejecting in \(\norm(U)\).
    By definition of \(\norm\), for all \(α ∈ \At\), \(δ_U(u, α)\) either reject to go to a dead state, and because \(⟨u⟩_U\) is all the reachable state from \(u\), therefore 
    \begin{align*}
        ⟨u⟩_U = & ⋃ \{⟨u'⟩ ∣ ∃ α ∈ \At, p ∈ K, δ_U(u, α) = (u', p)\} \\
        & ∪ \{u\}
    \end{align*}
    And because all of \(u'\) is dead, all of \(⟨u'⟩_U\) cannot be accepting (\Cref{thm:dead-iff-all-reachable-dead}), nor is \(u\) an accepting state, then \(⟨u⟩_U\) does not contain any accepting state, and by definition of dead state, \(u\) is dead.
\end{proof}

Before we introduce the optimized algorithm, we will first briefly sketch the liveness-check algorithm, which will use a depth-first search to check whether the there exists any accepting states in reachable states of \(s ∈ S\): if there exists any accepting state in \(⟨s⟩_S\), then the algorithm terminates immediately, and return that \(s\) is live, otherwise it would return all the states in \(⟨s⟩\), and by~\cref{thm:dead-iff-all-reachable-dead}, all the states in \(⟨s⟩\) is dead.
In this case, we use depth-first search for its simplicity to implement, other search algorithm will also be sound.

We will cache all the known dead states from the previous searches; whether a state \(s\) is in this cache can be checked by the function call \Call{knownDead\(_S\)}{$s$}. 
The function \Call{isDead\(_S\)}{$s$} will first check if \(s\) is known to be dead, and invoke the search algorithm in the coalgebra \(⟨s⟩\), if \(s\) is not in the cached dead states.
Because the non-symbolic version of liveness checking is similar to the symbolic version, we only provide the symbolic version of this algorithm in appendix
TODO: give the link to the section.


\begin{algorithm}
    \caption{On-the-fly bisimulation algorithm}\label{alg:bisim}
    \begin{algorithmic}
        \Function{equiv}{$s ∈ S, u ∈ U$}
        \If {\Call{eq}{$s, u$}} {\Return true} \EndIf
        \If {\Call{knownDead\(_S\)}{$s$}} 
            \State\Return \Call{isDead\(_U\)}{$u$}
        \EndIf
        \If {\Call{knownDead\(_U\)}{$u$}} 
            \State\Return \Call{isDead\(_S\)}{$s$}
        \EndIf
        \For{\(α ∈ \At\)}{}{
            \Match{$δ_{S}(s, α), δ_{U}(u, α)$}
            \Case{\(\accept, \accept\)} {\Continue} \EndCase
            \Case{\(\reject, \reject\)} {\Continue} \EndCase
            \Case{\(\reject, (u', q)\)} 
                \State\Return \Call{isDead\(_U\)}{$u'$} 
            \EndCase
            \Case{\((s', p), \reject\)}
                \State\Return\Call{isDead\(_S\)}{$s'$}
            \EndCase
            \Case{\((s', p), (u', q)\)} {
                \If {\(p = q\)}
                    \State\Call{union}{$s, u$}
                    \State\Return \Call{equiv}{$s, t$} 
                \EndIf
                \If{\Call{isDead\(_S\)}{$s$} and \Call{isDead\(_U\)}{$u$}}
                    \State\Continue
                \EndIf
                \State\Return false
            } \EndCase
            \Default { \Return false } \EndDefault
            \EndMatch
        }\EndFor
        \State\Return true
        \EndFunction
    \end{algorithmic}
\end{algorithm}

Finally, we present our equivalence-checking algorithm as~\cref{alg:bisim}, where we first check if one of \(s\) or \(u\) is known to be dead, if so we only need to check whether the other is dead, because of~\Cref{thm:bisim-one-dead}; otherwise, we will check the conditions in~\Cref{thm:recursive-construction}.
Because trace equivalence is an equivalence relation, therefore we can organize the previously explored states into equivalent classes using an efficient union-find structure, instead of a set of state pairs.
This union-find structure need to provide two functions: \(\Call{union}{$s, u$}\) unions the equivalence class of \(s\) and \(u\), whereas \(\Call{eq}{$s, u$}\) checks whether \(s\) and \(u\) is in the same equivalence class. 

Concerning the complexity, our algorithm have similar worst case complexity as the original algorithm.
In particular, when deciding the trace equivalence of two states \(s ∈ S\) and \(u ∈ U\), the original algorithm~\cite{smolka_GuardedKleeneAlgebra_2020} requires one pass of \(⟨s⟩\) and \(⟨u⟩\) to normalize them, then the bisimulation will visit each pair of states in \(⟨s⟩ × ⟨u⟩\) at most once, which means that they will visit at most \(|⟨s⟩ + ⟨u⟩ + ⟨s⟩ × ⟨u⟩|\) number of states.

Similarly, our equivalence-checking algorithm attempts to find a bisimulation first, which visits each pair in \(⟨s⟩ × ⟨u⟩\) at most once, and when a mismatch is found in the process, the liveness-checking algorithm is then invoked.
Crucially, our algorithm satisfy the following property: if the liveness-checking algorithm find a live or accepting state, the entire equivalence checking algorithm will halt and return false.
Thus, by caching all the known dead states, the liveness-checking only visits states in \(⟨s⟩\) and \(⟨u⟩\) at most once.
Therefore, the worst case of our algorithm is also \(|⟨s⟩ × ⟨u⟩ + ⟨s⟩ + ⟨u⟩|\).

However, the on-the-fly algorithm will always out-perform the original algorithm, in terms of number of states visited, because this algorithm only invoke liveness check when necessary.
In the extreme case when the two input states are infinite-trace equivalent, the on-the-fly algorithm can skip liveness checking entirely.


% \section{The Algorithm}

% In this section we will present the pseudo-code for our on-the-fly algorithm. 
% In order to implement the the inductive construction theorem (\cref{thm:recursive-construction}), we will need to determine the liveness of the state. This can be simply computed via a DFS from the state being checked. 

% TODO: we should merge the two so that it is easier to 
% \begin{algorithm}
%     \caption{Check whether a state \(s\) is dead}\label{alg:check-dead-main}
%     \begin{algorithmic}
%         \Function{isDeadLoop}{$s ∈ S$, explored}
%         \If {\(s ∈\) explored} {\Return explored} 
%         \Else { 
%             \For{\(α ∈ \At\)}{}{
%                 \Match{\(δ_{S}(s, α)\)}
%                 \Case{\(\accept\)} {\Return none} \Comment{\(s\) transition to accept}
%                 \EndCase
%                 \Case{\(\reject\)} {\texttt{continue}}
%                 \Comment{skip if \(s\) transition to reject}
%                 \EndCase
%                 \Case{($s', p$)}{ 
%                     \If{\Call{IsDeadLoop}{$s'$} = none} {\Return none} \Comment{\(s\) transitions to a live state \(s'\)}
%                     \Else { explored \(←\) (explored \(∪\) \Call{isDeadLoop}{$s'$, explored}) } \EndIf
%                 } \EndCase
%                 \EndMatch
%             }\EndFor}
%         \EndIf
%         \State {\Return explored}    
%         \EndFunction
%     \end{algorithmic}
% \end{algorithm}

% By~\cref{thm:dead-iff-all-reachable-dead}, if \(s\) is dead then all the reachable states of \(s\) (denoted by \(⟨s⟩\)). Then by returning all the reachable states of \(s\), we can cache these states to avoid checking them again. To encapsulate the caching, we have the following function, which we will actually use in our bisimulation algorithm.

% \begin{algorithm}
%     \caption{A cached algorithm to check whether a state is dead}\label{alg:is-dead}
%     \begin{algorithmic}
%         \State{deadStates \(← ∅\)}

%         \Function{isDead}{$s ∈ S$}
%         \If {\(s ∈\) deadStates} {\Return true} 
%         \ElsIf {\Call{isDeadLoop}{$s, ∅$} = none} {\Return false}
%         \Else 
%             \State {deadStates \(←\) (deadStates \(∪\) \Call{isDeadLoop}{$s, ∅$})}
%             \State {\Return {true}}
%         \EndIf
%         \EndFunction
%     \end{algorithmic}
% \end{algorithm}

% Given the direct correspondence between bisimulation and bisimulation equivalence and bisimulation in sub-algebra:
% \begin{align*}
%     & ∃ \text{ bisimulation } {∼} ⊆ ⟨s⟩ × ⟨t⟩ \text{ s.t. } s ∼ t \\
%     & ⟺ ∃ \text{ bisimulation } {∼} ⊆ (⟨s⟩∪⟨t⟩) × (⟨s⟩∪⟨t⟩) \text{ s.t. } s ∼ t 
%         & \text{\cref{thm:sub-coalg-preserve-bisim}}\\  
%     & ⟺ ∃ \text{ bisimulation equivalence } {≃} ⊆ (⟨s⟩∪⟨t⟩) × (⟨s⟩∪⟨t⟩) \text{ s.t. } s ≃ t 
%         & \text{\cref{thm:bisim-iff-bisim-equiv}}
% \end{align*}
% we can safely replace the bisimulation in inductive construction (\cref{thm:recursive-construction}) with bisimulation equivalence. 
% Dealing with equivalence relations allows us to leverage efficient data structures like union find in our bisimulation algorithm. 

% We will use \(\Call{union}{$s,t$}\) to denote the operation to equate \(s\) and \(t\) in a union-find, and use \(\Call{eq}{$s,t$}\) to check if \(s\) and \(t\) belongs to the same equivalence class, i.e. share the same representative.
% Specifically, we will use the union-find structures to keep track of the equivalence classes that we are in the process of checking, hence avoiding repeatedly checking the same pair of states to remove infinite loops.

% Our on-the-fly bisimulation algorithm will decide whether there exists a bisimulation relation in \(⟨s⟩ ∪ ⟨t⟩\) s.t. \(s ∼ t\). This algorithm generally reproduce the setting of inductive construction theorem~\cref{thm:recursive-construction};
% except by~\cref{thm:bisim-one-dead}, in the special case where \(s\) or \(t\) is dead, then we will only need to check whether the other is dead.

% Because the dead state detection algorithm is coalgebra-specific, we use a subscript on ``deadStates'' and ``\textsc{IsDead}'' to indicate the coalgebra. 
% The soundness and completeness of~\cref{alg:bisim} can be observed by the fact that \emph{when the algorithm terminate}, the algorithm returns true if and only if there exists a bisimulation between \(⟨s⟩\) and \(⟨t⟩\) s.t. \(s ∼ t\), which is then logically equivalent to trace equivalence.
% Such equivalence is a direct consequence of~\cref{thm:bisim-one-dead,thm:recursive-construction}.

% \begin{remark}
%     The caching of dead state and the shortcut to check whether \(s\) is dead when \(t\) is dead and vise versa, is not essential to the soundness and completeness of algorithm, they are here to trade speed with memory. 
%     In a memory-constraint situation, the ``\textnormal{deadStates}'' variable can be cleared periodically to save memory.
% \end{remark}

\section{Symbolic Coalgebra and Algorithm}

Although \cref{alg:bisim} is on-the-fly, it still uses GKAT coalgebra, which contains exponentially many transitions with respect to the number of primitive tests \(|T|\).
Concretely, the transition function \(δ: S → \At_T → \{\accept, \reject\} + S × K\) requires computing the transition result for each \emph{atom}, and the number of atoms \(\At_T ≅ 2^{T}\) is exponential to the size of primitive tests in \(T\).
This is also why several GKAT-related complexity results only consider constant sized \(T\).

Symbolic GKAT coalgebra, instead of computing the behavior of each atom individually, groups atoms into boolean expressions. 
This optimization leads to space-efficient coalgebras and an equivalence checking algorithm making use of off-the-shelf SAT solvers.
Specifically, given a set of primitive actions \(K\) and primitive tests \(T\), a \emph{symbolic GKAT coalgebra} \(Ŝ ≜ (S, ϵ̂, δ̂)\) consists of a state set \(S\) and an accepting function \(ϵ̂\) and a transition function \(δ̂\):
\begin{mathpar}
    ϵ̂: S → 𝒫(\BExp_T), \and
    δ̂: S → 𝒫(\BExp_T × S × K).
\end{mathpar}
This coalgebra is also required to satisfy the disjointedness condition, i.e. for all states \(s ∈ S\), the boolean expressions that \(s\) accepts \(ϵ̂(s)\) and the boolean expressions that enables \(s\) to transition \(\{b ∣ ∃ (b, s', p) ∈ δ(s)\}\) is disjoint; also the conjunction of any two distinct expressions from the set \(ϵ̂(s) ∪ \{b ∣ ∃ (b, s', p) ∈ δ(s)\}\) is equivalent to \(0\) under boolean algebra.

We name \(ϵ̂\) the accepting function because intuitively a state \(s\) accepts an atom \(α\) when there exists a \(b ∈ ϵ̂(s)\), s.t. \(α ≤ b\); similarly, \(δ̂\) is called the transition function because a state \(s\) transitions to \(s'\) via atom \(α\) while executing \(p\) when there exists \((b, s', p) ∈ δ̂(s)\) and \(α ≤ b\).
With the above intuition in mind, a symbolic GKAT coalgebra \(Ŝ ≜ (S, ϵ̂, δ̂)\) can be lowered into a GKAT coalgebra \(S ≜ (S, δ)\) in the following manner:
\begin{align}\label{cons:lowering}
δ(s, α) ≜ \begin{cases}
    \accept & ∃ b ∈ ϵ̂(s), α ≤ b \\[5px]
    (s', p) & 
        \begin{aligned}
            & ∃ b ∈ \BExp_T, α ≤ b \\
            & \text{ and } δ(s, b) = (s', p)  
        \end{aligned}\\[5px]
    \reject & \text{otherwise}
\end{cases}
\end{align}
This is well-defined, i.e. exactly one clause can be satisfied for any \(s ∈ S\) and \(α ∈ \At\), because of the disjointedness condition.
We usually use \(S\) to denote the lowering of \(Ŝ\); and the semantics of a state \(s ∈ Ŝ\) is defined as its semantics in the lowering \(⟦s⟧_{Ŝ} ≜ ⟦s⟧_{S}.\)

We will then use \(ρ̂(s): \BExp_T\) to represent all the atoms that the state \(s\) rejects in the lowering, and \(ρ̂(s)\) is computed as follows:
\begin{align*}
    ρ̂(s) ≜ ⋀ \{\overline{b} ∣{}
        & ∃ s' ∈ S, p ∈ K, (b, s', p) ∈ δ̂(s) \\
        & \text{ or } b ∈ ϵ̂(s)\}.
\end{align*}

\begin{remark}[Canonicity]\label{rem:canonicity}
    Symbolic GKAT coalgebra is not canonical, i.e. there exists two different symbolic GKAT coalgebra with the same lowering, consider the state set \(S ≜ \{s\}\):
    \begin{align*}
        {δ̂}₁(s) & ≜ \{b ↦ (s, p), \overline{b} ↦ (s, p)\} \\
        {δ̂}₂(s) & ≜ \{⊤ ↦ (s, p)\},
    \end{align*} 
    and both \(ϵ̂₁, ϵ̂₂\) will return constant \(0\).
    These two symbolic GKAT coalgebra \(Ŝ₁ ≜ (S, δ̂₁, ϵ̂₁)\) and \(Ŝ₂ ≜ (S, δ̂₂, ϵ̂₂)\) have the same lowering and semantics, yet, they are different.
    It is possible to construct symbolic representations that satisfies canonicity, yet we opt to use our current representation for ease of construction and computational efficiency.
\end{remark}

\begin{theorem}[Functoriality]\label{thm:lowering-functor}
    The lowering operation is a functor, every symbolic GKAT coalgebra homomorphism \(h: Ŝ → Û\), is also a GKAT coalgebra homomorphism \(h: S → U\).
\end{theorem}

\begin{proof}
    Since \(Ŝ\) and \(Û\) have the same states as their lowering, therefore \(h: S → U\) is indeed a function, then we only need to verify the homomorphism condition on \(h\).
    TODO: finish.
\end{proof}

The functoriality states that a homomorphism on two symbolic coalgebras induces a homomorphism of on their lowing; similarly, a bisimulation on symbolic GKAT coalgebra also induces a bisimulation on their lowering.
However, the converse is not true, precisely because of the canonicity problem noted in~\Cref{rem:canonicity}: take the \(Ŝ₁\) and \(Ŝ₂\) in~\Cref{rem:canonicity}, because they have the same lowering, therefore the identity homomorphism is a homomorphism on their lowerings, but there is no homomorphism from \(Ŝ₁\) to \(Ŝ₂\).

We can then define the symbolic equivalence algorithm, with its correctness stated in~\cref{thm:recursive-construction}.

\begin{theorem}[Symbolic Recursive Construction]\label{thm:symb-recursive-construction}
    Given two symbolic GKAT coalgebra \(Ŝ = (S, ϵ̂_S, δ̂_S)\) and \(Û = (U, ϵ̂_U, δ̂_U)\) and two states \(s ∈ S\) and \(u ∈ U\), \(s\) and \(u\) are trace equivalent \(⟦s⟧_{Ŝ} = ⟦u⟧_{Û}\), if and only if all the following holds:
    \begin{itemize}
        \item \(⋁ ϵ̂_S(s) ≡ ⋁ ϵ̂_U(u)\); 
        \item for all \((b, s', p) ∈ δ̂_S(s)\) and \(c ∈ ρ̂_U(u)\), if \(b ∧ c ≢ 0\), then \(s'\) is dead;
        \item for all \(b ∈ ρ̂_S(s)\) and \((c, u', q) ∈ δ̂_U(u)\), if \(b ∧ c ≢ 0\), then \(u'\) is dead;
        \item for all \((b, s', p) ∈ δ̂_S(s)\) and \((c, u', q) ∈ δ̂_U(u)\), if \(b ∧ c ≢ 0\) and \(p ≠ q\) then both \(s'\) and \(u'\) is dead; 
        \item for all \((b, s', p) ∈ δ̂_S(s)\) and \((c, u', q) ∈ δ̂_U(u)\), if \(b ∧ c ≢ 0\) and \(p = q\) then \(⟦s'⟧_{Ŝ} = ⟦u'⟧_{Û}\).
    \end{itemize}
\end{theorem}

\begin{proof}
    Reduces to~\Cref{thm:recursive-construction} i.e. all the above condition holds if and only if all the condition in~\Cref{thm:recursive-construction} holds in the lowering.
\end{proof}

\begin{algorithm*}
    \caption{Symbolic On-the-fly Bisimulation Algorithm}\label{alg:symb-bisim}
    \begin{algorithmic}
        \Function{equiv}{$s ∈ S, u ∈ U$}
        \If {\Call{eq}{$s, u$}} {\Return true}
        \ElsIf {\Call{knownDead\(_S\)}{$s$}} {\Return \Call{isDead\(_U\)}{$u$}} 
        \ElsIf {\Call{knownDead\(_U\)}{$u$}} {\Return \Call{isDead\(_S\)}{$s$}} 
        \Else {}
        \Return {
            \Comment{conditions of ~\cref{thm:symb-recursive-construction}}
            \\\vspace{5px}
            \(\qquad
            \begin{aligned}
                & ⋁ ϵ̂_S(s) ≡ ⋁ ϵ̂_U(u) \mathrel{\&\!\&} \\  
                & ∀ (b, s', p) ∈ δ̂_S(s), (c, u', q) ∈ δ̂_U(u), (b ∧ c) ≢ 0 ⟹ 
                \begin{cases}
                    \text{\Call{isDead$_S$}{$s$}} ∧ \text{\Call{isDead$_U$}{$u$}} & \text{if \(p ≠ q\)} \\
                    \text{\Call{Union}{$s$, $u$}}; \text{\Call{Equiv}{$s', u'$}} & \text{if \(p = q\)}
                \end{cases} \mathrel{\&\!\&}\\
                & ∀ (b, s', p) ∈ δ̂_S(s), c ∈ ρ̂_U(u), (b ∧ c) ≢ 0 ⟹ \text{\Call{isDead$_S$}{$s'$}} \mathrel{\&\!\&}\\
                & ∀ b ∈ ρ̂_S(s), (c, u', q) ∈ δ̂_U(u), (b ∧ c) ≢ 0 ⟹ \text{\Call{isDead$_U$}{$u'$}}
            \end{aligned}\)
        }
        \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm*}

Similar to the non-symbolic version of the algorithm, \cref{alg:symb-bisim} first check if either of the input states is dead, then  recursively check all the conditions in~\Cref{thm:symb-recursive-construction}, where \(\mathrel{\&\!\&}\) denotes the logical and operator on boolean data type.
The symbolic version of the liveness checking algorithm is also based on a graph search algorithm on all the reachable state, where we try to identify an accepting state by checking whether \(⋁ ϵ̂(s) ≡ 0\).
The only deviation from the non-symbolic cases is that we do not follow empty transitions in the symbolic case, i.e. if \(δ̂(s) ≜ (b, s', p)\) and \(b ≡ 0\), we will not search \(s'\) to check the liveness of \(s'\).
In~\Cref{sec:optimization-implementation}, we will outline a simple modification for the symbolic GKAT coalgebra generation algorithm in~\Cref{sec:symb-gkat-construction} to avoid generating empty transitions, allowing us to skip the emptiness check when deciding whether a state is dead.
TODO: I think we need to make sure that we are presenting the dead state checking algorithm ignoring the empty transition.
After which we can comment it here, to claim that this is for generality.


\section{Symbolic GKAT Coalgebra Construction}\label{sec:symb-gkat-construction}

The final piece of the puzzle is to convert any given expression into an \emph{equivalent} state in some symbolic GKAT coalgebra. 
This goal can be achieved by lifting existing constructions like derivatives and Thompson's construction~\cite{schmid_GuardedKleeneAlgebra_2021,smolka_GuardedKleeneAlgebra_2020} to the symbolic setting.
Then the correctness of non-symbolic version can be used to show the correctness of the symbolic construction i.e. we will prove that the lowering as shown in construction~\labelcref{cons:lowering} of these constructions will yield the conventional derivative.
However, several other important properties, like finiteness of derivative coalgebra and the correctness of the Thompson's construction can be established using a symbolic GKAT coalgebra homomorphism from the Thompson's construction to the derivative.

We introduce some new notations for the transitions of symbolic GKAT coalgebra: for a symbolic GKAT coalgebra \(Ŝ ≜ (S, δ̂, ϵ̂)\) and state \(s ∈ S\), we will use \(s \transAcc{S}{b}\) to denote \(b ∈ ϵ̂(s)\); and use \(s \transvia{b ∣ p}_S s'\) to denote \((b, s', p) ∈ δ̂(s)\).

\begin{figure*}
    \begin{mathpar}
        \inferrule[]{\\}
        {p \transvia{1 ∣ p}_{D̂} 1} \and  
        \inferrule[]{\\}
        {b \transAcc{D̂}{b}} \and  
        \inferrule[]
        {e \transvia{c ∣ p}_{D̂} e'}
        {e +_b f \transvia{b ∧ c ∣ p}_{D̂} e'} 
        \and
        \inferrule[]
        {e \transAcc{D̂}{c}}
        {e +_b f \transAcc{D̂}{b ∧ c}}
        \and
        \inferrule[]
        {f \transvia{c ∣ p}_{D̂} f'}
        {e +_b f \transvia{\overline{b} ∧ c ∣ p}_{D̂} f'}
        \and
        \inferrule[]
        {f \transAcc{D̂}{c}}
        {e +_b f \transAcc{D̂}{\overline{b} ∧ c}}
        \and
        \inferrule[]
        {e \transAcc{D̂}{b} \\ f \transAcc{D̂}{c}}
        {e; f \transAcc{D̂}{b ∧ c}}
        \and 
        \inferrule[]
        {e \transvia{b ∣ p}_{D̂} e'}
        {e; f \transvia{b ∣ p}_{D̂} e'; f}
        \and
        \inferrule[]
        {e \transAcc{D̂}{b} \\ f \transvia{c ∣ p}_{D̂} f'}
        {e; f \transvia{b ∧ c ∣ p}_{D̂} f'}
        \and  
        \inferrule[]
        {\\}
        {e^{(b)} \transAcc{D̂}{\overline{b}}}  
        \and  
        \inferrule[]
        {e \transvia{c ∣ p} e'}
        {e^{(b)} \transvia{b ∧ c ∣ p}_{D̂} e'; e^{(b)}}
    \end{mathpar}
    \caption{Symbolic Derivative Coalgebra \(D̂\)}\label{fig:derivatives-rules}
\end{figure*}

The symbolic derivative coalgebra \(D̂\), with expressions as states, is the least symbolic GKAT coalgebra (ordered by point-wise subset ordering on \(ϵ̂\) and \(δ̂\)) that satisfy the rules in~\Cref{fig:derivatives-rules}.
In more plain words, a transition is in \(D̂\) if and only if it is derivable by the rules in~\Cref{fig:derivatives-rules}.
These rules are very close to the rules of GKAT derivative by~\citeauthor{schmid_GuardedKleeneAlgebra_2021}~\cite{schmid_GuardedKleeneAlgebra_2021}.
This is no coincidence, as our definition exactly lowers to the definition of theirs.
This fact can be proven by case analysis on the shape of the source expression, and forms a basis on our correctness argument.
\begin{theorem}[Correctness]\label{thm:derivative-correctness}
    The lowering of \(D̂\), denoted \(D\), is exactly the derivative defined by~\citeauthor{schmid_GuardedKleeneAlgebra_2021}~\cite{schmid_GuardedKleeneAlgebra_2021}.
    Therefore, the semantics of the expression is equal to the semantics generated by the derivative coalgebra, \(⟦e⟧ = ⟦e⟧_{D} = ⟦e⟧_{D̂}\).
\end{theorem}
\begin{proof}
    TODO: unfold the statement.
\end{proof}


\begin{table*}
    \centering
    \begin{tabular}{c||c|c|l|l}
        Exp & \(S\) & \(s^*\)  
        & \(ϵ̂(s)\) & \(δ̂(s)\) \\\hline
        \(b\) & \(\{s^*\}\) 
        & \(s^*\)
        & \(\{b\}\) & \(∅\) \\  
        \(p\) & \(\{s^*, s₁\}\) 
        & \(s^*\)
        & \(\begin{cases}
           ∅ & s = s^* \\  
           \{1\} & s = s₁ 
        \end{cases}\) 
        & \(\begin{cases}
            \{(1, s₁, 0)\} & s = s^* \\  
            ∅ & s = s₁
        \end{cases}\)\\  
        \(e₁ +_b e₂\) & \(\{s^*\} + S₁ + S₂\) &
        \(s^*\) &
        \(\begin{cases}
            ⟨\{b\}| ϵ̂₁(s₁^*) ∪ ⟨\{b\}| ϵ̂₂(s₂^*) & s = s^* \\
            ϵ̂₁(s) & s ∈ S₁\\
            ϵ̂₂(s) & s ∈ S₂\\
        \end{cases}\) & 
        \(\begin{cases}
            ⟨\{b\}| δ̂₁(s₁^*) + ⟨\{b\}| δ̂₂(s₂^*) & s = s^* \\
            δ̂₁(s) & s ∈ S₁\\
            δ̂₂(s) & s ∈ S₂\\
        \end{cases}\) \\  
        \(e₁ ; e₂\) & \(S₁ + S₂\) & 
        \(s₁^*\) & 
        \(\begin{cases}
            ⟨ϵ̂₁(s)| ϵ̂₂(s₂^*)& s ∈ S₁ \\  
            ϵ̂₂(s) & s ∈ S₂
        \end{cases}\)& 
        \(\begin{cases}
            δ̂₁(s) + ⟨ϵ̂(s)| δ̂₂(s₂^*) & s ∈ S₁ \\  
            δ̂2(s) & s ∈ S₂
        \end{cases}\) \\  
        \(e₁^{(b)}\) & \(\{s^*\} + S₁\) & 
        \(s^*\) &
        \(\begin{cases}
            \{\overline{b}\} & s = s^*\\
            ⟨\{\overline{b}\}| ϵ̂1(s) & s ∈ S₁
        \end{cases}\) & 
        \(\begin{cases}
            ⟨\{b\}| δ̂₁(s₁^*) & s = s^* \\
            δ₁(s) ∪ ⟨\{b\}|⟨ϵ̂1(s)| δ̂₁(s₁^*) & s ∈ S₁
        \end{cases}\)
    \end{tabular}
    \caption{Symbolic Thompson's Construction}\label{tab:symb-Thompson-construction}
\end{table*}

Another way to construct a coalgebra from a GKAT expression is via Thompson's construction, we lift the original construction to the symbolic setting.
A useful operation for Thompson's construction is the following guard operation, denoted by \(⟨B|\), where \(B\) is a set of boolean expressions: for a transition function \(δ̂\), a accepting function \(ϵ̂\), and a state \(s\),
\begin{align*}
    ⟨B|ϵ̂(s) & ≜ \{b ∧ c ∣ b ∈ B, c ∈ ϵ(s)\}; \\
    ⟨B|δ̂(s) & ≜ \{(b ∧ c, s', p) ∣ b ∈ B, (c, s', p) ∈ δ(s)\}.
\end{align*}
Besides guarding transition and acceptance with different conditions in if statements and while loops, the guard operator can also be used to simulate uniform continuation. Specifically, we can use \(⟨ϵ̂(s)|δ(s')\) to connecting all the accepting transition of \(s\) to the dynamic \(δ(s')\).

With these definitions in mind, we can define symbolic Thompson's construction inductively as in~\Cref{tab:symb-Thompson-construction}, where we let \((S₁, ϵ̂₁, δ̂₁)\) and \((S₂, ϵ̂₂, δ̂₂)\) to be result of Thompson's construction for \(e₁\) and \(e₂\) respectively.
In this table, \(S₁ + S₂\) denotes the disjoint union of \(S₁\) and \(S₂\), and for any two transition dynamics \(δ₁(s₁): 𝒫(\BExp × S₁ × K)\) and \(δ₂(s₂): 𝒫(\BExp × S₂ × K)\), we can also compose them in parallel as \(δ₁(s₁) + δ₂(s₂)\):
\begin{align*}
    δ₁(s₁) & + δ₂(s₂): 𝒫(\BExp × (S₁ + S₂) × K) \\*
    δ₁(s₁) & + δ₂(s₂) ≜ \\*
        & \{(b, \mathrm{inj}ₗ(s₁'), p) ∣ (b, s₁', p) ∈ δ₁(s₁)\} ∪ \\*
        & \{(b, \mathrm{inj}ᵣ(s₂'), p) ∣ (b, s₂', p) ∈ δ₂(s₂)\},
\end{align*}
where \(\mathrm{inj}ₗ: S₁ → S₁ + S₂\) and \(\mathrm{inj}ᵣ: S₂ → S₁ + S₂\) are the canonical left/right injection of the coproduct.

Our construction deviates from the original construction~\cite{smolka_GuardedKleeneAlgebra_2020} by using a start state \(s^* ∈ S\) instead of a start dynamics (or pseudo-state).
This choice will make the proof of~\Cref{thm:hom-thompson-derivative} slightly easier. 
However, in~\Cref{sec:optimization-implementation}, we will explain that our implementation uses start dynamics instead of start state, to avoid unnecessary lookups and unreachable states.

We would like to explore several desirable theoretical properties of both derivatives and Thompson's construction.
Specifically, the \emph{correctness}, i.e. the semantics of the ``start state'' the both construction have the same preserves the trace semantics of the expression; \emph{finiteness}, i.e. the coalgebra generated is always finite, implying the termination of our equivalence algorithm; and finally, \emph{complexity}, i.e. the relationship between the number of reachable states and the size of the input expression, which serves as an estimated complexity of our equivalence checking algorithm.
Turns out, all of these questions can be answered by a homomorphism from symbolic Thompson's construction to the symbolic derivatives.

\begin{theorem}\label{thm:hom-thompson-derivative}
    Given any GKAT expression \(e\), the resulting symbolic GKAT coalgebra from Thompson's construction \(Ŝ_e\) have a homomorphism to derivatives \(h: Ŝ_e → D̂\), s.t. for the start state \(s^* ∈ S, h(s^*) = e\).
\end{theorem}

\begin{proof}
    By induction on the structure of \(e\). We will recall that \(h: Ŝ_e → ⟨e⟩_D\) is a symbolic GKAT coalgebra homomorphism when the following two conditions are true: \(s \transAcc{Sₑ}{b}\) if and only if \(h(s) \transAcc{D̂}{b}\); and \(s \transvia{b ∣ p}_{Sₑ} s'\) if and only if \(h(s) \transvia{b ∣ p}_{D̂} h(s')\).

    When \(e ≜ b\) for some tests \(b\), then the function \(h\) is defined as \(\{s^* ↦ b\}\).
    When \(e ≜ p\) for some primitive action \(p\), then the function \(h\) is defined as \(\{s^* ↦ p, * ↦ 1\}\).
    The homomorphism condition can then be verified by unfolding the definition.

    When \(e ≜ e₁ +_b e₂\), by induction hypothesis, we have homomorphisms \(h₁: Ŝ_{e₁} → ⟨e₁⟩_D\) and \(h₂: Ŝ_{e₂} → ⟨e₂⟩_D\).
    Then we define the homomorphism 
    \[h(s) ≜ \begin{cases}
        e₁ +_b e₂ & s = s^* \\  
        h₁(s) & s ∈ Ŝ_{e₁} \\  
        h₂(s) & s ∈ Ŝ_{e₂}
    \end{cases}\]
    We show that \(h\) is a homomorphism. Because \(Ŝₑ\) preserves the transition and acceptance of \(Ŝ_{e₁}\) and \(Ŝ_{e₂}\), then for all \(s ∈ Ŝ_{e₁} ∩ Ŝ_{e}\), we have
    \begin{align*}
        & s \transAcc{Ŝ_{e}}{c} 
        \text{ iff } s \transAcc{Ŝ_{e₁}}{c} \\*
        & \text{ iff } h₁(s) \transAcc{D̂}{c} 
        \text{ iff } h(s) \transAcc{D̂}{c}; \\ 
        & s \transvia{c ∣ p}_{Ŝ_{e}} s' 
        \text{ iff } s \transvia{c ∣ p}_{Ŝ_{e₁}} s' \\*
        & \text{ iff } h₁(s) \transvia{c ∣ p}_{D̂} h₁(s') 
        \text{ iff } h(s) \transvia{c ∣ p}_{D̂} h(s'). 
    \end{align*}
    And similarly for \(s ∈ Ŝ_{e₂} ∩ Ŝ_{e}\).
    So we only need to show the homomorphic condition for the start state \(s^*\):
    \begin{align*}
        & s^* \transAcc{Ŝ_{e}}{c} \\
        \text{iff } &
        (∃ a, b ∧ a = c \text{ and } 
        s₁^* \transAcc{Ŝ_{e₁}}{a}) \\*
        & \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        s₂^* \transAcc{Ŝ_{e₂}}{a}) \\
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        h₁(s₁^*) \transAcc{D̂}{a}) \\*
        & \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        h₂(s₂^*) \transAcc{D̂}{a}) \\  
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        e₁ \transAcc{D̂}{a}) \\*
        & \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        e₂ \transAcc{D̂}{a}) \\  
        \text{iff }& e₁ +_b e₂ \transAcc{D̂}{c}\\
        \text{iff }& h(s^*) \transAcc{D̂}{c}. \\[5px]
        & s^* \transvia{a ∣ p}_{Ŝ_{e}} s' \\
        \text{iff } &
        (∃ a, b ∧ a = c \text{ and } 
        s₁^* \transvia{a ∣ p}_{Ŝ_{e₁}} s') \\*
        & \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        s₂^* \transvia{a ∣ p}_{Ŝ_{e₂}} s') \\
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        h₁(s₁^*) \transvia{a ∣ p}_{D̂} h(s')) \\*
        & \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        h₂(s₂^*) \transvia{a ∣ p}_{D̂} h(s')) \\  
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        e₁ \transvia{a ∣ p}_{D̂} h(s')) \\*
        & \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        e₂ \transvia{a ∣ p}_{D̂} h(s')) \\  
        \text{iff }& e₁ +_b e₂ \transvia{a ∣ p}_{D̂} h(s') \\ 
        \text{iff }& h(s^*) \transvia{a ∣ p}_{D̂} h(s').
    \end{align*}

    When \(e ≜ e₁; e₂\), by induction hypothesis, we have two homomorphisms \(h₁: Ŝ_{e₁} → D̂\) and \(h₂: Ŝ_{e₂} → D̂\).
    We define \(h\) as follows:
    \[h(s) ≜ \begin{cases}
        h₁(s); e₂ & s ∈ Ŝ_{e₁} \\  
        h₂(s) & s ∈ Ŝ_{e₂}
    \end{cases}\]
    Then we can prove that \(h\) is a homomorphism by case analysis on \(s\). 
    First case is that \(s ∈ Ŝ_{e₁}\):
    \begin{align*}
        & s \transAcc{Ŝ_{e}}{c} \\
        \text{ iff } & 
            ∃ a, b, a ∧ b = c, 
                s \transAcc{Ŝ_{e₁}}{a} 
                \text{ and } 
                s₂^* \transAcc{Ŝ_{e₂}}{b} \\
        \text{ iff } & 
            ∃ a, b, a ∧ b = c, 
                h₁(s) \transAcc{D̂}{a} 
                \text{ and } 
                f \transAcc{D̂}{b} \\
        \text{ iff } & h₁(s); f \transAcc{D̂}{c} 
        \text{ iff } h(s) \transAcc{D̂}{c}.\\[5px]
        & s \transvia{c ∣ p}_{S_{e}} s' \\
        \text{ iff } & 
            \begin{aligned}[t]
                (∃ a, b, a ∧ b = c
                & \text{ and }
                s \transAcc{Ŝ_{e₁}}{a} \\*
                & \text{ and }
                s₂^* \transvia{b ∣ p}_{Ŝ_{e₂}} s') 
            \end{aligned} \\*
            & \text{ or }
            (s \transvia{c ∣ p}_{Ŝ_{e₁}} s') \\  
        \text{ iff } & 
            \begin{aligned}[t]
                (∃ a, b, a ∧ b = c
                & \text{ and }
                h₁(s) \transAcc{D̂}{a} \\*
                &\text{ and }
                e₂ \transvia{b ∣ p}_{D̂} h₂(s'))
            \end{aligned} \\*
            & \text{ or }
            (h₁(s) \transvia{c ∣ p}_{D̂} h₁(s')) \\
        \text{ iff } & h₁(s) \transvia{c ∣ p} h(s') 
        \text{ iff } h(s) \transvia{c ∣ p} h(s').
    \end{align*}
    The case where \(s₂ ∈ Ŝ_{e₂}\) is straightforward, as \(Ŝ_{e}\) preserves the transitions of \(Ŝ_{e₂}\):
    \begin{align*}
        & s \transAcc{Ŝ_{e}}{c} 
        \text{ iff } s \transAcc{Ŝ_{e₂}}{c} \\*
        & \text{ iff } h₂(s) \transAcc{D̂}{c} 
        \text{ iff } h(s) \transAcc{D̂}{c}, \\ 
        & s \transvia{c ∣ p}_{Ŝ_{e}} s' 
        \text{ iff } s \transvia{c ∣ p}_{Ŝ_{e₂}} s' \\*
        & \text{ iff } h₂(s) \transvia{c ∣ p}_{D̂} h₂(s') 
        \text{ iff } h(s) \transvia{c ∣ p}_{D̂} h(s').
    \end{align*}
    

    When \(e ≜ {e₁}^{(b)}\), by induction hypothesis, we have a homomorphism \(h₁: Ŝ_{e₁} → D̂\); the homomorphism \(h\) can be defined as follows: 
    \[h(s) ≜ \begin{cases}
        {e₁}^{(b)} & s ≜ s^* \\  
        h₁(s); e₁^{(b)} & s ∈ Ŝ_{e₁}
    \end{cases}\]
    We prove the homomorphism condition by case analysis on \(s\). First case is that \(s = s^*\), then:
    \begin{align*}
        (s^* \transAcc{Ŝ_e}{c})
        \text{ iff } & (s^* \transAcc{Ŝ_e}{c} \text{ and } c = \overline{b}) \\*
        \text{ iff } & ({e₁}^{(b)} \transAcc{D̂}{c} \text{ and } c = \overline{b}) \\*
        \text{ iff } & (h(s^*) \transAcc{D̂}{c}); \\
        (s^* \transvia{c ∣ p}_{Ŝ_e} s')
        \text{ iff } & (∃ a, b ∧ a = c \text{ and } s₁^* \transvia{a ∣ p}_{Ŝ_{e₁}} s') \\
        \text{ iff } & (∃ a, b ∧ a = c \text{ and } e₁ \transvia{a ∣ p}_{D̂} h₁(s')) \\ 
        \text{ iff } & {e₁}^{(b)} \transvia{a ∣ p}_{D̂} h₁(s') 
        \text{ iff } h(s^*) \transvia{a ∣ p}_{D̂} h(s')
    \end{align*}
    The second case is when \(s ∈ Ŝ_{e₁}\), then:
    \begin{align*}
        & s \transAcc{Ŝ_e}{c}\\
        \text{ iff } & (∃ a, \overline{b} ∧ a = c \text{ and } s \transAcc{Ŝ_{e₁}}{a}) \\  
        \text{ iff } & (∃ a, \overline{b} ∧ a = c \text{ and } h₁(s) \transAcc{D̂}{a}) \\
        \text{ iff } & h₁(s);e₁^{(b)} \transAcc{D̂}{c} 
        \text{ iff } h(s) \transAcc{D̂}{c} \\[5px]
        & s \transvia{c ∣ p}_{Ŝ_e} s' \\
        \text{ iff } & 
            (s \transvia{c ∣ p}_{Ŝ_{e₁}} s' \\
            & \text{ or } 
            \begin{aligned}[t]
                ∃ a₁, a₂, {}
                & b ∧ a₁ ∧ a₂ = c \\*
                & \text{ and } 
                s \transAcc{Ŝ_{e₁}}{a₁} \\*
                & \text{ and } 
                s₁^* \transvia{a₂ ∣ p}_{Ŝ_{e₁}} s')
            \end{aligned} \\
        \text{ iff } & 
            (h₁(s) \transvia{c ∣ p}_{D̂} h₁(s') \\*
            & \text{ or } 
            \begin{aligned}[t]
                ∃ a₁, a₂, &{} b ∧ a₁ ∧ a₂ = c \\*
                & \text{ and } 
                h₁(s) \transAcc{D̂}{a₁} \\*
                & \text{ and } 
                e₁ \transvia{a₂ ∣ p}_{D̂} h₁(s')) 
            \end{aligned}\\
        \text{ iff } & 
            (h₁(s) \transvia{c ∣ p}_{D̂} h₁(s')  \\
            & \text{ or } 
            \begin{aligned}[t]
                ∃ a₁, a₂, &{} b ∧ a₁ ∧ a₂ = c \\
                & \text{ and } 
                h₁(s) \transAcc{D̂}{a₁} \\
                &{} \text{ and } 
                e₁^{(b)} \transvia{b ∧ a₂ ∣ p}_{D̂} h₁(s');e₁^{(b)}) \\ 
            \end{aligned}\\
        \text{ iff } &
            (h₁(s); e₁^{(b)} \transvia{c ∣ p}_{D̂} h₁(s'); e₁^{(b)}) \\
        \text{ iff } & h(s) \transvia{c ∣ p}_{D̂} h(s').
        \qedhere
    \end{align*}
\end{proof}

\Cref{thm:hom-thompson-derivative} have several consequences, one of the more obvious one is that we can use the functoriality of the lowering operation to show the semantic equivalence of the start state in the Thompson's construction and the expression in derivative. 

\begin{corollary}[Correctness]
    Given any expression \(e\) and its Thompson's coalgebra \(Ŝ_{e}\) with the start state \(s^* ∈ Ŝ_{e}\), then the semantics of the start state is equivalent to the semantics of \(e\): \(⟦s^*⟧_{Ŝ_{e}} = ⟦e⟧.\)
\end{corollary}

\begin{proof}
    By functoriality of lowering~\cref{thm:lowering-functor}, then \(h: S_{e} → D\) is a homomorphism on their lowerings. 
    Because homomorphism preserves semantics (\Cref{thm:hom-preserves-semantics}) \(⟦s^*⟧^{ω}_{S_{e}} = ⟦h(s^*)⟧^{ω}_{D} = ⟦e⟧^{ω}_{D}\), where \(⟦-⟧^{ω}\) is the infinite trace semantics i.e. the unique map into the final GKAT coalgebra \(𝒢_ω\).

    Finally, because infinite trace equivalence implies finite trace equivalence (\Cref{thm:inf-trace-equiv-implies-fin-trace-equiv}) and the correctness of derivative (\Cref{thm:derivative-correctness}), we obtain the following chain of equalities:
    \(⟦s^*⟧_{S_{e}} = ⟦e⟧_{D} = ⟦e⟧\).
\end{proof}

A not so obvious consequence of the homomorphism in~\Cref{thm:hom-thompson-derivative}, is the complexity of the algorithm based on derivatives.
Our bisimulation algorithm (\Cref{alg:symb-bisim}) only explores the principle sub-coalgebra of the start state, i.e. \(s^*\) in the Thompson's construction \(Ŝ_{e}\) or \(e\) in the derivative \(D̂\); thus, deducing an upper bound on the size of the principle sub-coalgebras \(⟨s^*⟩_{Ŝ_{e}}\) and \(⟨e⟩_{D̂}\) are crucial to our complexity analysis.
An upper bound on \(⟨s^*⟩_{Ŝ_{e}}\) is easy to obtain, as the size of \(Ŝ_{e}\), which subsumes the states of \(⟨s^*⟩_{Ŝ_{e}}\), is linear to the size of expression \(e\); therefore \(⟨s^*⟩_{Ŝ_{e}}\) is at most linear to the size of the expression \(e\).
On the other hand the size of \(⟨e⟩_{D̂}\) can, again, be derived from the homomorphism in~\cref{thm:hom-thompson-derivative}.

\begin{corollary}\label{thm:suj-hom-thompson-derivative}
    There exists a surjective homomorphism \(h': ⟨s^*⟩_{Ŝ_{e}} → ⟨e⟩_{D̂}\). 
    Because the size of \(⟨s^*⟩_{Ŝ_{e}}\) is linear to \(e\), the size of \(⟨e⟩_{D̂}\) is at most linear to the size of expression \(e\).
\end{corollary}

\begin{proof}
    We define \(h'\) to be point-wise equal to \(h\), i.e. \(h'(s) ≜ h(s)\), i.e. \(h'\) is \(h\) restricted on the domain \(⟨s^*⟩_{Ŝ_{e}}\). 
    We need to show that \(h'\) is well-defined and surjective, which is a consequence of homomorphic image preserves principle sub-coalgebra (\Cref{thm:homo-img-preserve-principle-sub-coalg}): \(h(⟨s^*⟩_{Ŝ_{e}}) = ⟨h(s)⟩_{D̂} = ⟨e⟩_{D̂}.\)
    In other words, the image of \(h\) on \(⟨s^*⟩_{Ŝ_{e}}\) is equal to \(⟨e⟩_{D̂}\); thus, because \(h'\) the restriction of \(h\) on \(⟨s^*⟩_{Ŝ_{e}}\), the range of \(h'\) contains its codomain \(⟨e⟩_{D̂}\), showing that \(h'\) is surjective.
\end{proof}

Because of the surjectivity of \(h'\) in~\Cref{thm:suj-hom-thompson-derivative}, \(⟨s^*⟩_{Ŝ_{e}}\) has no fewer states than \(⟨e⟩_{D̂}\).
And, the number of states in \(Ŝ_{e}\) is greater than \(⟨s^*⟩_{Ŝ_{e}}\), which is greater than \(⟨e⟩_{D̂}\); by induction, the number of states in \(Ŝ_{e}\) is linear to the size of the input expression, therefore the number of states in \(⟨e⟩_{D̂}\) is at most linear to the size of the expression \(e\). 

Although the size of the coalgebra generated by the derivative is smaller than Thompson's construction, the decision procedure based on derivative is not always more efficient than those based on Thompson's construction. 
Crucially, the states in the derivative coalgebra \(D̂\) are expressions, which are more expensive to store; and computing the next transition of the coalgebra can also be more computationally expensive.
Whereas, Thompson's construction only requires inductively going through the expression once to construct the entire coalgebra \(Ŝ_{e}\), and its states can be represented by more efficient constructs, like integers. 

\section{Implementation}
We implement our symbolic on-the-fly bisimulation algorithm~(\Cref{alg:symb-bisim}) in Rust along with derivative based~(\Cref{fig:derivatives-rules}) and Thompson's construction based~(\Cref{tab:symb-Thompson-construction}) algorithms for constructing symbolic GKAT coalgebras. The source code and benchmark suite is freely available in our repository TODO.

\subsection{Optimization}\label{sec:optimization-implementation}
\begin{definition}[blocked transition]
    For any \(s \in S\), a transition \((b, s', p) \in \hat{\delta}_S(s)\) is blocked if \(b \equiv 0\).
\end{definition}

Notice that during bisimulation~(\Cref{alg:symb-bisim}) for some \(s \in S\) and \(u \in U\), if there is \((b,s',p) \in \hat{\delta}_S(s)\) where \(b\) is a semantically false boolean expression, then implications of form \((b \land \cdot) \not{\equiv} 0 \Rightarrow \cdot\) are trivially satisfied. In other words, \textit{blocked} transitions in \(S\) do not contribute to the result of bisimulation. The same observation holds true symmetrically for \(U\). So blocked transitions of a coalgebra can be safely pruned without impacting the result of bisimulation.

While blocked transitions are irrelevant regarding the result of bisimulation, they can negatively impact the performance of bisimulation as the algorithm may perform many unneeded satisfiability checks. To prevent performance degradation due to blocked transitions, we remove them through an \textit{eager-pruning} optimization. Basically, in our coalgebra construction algorithms, \(\hat{\delta}\) is constructed with only transitions \((b, s, p)\) where \(b \not{\equiv} 0\). The coalgebras produced in this manner essentially have their blocked transitions pruned at the time of construction (eager). 

Eager-pruning also improves the efficiency of the Thompson's construction algorithm~(\Cref{tab:symb-Thompson-construction}) significantly. This is due to the fact that eager-pruning can reduce the size of \(\hat{\delta}'\) computed recursively for sub-expressions, which in turn makes computing \(\hat{\delta}\) for the overall expression faster.

As mentioned in \Cref{sec:symb-gkat-construction}, our implementation of Thompson's construction uses start dynamics (pseudo-state) instead of an explicit start state. This reduces the number of indirections needed to access the behaviors \(\hat{\epsilon}(s^*) \) and \(\hat{\delta}(s^*)\) of start state \(s^*\). More specifically, we track the behaviors \(\epsilon^*\) and \(\delta^*\) which characterize \(\hat{\epsilon}(s^*) \) and \(\hat{\delta}(s^*)\) respectively. The start state \(s^*\) does not appear explicitly during coalgebra construction. After the construction algorithm terminates, we generate a fresh state \(s^*\) and set \(\hat{\epsilon}(s^*) = \epsilon^*\) and \(\hat{\delta}(s^*) = \delta^*\). The final coalgebra that our implementation produces is equivalent to what would have been obtained from the original algorithm (\Cref{tab:symb-Thompson-construction}) modulo consistent renaming of states.

\subsection{Performance}\label{sec:performance-implementation}
Our experiments are performed on a laptop with an Apple M4 Pro CPU and 24 GB RAM. \Cref{tab:benchmark} presents the results of equivalence checking based on derivatives (DRV), Thompson's construction (TC) and SymKAT~\cite{pous_SymbolicAlgorithmsLanguage_2015}. Each benchmark consists of 50 expression pairs with properties described by the benchmark's name. For example, expressions in E250B5P10RD have around 250 primitive actions in total (E250), a maximum boolean expression size of 5 (B5), 10 possible unique primitive actions (P10) and are completely random (RD). Benchmarks with the suffix EQ have expression pairs that are known to be equivalent. The time columns measure the total time used (in seconds) for checking all expression pairs in the benchmarks. The mem columns measure the peak memory used (in MB).

It is clear from these results that our algorithms (DRV and TC) are more efficient than SymKAT in terms of both time and memory usage. Moreover, our Thompson's construction (TC) based algorithm is the only one which successfully completed all benchmarks without timeout.

\begin{table*}
\centering
\begin{tabular}{l l l l l l l}
    Benchmark & Time (DRV) & Time (TC) & Time (SK) & Mem (DRV) & Mem (TC) & Mem (SK) \\
    \hline
    E250B5P10RD    & 0.26    & 0.25  & 16.99   & 15.36  & 14.76  & 329.48  \\
    E250B5P10EQ    & 0.28    & 0.25  & 2.99    & 15.56  & 14.95  & 100.48  \\
    E500B5P50RD    & 0.30    & 0.28  & 113.88  & 16.26  & 15.49  & 1560.54 \\
    E500B5P50EQ    & 0.35    & 0.28  & 14.06   & 17.97  & 15.54  & 546.91  \\
    E1000B10P100RD & 0.43    & 0.37  & Timeout & 18.36  & 21.41  & N/A     \\
    E1000B10P100EQ & 0.61    & 0.35  & 77.83   & 21.49  & 17.66  & 5822.66 \\
    E2000B20P200RD & 3.25    & 2.62  & Timeout & 241.07 & 283.45 & N/A     \\
    E2000B20P200EQ & Timeout & 46.45 & Timeout & N/A    & 638.00 & N/A     \\
\end{tabular}
\caption{Benchmarks}\label{tab:benchmark}
\end{table*}

\section{Related Works and Discussions}

\subsection{Generic Symbolic Techniques}

There are many studies that utilizes symbolic techniques to solve various problems surrounding (extended) regular expressions, and have found a wide range of real-world applications, including but not limited to low level program analysis~\cite{dallapreda_AbstractSymbolicAutomata_2015a}, list comprehension~\cite{saarikivi_FusingEffectfulComprehensions_2017}, constraint solving~\cite{stanford_SymbolicBooleanDerivatives_2021}, HTML decoding, malware fingerprinting, image blurring, location privacy~\cite{veanes_SymbolicFiniteState_2012}, regex processing, and string sanitizer~\cite{veanes_ApplicationsSymbolicFinite_2013}.

Our study, on the other hand,  focus on GKAT and GKAT automata (represented as coalgebra throughout the paper). 
Although previous works on deterministic symbolic transducer~\cite{saarikivi_FusingEffectfulComprehensions_2017,veanes_SymbolicFiniteState_2012} might seem similar to that of GKAT, the automata shape are subtly different, specifically, instead of having accept and rejecting states, states in GKAT automata will accept or reject its input.

Another difference between aforementioned works and ours is that we utilize the coalgebraic theory of GKAT to streamline some of our proofs.
In fact, we are not the first to look at symbolic transition system through the lens of coalgebra, \citeauthor{bonchi_CoalgebraicSymbolicSemantics_2009}~\cite{bonchi_CoalgebraicSymbolicSemantics_2009} have an elegant coalgebra theory surrounding symbolic transition system.
However, instead of defining the symbolic semantics via the coalgebra theory, we opt to use the notion of lowering to connect symbolic GKAT coalgebra and GKAT coalgebra.
This approach allows us to leverage previous correctness proofs like in~\Cref{thm:derivative-correctness}, and also enables a non-symbolic equivalence-checking algorithm as in~\cref{alg:bisim}.
Another notable difference is that our equivalence checking also need to handle normalization, which is a unique property of GKAT not found in general coalgebra.

\subsection{KAT and GKAT}

GKAT is a guarded fragment of Kleene Algebra with Tests (KAT), which enjoys a symbolic algorithm~\cite{pous_SymbolicAlgorithmsLanguage_2015}.
This algorithm by~\citeauthor{pous_SymbolicAlgorithmsLanguage_2015} generalizes the notion of derivatives also to boolean expressions, which uses binary decision diagram (BDD) as transition between symbolic expressions.
Although some of our examples utilizes BDD to solve equivalence and inequivalence of boolean expressions, because of the structure of GKAT, our algorithm is not bounded by a particular boolean representation or solvers.
In fact, we have demonstrated that in many scenarios, solvers like z3 and sentential decision diagram (SDD) can achieve better performance than BDD.

In similar veins, KATch~\cite{moeller_KATchFastSymbolic_2024} is a symbolic solver for NetKAT~\cite{anderson_NetKATSemanticFoundations_2014} based on forwarding decision diagram, and achieved outstanding performance among state-of-the-art tools for reasoning about software-defined networks.
GKAT also stem from the research of the research of software-defined networks~\cite{smolka_ScalableVerificationProbabilistic_2019,smolka_GuardedKleeneAlgebra_2020}, and quickly found applications in probabilistic verification~\cite{ro.zowski_ProbabilisticGuardedKAT_2023}, probabilistic program logic~\cite{gomes_KleeneAlgebraTests_2024}, networks~\cite{wasserstein_GUARDEDNETKATSOUNDNESS_2023}, and control-flow validation~\cite{zhang_CFGKATEfficientValidation_2025}.
It would be interesting to see how our symbolic algorithm can be used to speed up these applications.




\printbibliography

\newpage
\appendix


\end{document}
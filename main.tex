% Please compile this document using LuaLaTeX.
% because of the use of unicode-math
% XeLaTeX and PDFLaTeX will result in error.

\newif\iffull\fulltrue
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

% page number
\pagestyle{plain}

% \usepackage{cite}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%%%%% PACKAGES
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[margin=2.5cm]{geometry}

% move proofs to the end
\usepackage[conf={restate,no link to proof}]{proof-at-the-end}

% allow page break in align environment
\allowdisplaybreaks

% Biblatex
\usepackage[style=numeric]{biblatex}
\addbibresource{refs.bib}

% For adding inline comments in the text.
\usepackage[margin=false,inline=true]{fixme}
\FXRegisterAuthor{aaa}{anaaa}{\color{cyan}AAA}
\FXRegisterAuthor{mg}{anmg}{\color{red}MG}
\FXRegisterAuthor{cz}{ancz}{\color{orange}CZ}
% \newcommand{\aaa}[1]{\aaanote{#1}}
% \newcommand{\mg}[1]{\mgnote{#1}}
% \newcommand{\cz}[1]{\cznote{#1}}
\newcommand{\aaa}[1]{}
\newcommand{\mg}[1]{}
\newcommand{\cz}[1]{}

\usepackage{stmaryrd}

\usepackage{stackengine}
\usepackage{mathrsfs}
\usepackage{braket}
\usepackage{annotate-equations}
\usepackage{scalerel}

% graphs
\usepackage{tikz}
\usetikzlibrary{arrows,automata,positioning}

% commutative diagram
\usepackage{tikz-cd}

% ref
\usepackage{hyperref}
\usepackage{cleveref}

% inference rule
\usepackage{mathpartir}
% cross-referencing infer rule
% based on https://tex.stackexchange.com/questions/340788/cross-referencing-inference-rules
\makeatletter
\let\originferrule\inferrule
\DeclareDocumentCommand \inferrule { s O {} m m}{%
  \IfBooleanTF{#1}%
  {%
    \mpr@inferstar[#2]{#3}{#4}%
  }{%
    \mpr@inferrule[#2]{#3}{#4}%
  }%
  \IfValueT{#2}%
  {%
    \my@name@inferrule{#2}%
  }%
}
\NewDocumentCommand \my@name@inferrule { m }{%
  \def\@currentlabelname{\textsc{#1}}%
}
\makeatother

% item spacing
\usepackage{enumitem}

% for code
\usepackage{listings}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{framextopmargin=50pt,frame=bottomline}
\lstset{language=caml, escapeinside={[*}{*]}}
% Clever ref names
\crefname{lstlisting}{listing}{listings}
\Crefname{lstlisting}{Listing}{Listings}

% for algorithm
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
% switch statement for pattern matching
\algnewcommand\algorithmicmatch{\textbf{match}}
\algnewcommand\algorithmicwith{\textbf{with}}
\algnewcommand\algorithmiccase{\textbf{case}}
\algnewcommand\algorithmicdefault{\textbf{default}}
\algnewcommand\Continue{\textbf{continue}}
\algdef{SE}[MATCH]{Match}{EndMatch}[1]{\algorithmicmatch\ #1\ \algorithmicwith}{\algorithmicend\ \algorithmicmatch}%
\algdef{SE}[CASE]{Case}{EndCase}[1]{\algorithmiccase\ #1 \algorithmicthen}{\algorithmicend\ \algorithmiccase}%
\algdef{SE}[DEFAULT]{Default}{EndDefault}{\hskip\algorithmicindent\algorithmicdefault}{\algorithmicend\algorithmicdefault}%
\algtext*{EndMatch}%
\algtext*{EndCase}%
\algtext*{EndDefault}%

% for better table
\usepackage{booktabs}

% subcaption
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption

% unicode math symbols
\usepackage{unicode-math}
% support for hat, overline, underline, vec, and sim combining charactors
\protected\def\afteracc{\directlua{
    local nest = tex.nest[tex.nest.ptr]
    local last = nest.tail
    if not (last and last.id == 18) then
      error'I can only put accents on simple noads.'
    end
    if last.sub or last.sup then
      error'If you want accents on a superscript or subscript, please use braces.'
    end
    local acc = node.new(21, 1)
    acc.nucleus = last.nucleus
    last.nucleus = nil
    local is_bottom = token.scan_keyword'bot' and 'bot_accent' or 'accent'
    acc[is_bottom] = node.new(23)
    acc[is_bottom].fam, acc[is_bottom].char = 0, token.scan_int()
    nest.head = node.insert_after(node.remove(nest.head, last), nil, acc)
    nest.tail = acc
    node.flush_node(last)
  }}
\AtBeginDocument{
\begingroup
  \def\UnicodeMathSymbol#1#2#3#4{%
    \ifx#3\mathaccent
      \def\mytmpmacro{\afteracc#1 }%
      \global\letcharcode#1=\mytmpmacro
      \global\mathcode#1="8000
    \else\ifx#3\mathbotaccentwide
      \def\mytmpmacro{\afteracc bot#1 }%
      \global\letcharcode#1=\mytmpmacro
      \global\mathcode#1="8000
    \fi\fi
  }
  \input{unicode-math-table}
\endgroup
}

% math font, this is needed to render \setminus command
\setmathfont{latinmodern-math}
\setmathfont[range=\setminus]{STIX Two Math}
\setmathfont[range=\similarrightarrow]{STIX Two Math}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}

\newtheorem{definition}{Definition}
\newtheorem*{definition*}{Definition}
\newtheorem{example}[definition]{Example}
\newtheorem{remark}[definition]{Remark}

%%%% Macros %%%%%

% Math?
\newcommand{\true}{\mathrm{true}}
\newcommand{\false}{\mathrm{false}}
\newcommand{\At}{\mathbf{At}}


% operators
\newcommand{\dom}[1]{\mathrm{dom}(#1)}
\newcommand{\cod}[1]{\mathrm{cod}(#1)}
\DeclareMathOperator{\post}{\mathrm{post}}
\newcommand{\reject}{\mathinner{\mathrm{rej}}}
\newcommand{\accept}{\mathinner{\mathrm{acc}}}
\DeclareMathOperator*{\bigplus}{\scalerel*{+}{\sum}}
\newcommand{\clos}[1]{\mathrel{\overline{#1}}}
\DeclareMathOperator{\norm}{\mathrm{norm}}
\DeclareMathOperator{\dead}{\mathrm{dead}}
\DeclareMathOperator{\symb}{\mathrm{symb}}
\DeclareMathOperator{\unsymb}{\symb^{-1}}


% commands 
\newcommand{\command}[1]{{\mathtt{#1}}}
\newcommand{\comAssume}[1]{\command{assume}~#1}
\newcommand{\comITE}[3]{\command{if}~#1~\command{then}~#2~\command{else}~#3}
\newcommand{\comWhile}[2]{\command{while}~#1~\command{do}~#2}

% set of models
\newcommand{\theoryOf}[1]{\ensuremath{\mathsf{#1}}}
\newcommand{\Exp}{\theoryOf{Exp}}
\newcommand{\BExp}{\theoryOf{BExp}}
\DeclareMathOperator{\GS}{\mathrm{GS}}

\newcommand\altxrightarrow[2][0pt]{\mathrel{\ensurestackMath{\stackengine%
  {\dimexpr#1-7.5pt}{\xrightarrow{\phantom{#2}}}{\scriptstyle\!#2\,}%
  {O}{c}{F}{F}{S}}}}
\newcommand{\transvia}[1]{
    \mathrel{\raisebox{-2px}{\(\altxrightarrow[-2px]{#1}\)}}
}
\newcommand{\transAcc}[2]{⇒_{#1} #2}
\newcommand{\transRej}[2]{↓_{#1} #2}

 

\begin{document}

\title{On-the-fly Algorithms for GKAT Equivalences
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Cheng Zhang\textsuperscript{\textsection}}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{University College London}\\
London, United Kingdom \\
0000-0002-8197-6181}
\and
\IEEEauthorblockN{Qiancheng Fu}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
qcfu@bu.edu}
\and
\IEEEauthorblockN{Hang Ji}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
email address or ORCID}
\and
\IEEEauthorblockN{Ines Santacruz}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
email address or ORCID}
\and
\IEEEauthorblockN{Marco Gaboardi}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
email address or ORCID}
}

\maketitle
\begingroup\renewcommand\thefootnote{\textsection}
\footnotetext{Work largely performed at Boston University}
\endgroup

\begin{abstract}
We present several new algorithms to efficiently decide Guarded Kleene Algebra with Tests (GKAT) equivalences.
Although the current algorithm scales nearly-linearly with the size of the expression, its performance is hindered by the need to compute the entire coalgebra for the sake of normalization and exponential performance degradation when expanding the set of primitive tests.
To address these problems, we introduce two new algorithms for bisimulation checking: an on-the-fly bisimulation algorithm based on greedy bisimulation, which enables immediate termination upon discovering a counter-example and supports on-the-fly coalgebra generation with derivatives; and a symbolic algorithm that builds upon the on-the-fly algorithm while leveraging efficient SAT solvers to speed up boolean comparisons and compress the transition structures of GKAT coalgebra.
To utilize the symbolic bisimulation checking, we also provide two methods to generate symbolic GKAT coalgebras from GKAT expressions, analogous to derivative and Thompson's construction in previous works.
We have proven the correctness of our algorithms and conducted detailed complexity analyses.
Finally, we implemented the symbolic algorithms in Rust, demonstrating significant performance improvements over state-of-the-art tools.
\end{abstract}

\begin{IEEEkeywords}
Guarded Kleene Algebra With Tests, Kleene Algebra, Coalgebra, Program Verification, Algorithm
\end{IEEEkeywords}



\section{Introduction}

\paragraph{Notation:} In this paper, we will use un-curried notation to apply curried functions, for example, given a function \(δ: X → Y → Z\), we will write the function applications as \(δ(x): Y → Z\) and \(δ(x, y): Z\).

\section{Background on Coalgebra and GKAT}

\subsection{Concepts in Universal Coalgebra}

In this paper, we will make heavy use of coalgebraic theory, thus we will recall useful notions and theorems in universal coalgebra.
Given a functor \(F\) on the category of set and functions, a \emph{coalgebra over \(F\)} or \emph{\(F\)-coalgebra} consists of a set \(S\) and a function \(σ_S: S → F(S)\).
We typically call elements in \(S\) the \emph{states} of the coalgebra, and \(σ_S(s)\) the \emph{dynamic} of state \(s\).
We will sometimes use the states \(S\) to denote the coalgebra, when no ambiguity can arise. 

A homomorphism between two \(F\)-coalgebra \(S\) and \(U\) is a map \(h: S → U\) that preserves the function \(σ\); diagrammatically, the following diagram commutes:
\[
    \begin{tikzcd}
        S \ar{r}{h} \ar[swap]{d}{σ_S} & U \ar{d}{σ_U} \\  
        F(S) \ar{r}{F(h)} & F(U)
    \end{tikzcd}    
\]

When we can restrict the homomorphism map into an inclusion map \(i: S' → S\) for \(S' ⊆ S\) then we say that \(S'\) is a \emph{sub-coalgebra} of \(S\), denoted as \(S' ⊑ S\). Specifically, the following diagram commutes when \(S' ⊑ S\):
\[
    \begin{tikzcd}
        S' \ar[hook]{r}{i} \ar[swap]{d}{σ_{S'}} & S \ar{d}{σ_S} \\  
        F(S') \ar[hook]{r}{F(i)} & F(S)
    \end{tikzcd}    
\]
In fact, the function \(σ_{S'}\) is uniquely determined by the states \(S'\)~\cite[Proposition 6.1]{rutten_UniversalCoalgebraTheory_2000}.
Sub-coalgebras are also preserved under homomorphic images and pre-images: 
\begin{theoremEnd}{lemma}[Theorem 6.3~\cite{rutten_UniversalCoalgebraTheory_2000}]\label{thm:hom-(pre)img-preserve-sub-coalg}
    given a homomorphism \(h: S → U\), and sub-coalgebras \(S' ⊑ S\) and \(U' ⊑ U\), then 
    \[h(S') ⊑ U \text{ and } h^{-1}(U') ⊑ S.\]
\end{theoremEnd}

One particularly important sub-coalgebra of a coalgebra \(S\) is the least sub-coalgebra that contains a state \(s\). 
We will denote this sub-coalgebra as \(⟨s⟩_{S}\), and call it \emph{principle sub-coalgebra} generated by \(s\). 
We sometimes omit the subscript \(S\) when it can be inferred from context or irrelevant.
Intuitively, we usually think of principle sub-coalgebra \(⟨s⟩_S\) as the sub-coalgebra that is formed by all the ``reachable state'' from \(s\).
This coalgebraic characterization of reachable state can allow us to avoid induction on the length of path from \(s\) to a state in \(⟨s⟩_S\).

For all coalgebra \(S\) and a state \(s ∈ S\), principle sub-coalgebra \(⟨s⟩_S\) always exists and is unique, because sub-coalgebra of any coalgebra forms a complete lattice~\cite[theorem 6.4]{rutten_UniversalCoalgebraTheory_2000}; thus taking the meet of all the sub-coalgebra that contains \(s\) will yield \(⟨s⟩_S\).

Similar to sub-coalgebra, principle sub-coalgebra is also preserved under homomorphic image:
\begin{theoremEnd}{theorem}\label{thm:homo-img-preserve-principle-sub-coalg}
    Homomorphic image preserves principle sub-GKAT coalgebra. Specifically, given a homomorphism \(h: S → U\):
    \[h(⟨s⟩_S) = ⟨h(s)⟩_U\]
\end{theoremEnd}

\begin{proofEnd}
    We will need to show that \(h(⟨s⟩_{S})\) is the smallest sub-GKAT coalgebra of \(S\) that contain \(h(s)\). 
    By definition of image, \(h(⟨s⟩_{S})\) indeed contain \(h(s)\). 
    By \Cref{thm:hom-(pre)img-preserve-sub-coalg}, \(h(⟨s⟩_S) ⊑ U\), i.e. \(h(⟨s⟩_S)\) is a sub-coalgebra of \(U\).
    Finally, take any sub-coalgebra \(U' ⊑ U\) s.t. \(h(s) ∈ U'\): 
    \begin{align*}
        h(s) ∈ U' 
        & ⟹ s ∈ h^{-1}(U') \\  
        & ⟹ ⟨s⟩_S ⊑ h^{-1}(U') & \text{definition of \(⟨s⟩_S\)}\\  
        & ⟹ h(⟨s⟩_S) ⊑ U' & \text{\Cref{thm:hom-(pre)img-preserve-sub-coalg}}
    \end{align*}
    Hence \(h(⟨s⟩_S)\) is the smallest sub-GKAT coalgebra of \(U\) that contains \(h(s)\).
\end{proofEnd}

% bisimulation

A \emph{final coalgebra} \(ℱ\) over a signature \(F\), sometimes called the \emph{behavior} or \emph{semantics} of coalgebras over \(F\), is an \(F\)-coalgebra s.t. for all \(F\)-coalgebra \(S\), there exists a unique homomorphism \(\mathrm{beh}_S: S → ℱ\).

Given two \(F\)-coalgebra \(S\) and \(U\), the \emph{behavioral equivalence} between states in \(S\) and \(U\) can be computed by a notion called \emph{bisimulation}.
A relation \({∼} ⊆ S × U\) is called a \emph{bisimulation relation} if it forms an \(F\)-coalgebra: \[σ_{∼}: {∼} → F(∼),\] 
and its projections \(π₁: {∼} → S\) and \(π₂: {∼} → U\) are both homomorphisms:
\[
    \begin{tikzcd}[column sep=1.25cm]
        S \ar[swap]{d}{σ_S}
            & ∼ \ar[swap]{l}{π₁} \ar{r}{π₂} \ar{d}{σ_{∼}} 
            & U \ar{d}{σ_T}\\  
        F(S) & F(∼) \ar{l}{F(π₁)} \ar[swap]{r}{F(π₂)} & F(T)
    \end{tikzcd}
\]
In a special case, when there exists a homomorphism \(h: S → U\), then we can simply pick \({∼} ⊆ S × U\) to be \(\{(s, h(s)) ∣ s ∈ S\}\), which gives us a bisimulation with the lift \(σ_{∼} ≜ σ_S × σ_U\).

When given a homomorphism \(h: S → U\), we can construct a bisimulation \({∼} ⊆ S × U\), where it relates all the states \(s ∈ S\) with \(h(s) ∈ U\); and each component of the pair will transition individually via their respective transition function \(δ_∼ ≜ δ_S × δ_U\), where \(×\) is the bifunctorial lift of product.
\begin{theoremEnd}{corollary}\label{thm:hom-preserves-semantics}
    Given a homomorphism \(h: S → U\) between two \(F\)-coalgebra \(S, U\), then all for all states \(s ∈ S\), \(\mathrm{beh}_S(s) = \mathrm{beh}_U(h(s)).\)
\end{theoremEnd}

\subsection{GKAT and Its Coalgebra}

Guarded Kleene Algebra with Tests, or GKAT~\cite{smolka_GuardedKleeneAlgebra_2020}, is a deterministic fragment of Kleene Algebra with Tests. 
The syntax of GKAT over a set of primitive actions \(K\) and a set of primitive tests \(T\) can be defined in two sorts, boolean expressions \(\BExp\) and GKAT expressions \(\Exp\):
\begin{align*}
    a, b, c ∈ \BExp 
        & ≜ 1 ∣ 0 ∣ t ∈ T ∣ b ∧ c ∣ b ∨ c ∣ \overline{b} \\  
    e, f ∈ \Exp 
        & ≜ p ∈ K ∣ b ∈ \BExp ∣ e +_b f ∣ e ; f ∣ e^{(b)} 
\end{align*}
where \(e +_b f\) is the if statement with condition \(b\), \(e;f\) is the sequencing of expression \(e\) and \(f\), and \(e^{(b)}\) is the while loop with body \(e\) and condition \(b\).
We use notation like \(b ≤ c\), \(b ≡ c\), and \(b ≢ c\) for the usual order, equivalence, and inequivalence in Boolean Algebra.
A GKAT expression can be unfolded into a KAT expression in the usual manner~\cite{kozen_KleeneAlgebraTests_1997c}:
\begin{align*}
    e +_b f & ≜ b; e + \overline{b}; f &
    e^{(b)} & ≜ (b; e)^*; \overline{b}.
\end{align*}
Then the semantics of each expression \(⟦e⟧\) can be computed by the semantics of Kleene Algebra with tests~\cite{kozen_KleeneAlgebraTests_1997c}.
An important construct in the semantics is \emph{atoms}, which are conjunctions of all the primitive tests either in its positive or negative form: for \(T ≜ \{t₁, t₂, …, tₙ\}\)
\[\At_T ≜ \{t₁' ∧ t₂' ∧ ⋯ ∧ tₙ' ∣ tᵢ' ∈ \{tᵢ, \overline{tᵢ}\}\}.\]
Follow the conventional notation, we denote an atom using \(α, β\); and we sometimes omit the subscript \(T\) when no confusing can arise.
Alternatively, atoms can also be thought of as truth assignments to each primitive tests, indicating which primitive tests is satisfied in the current program states; and \(α ≤ b\) if and only if the truth assignment represented by \(α\) satisfies \(b\).

For the sake of brevity, we omit the complete definition of GKAT and KAT semantics, we refer the reader to previous works~\cite{smolka_GuardedKleeneAlgebra_2020,schmid_GuardedKleeneAlgebra_2021,kozen_KleeneAlgebraTests_1997c}, which explains these semantics in detail.
Our work avoids direct interaction with the semantics by leveraging prior results in the coalgebraic theory of GKAT, which we will recap below.

Formally, GKAT coalgebras over primitive actions \(K\) and primitive tests \(T\) are coalgebras over the following functor:
\[G(S) ≜ (\{\accept, \reject\} + S × K)^{\At_T}.\] 
Intuitively, given a state \(s ∈ S\) and an atom \(α ∈ \At\), \(δ(s, α)\) will deterministically execute one of the following: reject \(α\) when \(δ(s, α) = \reject\); accept \(α\) when \(δ(s, α) = \accept\); or transition to a state \(s' ∈ S\) and execute action \(p ∈ K\) when \(δ(s, α) = (s', p)\).

In particular, \(G\) is a simple polynomial functor~\cite{jacobs_IntroductionCoalgebraMathematics_2016}, allowing the sub-coalgebra to preserve and reflect bisimulations.
\begin{theoremEnd}{theorem}[sub-coalgebras perserve and reflect bisimulation]\label{thm:sub-coalg-preserve-bisim}
    Given two states in sub-coalgebra \(s ∈ S' ⊑ S\) and \(u ∈ U' ⊑ U\), there exists a bisimulation \({∼'} ⊆ S' × U'\) s.t. \(s ∼' u\) if and only if there exists a bisimulation \({∼} ⊆ S × U\) s.t. \(s ∼ u\).
\end{theoremEnd}

\begin{proofEnd}
    For \(⟹\) direction, we will show that if \({∼'} ⊆ S' × U'\) is a bisimulation, then \({∼'}\) is also a bisimulation between \(S\) and \(U\):
    \[
        \begin{tikzcd}
            S \ar{d}{δ_S} & S' \ar[hook',swap]{l}{i} \ar{d}{δ_S}
            & ∼ \ar[swap]{l}{π₁} \ar{r}{π₂} \ar{d}{δ_∼}
            & U' \ar[hook]{r}{i} \ar{d}{δ_S} & U \ar{d}{δ_T}\\  
            G(S) & G(S') \ar[hook',swap]{l}{G(i)} 
            & G(∼) \ar[swap]{l}{G(π₁)} \ar{r}{G(π₂)} & U' \ar[hook]{r}{G(i)} & U 
        \end{tikzcd}
    \]
    Because the inclusion homomorphism \(i\) doesn't change the input, we have:
    \begin{align*}
        & {∼} \xrightarrow{π₁} S' \xrightarrow{i} S = {∼} \xrightarrow{π₁} S; \\
        & {∼} \xrightarrow{π₂} U' \xrightarrow{i} U = {∼} \xrightarrow{π₂} U.
    \end{align*}

    To prove the \(⟸\) we will show that if \({∼} ⊆ S × U\) is a bisimulation, then the restriction \({∼}' ≜ \{(s, u) ∈ {∼} ∣ s ∈ S', u ∈ U'\}\) is a bisimulation between \(S'\) and \(U'\).
    We first realize that \(∼_{S', U'}\) is a pre-image of the maximal bisimulation \(≣_{S', U'}\) along the inclusion homomorphism \(i: {∼} → {≡_{S, U}}\).
    This means that \(∼_{S', U'}\) can be formed by a pullback square:
    \[
        \begin{tikzcd}
            ∼_{S', U'} \ar{r}{i} \ar[swap]{d}{i} \ar[phantom, very near start]{dr}{\scalebox{1.5}{\(\lrcorner\)}} & ≣_{S', U'} \ar{d}{i}\\ 
            {∼} \ar[swap]{r}{i} & {≡_{S, U}}
        \end{tikzcd}
    \]
    Recall that elementary polynomial functor~\cite{jacobs_IntroductionCoalgebraMathematics_2016} like \(G\) preserves pullback, hence the pullback also uniquely generates a GKAT coalgebra~\cite{rutten_UniversalCoalgebraTheory_2000}
\end{proofEnd}

Besides nice property with bisimulation, GKAT coalgebra is also deterministic, unlike Kleene coalgebra with tests (KCT)~\cite{kozen_CoalgebraicTheoryKleene_2017}.
For each atom, states in a KCT can accept or reject the atom (but not both), yet states can also non-deterministically transition to multiple different states via the same atom, while executing different actions.
As we will see later, the deterministic behavior of GKAT coalgebra not only enables a more versatile symbolic algorithm than KCT~\cite{pous_SymbolicAlgorithmsLanguage_2015}, but also present challenges. 
Specifically, GKAT coalgebra requires normalization to compute finite trace equivalences~\cite{smolka_GuardedKleeneAlgebra_2020}, where we reroute all the transitions that cannot lead to acceptance immediately into rejection; and states that can never lead to acceptance is called \emph{dead states}.

In previous works, these dead states are detected after the necessary coalgebra is computed and stored.
This approach requires storing all the transition and states of coalgebra in memory, meaning that the algorithm does not short circuit even if a mismatch can be detected in the starting state; for example, when for some \(α ∈ \At\), \(δ_S(s, α) = \reject\) and \(δ_U(u, α) = \accept\).

Our algorithm is lazy in the dead state detection i.e. the dead state are only checked when a mismatch requires it; even then, our dead state detection algorithm will only check the necessary states to compute the liveness of the states causing the mismatch.
This not only avoids unnecessarily checking the liveness of a state, but also enables allows on-the-fly generation of the coalgebra, like using derivatives, where we can remove irrelevant states from memory after it has been explored. 

However, to truly understand our on-the-fly algorithms, we will first need to define ``dead states'', and its role in defining the coalgebraic semantic of GKAT. 

\subsection{Liveness and Sub-GKAT coalgebras}

Traditionally, live and dead states are defined by whether they can reach an accepting state~\cite{smolka_GuardedKleeneAlgebra_2020}.
\begin{definition}[Liveness]\label{def:liveness-of-states}
    A state \(s\) is \emph{accepting} if there exists an atom \(α ∈ \At\) s.t. \(δ(s, α) = \accept\).
    A state \(s₀\) is \emph{live} if there exists an accepting state \(sₙ\) s.t. there is a liveness chain:
    \[s₀ \transvia{α₁ ∣ p₁} s₁ \transvia{α₂ ∣ p₂} s₂ ⋯ \transvia{αₙ ∣ pₙ} sₙ.\]
    A state is \emph{dead} if it is not dead.
\end{definition}
However, besides directly working with this explicit definition, we can also give a more coalgebraic characterization of these properties.
Specifically, by induction on the length of the path from one state to the next, we can show that the principle sub-coalgebra \(⟨s⟩_S\) exactly contains all the reachable state of \(s\) in the GKAT coalgebra \(S\).
Thus we obtain the following coalgebraic characterization of liveness:
\begin{theoremEnd}{theorem}\label{thm:liveness-principle-sub-coalg}
    A state \(s\) is \emph{live} if there exists an accepting state \(s' ∈ ⟨s⟩\); and a state \(s\) is \emph{dead} if there is no accepting state in \(⟨s⟩\).
\end{theoremEnd}
This alternative liveness definition can help us prove important theorems regarding reachability and liveness without explicitly performing induction on the liveness chain. 
We show the following theorems as examples:
\begin{theoremEnd}{lemma}\label{thm:dead-iff-all-reachable-dead}
    A state \(s\) is dead if and only if all elements in \(⟨s⟩\) is dead.
\end{theoremEnd}
\begin{proofEnd}
    \(⟸\) direction is true, because \(s ∈ ⟨s⟩\): if all \(⟨s⟩\) is dead, then \(s\) is dead. 
    \(⟹\) direction can be proven as follows.
    Take any \(s' ∈ ⟨s⟩\), then \(⟨s'⟩ ⊑ ⟨s⟩\) because \(s'\) is the minimal sub-coalgebra that contains \(s'\). 
    Since there is no accepting state in \(⟨s⟩\), thus there cannot be any accepting state in \(⟨s'⟩\), hence \(s'\) is also dead.
\end{proofEnd}

\begin{theoremEnd}{theorem}[homomorphism perserves liveness]\label{thm:hom-preserve-liveness}
    Given a homomorphism \(h: S → U\) and a state \(s ∈ S\):
    \[\text{\(s\) is live} ⟺ \text{\(h(s)\) is live}\]
\end{theoremEnd}

\begin{proofEnd}
    Because homomorphic image preserves principle sub-GKAT coalgebra (\Cref{thm:homo-img-preserve-principle-sub-coalg})
    \[h(⟨s⟩_S) = ⟨h(s)⟩_U;\]
    therefore for any state \(s' ∈ S\):
    \[s' ∈ ⟨s⟩_S ⟺ h(s') ∈ h(⟨s⟩_S) ⟺ h(s') ∈ ⟨h(s)⟩_U.\]
    And because \(s'\) is accepting if and only if \(h(s')\) accepting by definition of homomorphism; then \(⟨s⟩_S\) contains an accepting state if and only if \(⟨h(s)⟩_U\) contains an accepting state. 
    Therefore, \(s\) is live in \(S\) if and only if \(h(s)\) is live in \(U\).
\end{proofEnd}

The above theorem then leads to several interesting liveness preservation properties for structures on coalgebras, like sub-coalgebra and bisimulation.

\begin{theoremEnd}{corollary}[sub-coalgebra perserves liveness]\label{thm:sub-coalg-preserve-liveness}
    For a sub-coalgebra \(S' ⊑ S\) and a state \(s ∈ S'\), \(s\) is live in \(S'\) if and only if \(s\) is live in \(S\).
\end{theoremEnd}

\begin{proofEnd}
    Let the homomorphism \(h\) in \cref{thm:hom-preserve-liveness} be the inclusion homomorphism \(i: S' → S\).
\end{proofEnd}

\begin{theoremEnd}{corollary}[bisimulation preserves liveness]\label{thm:bisim-preserve-liveness}
    If there exists a bisimulation \(∼\) between GKAT coalgebra \(S\) and \(U\) s.t. \(s ∼ u\) for some states \(s ∈ S\) and \(u ∈ U\), then \(s\) and \(u\) has to be either both accepting, both live or both dead.
\end{theoremEnd}

\begin{proofEnd}
    Because for a \(∼\) is a bisimulation when both \(π₁: {∼} → S\) and \(π₂: {∼} → U\) are homomorphisms.
    Therefore, 
    \begin{align*}
        s \text{ is live in } S 
        & ⟺ π₁((s, u)) \text{ is live in } S \\        
        & ⟺ (s, u) \text{ is live in } {∼} \\  
        & ⟺ π₂((s, u)) \text{ is live in } U  \\
        & ⟺ u \text{ is live in } U. 
        \qedhere
    \end{align*}
\end{proofEnd}


\subsection{Normalization And Semantics}

Infinite-trace model \(𝒢_ω\) is the final coalgebra of GKAT coalgebras~\cite{schmid_GuardedKleeneAlgebra_2021}.
The finality of the model allows us to define the semantics of each state in a given GKAT coalgebra \(S\), which we will denote as \(⟦-⟧^{ω}_{S}: S → 𝒢_ω\), where the semantic equivalences can identified by bisimulation~\cite{schmid_GuardedKleeneAlgebra_2021}:
\(⟦s⟧^{ω}_{S} = ⟦t⟧^{ω}_{T}\) if and only if there exists a bisimulation \({∼} ⊆ S × T\), s.t. \(s ∼ t\).

The infinite trace equivalences can be directly computed with bisimulation on derivative, which supports on-the-fly algorithm as demonstrated by similar systems~\cite{kozen_CoalgebraicTheoryKleene_2017,almeida_DecidingKATHoare_2012,pous_SymbolicAlgorithmsLanguage_2015}. 
However, the \emph{finite} trace model \(𝒢\) is the final coalgebra of GKAT coalgebras without dead states, which we call \emph{normal GKAT coalgebra}~\cite{smolka_GuardedKleeneAlgebra_2020}. 
Fortunately every GKAT coalgebra can be normalized by rerouting all the transition from dead states to rejection.
Concretely, given a GKAT coalgebra \(S ≜ (S, δ_S)\), \(δ_{\norm(S)} : S → G(S)\) is defined as \(δ_{\norm(S)}(s, α) ≜ \reject\) when \(δ_S(s, α) = (s', p)\) and \(s'\) is dead in \(S\); and \(δ_{\norm(S)}(s, α) ≜ δ_S(s, α)\) otherwise. 
We call \(\norm(S) ≜ (S, δ_{\norm(S)})\) the \emph{normalized} coalgebra of \(S\).

And we use \(⟦-⟧_S: \norm(S) → 𝒢\) to denote the finite trace semantics of GKAT coalgebra, which is the unique homomorphism into the final coalgebra \(𝒢\). 
The finite trace equivalence between \(s ∈ S\) and \(u ∈ U\) can be decided by first normalizing \(S\) and \(U\) then deciding whether there is a bisimulation \({∼} ⊆ \norm(S) × \norm(U)\) s.t. \(s ∼ t\).
For a more detailed explanation on the finite trace semantics, we refer the reader to the work of \Citeauthor{smolka_GuardedKleeneAlgebra_2020}~\cite{smolka_GuardedKleeneAlgebra_2020}, however we will recall the correctness theorem here.

\begin{theoremEnd}{theorem}[Correctness~\cite{smolka_GuardedKleeneAlgebra_2020}]\label{thm:norm-bisim-correctness}
    Given two states in two GKAT coalgebra \(s ∈ S\) and \(u ∈ U\), then there exists a bisimulation between normalized coalgebras \({∼} ⊆ \norm(S) × \norm(U)\) s.t. \(s ∼ u\) if and only if \(s\) and \(u\) are trace equivalent \(⟦s⟧_S = ⟦u⟧_U\)
\end{theoremEnd}

Besides giving us the finite-trace semantics, the normalization operation also satisfy several nice properties.

First, normalization preserves liveness, i.e. a state \(s ∈ S\) is dead (live) if and only if it is dead (live) in \(\norm(S)\), allowing us to perform liveness analysis in \(\norm(S)\) to obtain the liveness in \(S\), and vise versa.

\begin{theoremEnd}{theorem}[normalization perserves liveness]\label{thm:norm-perserve-liveness}
    A state \(s ∈ S\) is dead (live) in \(S\) if and only if it is dead (live) in \(\norm(S)\).
\end{theoremEnd}

\begin{proofEnd}
    By definition, every state is either dead of live, thus we will only need to show the \(⟹\) direction, and 

    This proof unfortunately requires us unfolding the path to accepting states.

    If \(s\) is dead in \(S\), then by~\Cref{thm:dead-iff-all-reachable-dead}, for all \(α, p\), \(s \transvia{α ∣ p}_S s'\) implies \(s'\) is dead. 
    Because \(s\) cannot accept any atom in \(S\), \(s\) will reject all atom in \(\norm(S)\); which implies that \(s\) is dead in \(\norm(S)\).

    If \(s\) is live in \(S\), then there exists a walk \(s \transvia{b₁ ∣ p₁}_S s₁ \transvia{b₂ ∣ p₂}_S ⋯ \transvia{bₙ ∣ pₙ}_S sₙ\) s.t. \(sₙ\) is accepting, which implies every state \(sᵢ\) on the walk is live. 
    Hence, this walk also exists in \(\norm(S)\), and \(s\) is live in \(\norm(S)\).
\end{proofEnd}

Second, normalization is an endofunctor in the category of GKAT coalgebra, this result can help us obtain sub-coalgebras of normalized coalgebra, and also connect the finite trace and infinite trace semantics.

\begin{theoremEnd}{theorem}\label{thm:norm-functor}
    \(\norm\) is an endofunctor in the category GKAT coalgebra.
    More specifically, if \(h: S → U\) is a GKAT homomorphism, then \(h: \norm(S) → \norm(U)\) is also a homomorphism.
\end{theoremEnd}

\begin{proofEnd}
    Recall that \(h\) is a homomorphism if and only if for all \(s ∈ S\) and \(α ∈ \At\):
    \begin{itemize}[nosep]
        \item for a result \(r ∈ \{\reject, \accept\}\), 
        \[δ_S(s, α) = r ⟺ δ_U(h(s), α) = r;\]
        \item for any \(s' ∈ S\) and \(p ∈ K\), 
        \[δ_S(s, α) = (s', p) ⟺ δ_{U}(h(s), α) = (h(s'), p).\]
    \end{itemize}

    Then we show that \(h: \norm(S) → \norm(U)\) is a homomorphism, this is a consequence of homomorphism preserves liveness (\Cref{thm:hom-preserve-liveness}): for all \(s ∈ \norm(S)\) and \(α ∈ \At\):
    \begin{align*}
        & δ_{\norm(S)}(s, α) = \accept \\*
        ⟺{}& δ_S(s, α) = \accept \\*  
        ⟺{}& δ_U(h(s), α) = \accept \\*  
        ⟺{}& δ_{\norm(U)}(h(s), α) = \accept;\\
        & δ_{\norm(S)}(s, α) = \reject \\*  
        ⟺{}& δ_{S}(s, α) = \reject 
        \text{ or } δ_{S}(s, α) = (s', p), s' \text{ is dead} \\*
        ⟺{}& δ_{U}(h(s), α) = \reject \\*
        & \text{ or } δ_{U}(h(s), α) = (h(s'), p), h(s') \text{ is dead} \\*  
        ⟺{}& δ_{\norm(U)}(h(s), α) = \reject;\\
        & δ_{\norm(S)}(s, α) = (s', p) \\*
        ⟺{}& δ_{S}(s, α) = (s', p), s' \text{ is live} \\*  
        ⟺{}& δ_{U}(h(s), α) = (h(s'), p), h(s') \text{ is live}\\* 
        ⟺{}& δ_{\norm(U)}(h(s), α) = (h(s'), p).
        \qedhere
    \end{align*}
\end{proofEnd}

\begin{theoremEnd}{corollary}\label{thm:norm-sub-coalg}
    Normalization preserves sub-coalgebra, i.e. if \(S' ⊑ S\) then \(\norm(S') ⊑ \norm(S)\).
\end{theoremEnd}

\begin{proofEnd}
    By letting the homomorphism in~\Cref{thm:norm-functor} to be the inclusion homomorphism \(i: S' → S\)
\end{proofEnd}

Because of the functoriality, we can show that two states are infinite-trace equivalent implies these two states are finite-trace equivalent.
This gives us more tool in proving semantic equivalence between two states in GKAT coalgebras: proving bisimulation of these two states in the \emph{non-normalized} GKAT coalgebra can also obtain semantic equivalence for two states.

\begin{theoremEnd}{corollary}\label{thm:inf-trace-equiv-implies-fin-trace-equiv}
    Given two states in two GKAT coalgebra \(s ∈ S, u ∈ U\), \(⟦s⟧^{ω}_{S} = ⟦u⟧^{ω}_{U} ⟹ ⟦s⟧_{S} = ⟦u⟧_{U}\).
\end{theoremEnd}

\begin{proofEnd}
    Because \(⟦s⟧^{ω}_{S} = ⟦u⟧^{ω}_{U}\), there exists a bisimulation \({∼} ⊆ S × U\) s.t. \(s ∼ u\)~\cite{schmid_GuardedKleeneAlgebra_2021}.
    Therefore, we have the following span in the category of GKAT coalgebra:
    \[\begin{tikzcd}
        S & ∼ \ar{r}{π₂} \ar[swap]{l}{π₁} & U
    \end{tikzcd}\]
    Then by~\Cref{thm:norm-functor}, \(\norm(∼)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\):
    \[\begin{tikzcd}
        \norm(S) 
        & \norm(∼) \ar{r}{π₂} \ar[swap]{l}{π₁} 
        & \norm(U)
    \end{tikzcd}\]
    Because \(s ∼ u\) and normalization operation preserves states in \(∼\), therefore \((s, u) ∈ \norm(∼)\), and because \(\norm(∼)\) is a bisimulation between the normalization of \(S\) and \(U\), therefore \(⟦s⟧_{S} = ⟦u⟧_{U}\) (\Cref{thm:norm-bisim-correctness}).
\end{proofEnd}

\section{On-The-Fly Bisimulation}

The original algorithm for deciding GKAT equivalences~\cite{smolka_GuardedKleeneAlgebra_2020} requires the entire coalgebra to be known prior to the execution of the bisimulation algorithm; specifically, it is necessary to iterate through the entire coalgebra in order to identify the liveness of every single states and perform the normalization operation.
This limitation poses challenges to design an efficient on-the-fly algorithm for GKAT.
To make the equivalence-checking procedure scalable, we propose to greedily run the bisimulation algorithm and only invoke liveness detection when a discrepancy is found. 
For example, when deciding the trace equivalence between \(s ∈ S\) and \(u ∈ U\), if \(δ_S(s, α) = (s', p)\) and \(δ_U(u, α) = (u', p)\), we will proceed to recurse on \(s'\) and \(u'\), without checking the liveness of \(s'\) and \(u'\).
However, if \(s\) rejects \(α\) instead of transitioning to \(s'\), we will then check whether \(u'\) is dead.

This approach offers several benefit: first, because the liveness detection is only called when necessary, thus it is unlikely to iterate through the entire coalgebra; second, the algorithm can short-circuit on a counter-example, allowing termination even before all the states are explored; finally, the algorithm supports on-the-fly constructions like derivatives, because the coalgebra no longer is required to be fully constructed.

The correctness argument of this on-the-fly algorithm is centered around the following coalgebraic structure, which we named ``greedy bisimulation''.

\begin{definition}[Greedy Bisimulation]\label{def:greedy-bisim}
    Given two GKAT coalgebra \(S\) and \(U\), and a bisimulation \({∼} ⊆ \norm(S) × \norm(U)\), a greedy bisimulation \({≈} ⊆ S × U\) is the least GKAT coalgebra that contains \(∼\) with the following transition function:
    \[
        δ_≈((s, u), α) ≜ \begin{cases}
            \accept & \begin{aligned}
                & \text{if } δ_S(s, α) = \accept \\[-3px]
                & \text{and } δ_U(s, α) = \accept
            \end{aligned}\\[5px]
            ((s', u'), p) & 
                \begin{aligned}
                    & \text{if } s \transvia{α ∣ p}_{S} s' \\[-3px]
                    & \text{and } u \transvia{α ∣ p}_{U} u' 
                \end{aligned}\\[5px]
            \reject & \text{otherwise}
        \end{cases}
    \]
    We call \(∼\) \emph{the underlying bisimulation} of \(≈\).
\end{definition}

Given any bisimulation \({∼} ⊆ S × U\), we can construct a GKAT coalgebra \(∼_d\) s.t. it is almost a greedy bisimulation, i.e. it is closed under \(δ_≈\) in~\Cref{def:greedy-bisim}, and contains \(∼\), except it is not the \emph{least} GKAT coalgebra that satisfy these properties.

\begin{theoremEnd}{lemma}\label{thm:dead-construction-greedy-bisim}
    Given any eager bisimulation \({≈} ⊆ S × U\) with the underlying bisimulation \({∼}\), the following relation \({∼_d} ⊆ S × U\):
    \[{∼_d} ≜ {∼} ∪ \{(s, u) ∣ \text{\(s\) and \(u\) are dead in \(S\) and \(U\)}\},\]
    form a GKAT coalgebra the transition function \(δ_≈\) in~\Cref{def:greedy-bisim}, which implies \({≈} ⊑ {∼_d}\).
    Furthermore, \(\norm(∼_d)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\).
\end{theoremEnd}

\begin{proofEnd}
    We will need to show \(∼_d\) is closed under the transition of \(δ_{≈}\) in~\Cref{def:greedy-bisim}.
    Specifically, if \(s ∼_d u\) and \(s \transvia{α ∣ p}_{S} s'\) and \(u \transvia{α ∣ p} u'\), then \(s' ∼_d u'\).
    By case analysis on the liveness of \(s'\) and \(u'\) in \(S\) and \(U\):
    \begin{itemize}
        \item If both \(s'\) and \(u'\) are dead, then \(s ∼_d u\) by definition of \(∼_d\).
        \item If both \(s'\) and \(u'\) are live, then \(s, s'\) and \(u, u'\) are live in \(S\) and \(U\) respectively. 
        Thus, \(s \transvia{α ∣ p}_{\norm(S)} s'\), and \(u \transvia{α ∣ p}_{\norm(U)} u'\).
        Therefore, \(s' ∼ u'\) by definition of bisimulation on \(\norm(S)\) and \(\norm(U)\); and \(s ∼_d u'\).
        \item If \(s'\) is live but \(u'\) is dead, then \(s\) is live, and \(s ∼ u\).
        However, \(s \transvia{α ∣ p}_{\norm(S)} s'\) yet \(u \transRej{\norm(U)}{u}\), which violates the condition for bisimulation, thus this case cannot happen.
        \item If \(u'\) is live but \(s'\) is dead, this case also cannot happen similar to the previous case.
    \end{itemize}

    By~\Cref{thm:bisim-between-dead}, we can construct a bisimulation \(∼_{s,u} ≜ \{(s, u)\} ⊆ \norm(S) × \norm(U)\) between any two dead states \(s ∈ S\) and \(u ∈ U\).
    Then because bisimulation is closed under union~\cite[Theorem 5.5]{rutten_UniversalCoalgebraTheory_2000}, \(\norm(∼_d) = {∼} ∪ ⋃ \{∼_{s, u} ∣ \text{\(s, u\) are dead}\}\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\).
    TODO: bisimulation are uniquely determined by its carrier.
\end{proofEnd}

Although the proof of~\Cref{thm:dead-construction-greedy-bisim} is basically by unfolding the definition, \(∼_d\) turns out to be a important structure. 
Specifically, \(∼_d\) serves a connection between \(≈\) and its underlying bisimulation \(∼\), and the order \({∼} ⊆ {≈} ⊑ {~_d}\) will reveal many structural properties of \(≈\).
We show the following corollary to demonstrate the usefulness of \(∼_d\).

\begin{theoremEnd}{corollary}\label{thm:greedy-bisim-exists-unique}
    For any bisimulation between two normalized GKAT coalgebra \({∼} ⊆ \norm(S) × \norm(U)\), a greedy bisimulation \(≈\) with \(∼\) as underlying bisimulation exists and is unique.
\end{theoremEnd}

\begin{proofEnd}
    Because sub-coalgebra form a complete lattice under union and intersection~\cite[Theorem 6.4]{rutten_UniversalCoalgebraTheory_2000}, then we can construct \(≈\) by taking the intersection of all the sub-coalgebra of \(∼_d\) in~\Cref{thm:dead-construction-greedy-bisim}.
\end{proofEnd}

\begin{theoremEnd}{corollary}\label{thm:greedy-bisim-dead-or-bisim}
    For every pair of states \((s, u)\) in a greedy bisimulation \({≈} ⊆ S × U\), either \((s, u)\) is in the underlying bisimulation \(∼\), or both \(s\) and \(u\) are dead in \(S\) and \(U\).
\end{theoremEnd}

\begin{proofEnd}
    Because \(∼_d\) is a GKAT coalgebra with transition \(δ_≈\) and also contains all the states in \(∼\), therefore \({≈} ⊑ {∼_d}\).
    Thus, for all \((s, u) ∈ {≈}\) but not in \(∼\), \(s\) and \(u\) must be dead in \(S\) and \(U\).
\end{proofEnd}

\begin{theoremEnd}{corollary}[Perservation Of Liveness]\label{thm:greedy-bisim-perserve-liveness}
    Given a pair of states in a greedy bisimulation \(s ≈ u\) where \({≈} ⊆ S × U\), then the following are equivalent
    \begin{itemize}
        \item \((s, u)\) is live (or dead) in \({≈}\);
        \item \(s\) is live (or dead) in \(S\);  
        \item \(u\) is live (or dead) in \(U\).
    \end{itemize}
    Furthermore, if \((s, u)\) is in the underlying bisimulation \(∼\) of \(≈\), then all of above are also equivalent to \((s, u)\) is live (or dead) in \(∼\).
\end{theoremEnd}

\begin{proofEnd}
    Let \(∼\) by the underlying bisimulation of \(≈\), consider the \(∼_d\) in~\Cref{thm:dead-construction-greedy-bisim}, then we know that \({≈} ⊑ {∼_d}\).
    
    Because \(\norm(∼_d)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\), then
    \begin{align*}
        & \text{\(s\) is live in \(S\)} \\
        \text{ iff } & \text{\(s\) is live in \(\norm(S)\)} 
            & \text{\Cref{thm:norm-perserve-liveness}}\\  
        \text{ iff } & \text{\((s, u)\) is live in \(\norm(∼_d)\)} 
            & \text{\Cref{thm:bisim-preserve-liveness}}\\
        \text{ iff } & \text{\((s, u)\) is live in \(∼_d\)} 
            & \text{\Cref{thm:norm-perserve-liveness}}\\
        \text{ iff } & \text{\((s, u)\) is live in \(≈\)} 
            & \text{\Cref{thm:sub-coalg-preserve-liveness}}
    \end{align*}
\end{proofEnd}

\begin{theoremEnd}{corollary}\label{thm:greedy-bisim-iff-norm-bisim}
    Given two GKAT coalgebras \(S\) and \(U\), and a relational GKAT coalgebra \({≈} ⊆ S × U\) with the transition \(δ_≈\) as in~\cref{def:greedy-bisim}.
    \(≈\) is a greedy bisimulation if and only if \(\norm(≈)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\)
\end{theoremEnd}

\begin{proofEnd}
    The \(⟹\) direction, we let \(∼_d\) be defined as in~\Cref{thm:dead-construction-greedy-bisim}.
    Because \({≈} ⊑ {∼_d}\) and~\Cref{thm:norm-sub-coalg}, \(\norm(≈) ⊑ \norm(∼_d)\). 
    Because \(\norm(∼_d)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\), therefore \(\norm(≈)\) is a bisimulation (\Cref{thm:greedy-bisim-iff-norm-bisim}).

    The \(⟸\) direction can be proven by simply letting \(\norm(≈)\) be the underlying bisimulation of \(≈\), because \(\norm(≈)\) has the same carrier set of \(≈\), then \(≈\) has to be the least GKAT coalgebra with \(δ_≈\) that contains \(\norm(≈)\), satisfying all the conditions for greedy bisimulation.
\end{proofEnd}

\begin{theoremEnd}{corollary}\label{thm:max-bisim-is-max-greedy-bisim}
    Given two GKAT coalgebra \(S\) and \(U\), carrier of the maximal bisimulation \({≡} ⊆ \norm(S) × \norm(U)\) is also the carrier of maximal greedy bisimulation, which we will denote as \(≊ ⊆ S × U\).
    We will abuse the notation \(≡\) to represent both the maximal bisimulation and maximal greedy bisimulation.
\end{theoremEnd}

\begin{proofEnd}
    by definition of greedy bisimulation, the carrier of \({≡}\) is a subset of the carrier of \({≊}\). 
    Then because of~\Cref{thm:greedy-bisim-iff-norm-bisim}, \(\norm(≊)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\), which implies \(\norm(≡') ⊑ {≡}\).
    Finally because \(\norm\) do not change the carrier set, therefore the carrier of \(≡\) and \(≊\) necessarily coincide.

    Then we will need to show that \(≡\) with the \(δ_≈\) defined in~\Cref{def:greedy-bisim} is the maximal greedy bisimulation.
    Take an arbitrary greedy bisimulation \({≈} ⊆ S × U\) with underlying bisimulation \(∼\), and a pair of states \(s ≈ u\), we will need to show \(s ≡ u\).
    By~\cref{thm:greedy-bisim-dead-or-bisim}, we can case analysis on \(s\) and \(u\): if \(s ∼ u\), then \(s ≡ u\) because \(≡\) is the maximal bisimulation; and if \(s\) and \(u\) are both dead, then by~\cref{thm:bisim-between-dead}, \(s ≡ u\).
\end{proofEnd}

\Cref{thm:max-bisim-is-max-greedy-bisim} allows us to take the maximal bisimulation as a greedy bisimulation, aiding several proofs.



Similar to the correctness of bisimulation between normalized coalgebra (\Cref{thm:norm-bisim-correctness}), we also establish a correctness theorem for greedy bisimulation, i.e. there exists 

\begin{theoremEnd}{theorem}[Correctness]
    Given two states in GKAT coalgebras \(s ∈ S\) and \(u ∈ ,U\), there exists a greedy bisimulation \({≈} ⊆ S × U\) s.t. \(s ≈ u\), if and only if \(⟦s⟧_S = ⟦u⟧_U\).
\end{theoremEnd}

\begin{proofEnd}
    \(⟹\) direction, because normalization preserves carrier set, therefore \((s, u) ∈ \norm(≈)\).
    And because \(\norm(≈)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\) (\Cref{thm:greedy-bisim-iff-norm-bisim}), \(⟦s⟧_S = ⟦u⟧_U\).

    \(⟸\) direction, because \(⟦s⟧_S = ⟦u⟧_U\), then by~\cref{thm:norm-bisim-correctness}, then there a bisimulation \(∼ ⊆ \norm(S) × \norm(U)\) s.t. \(s ∼ u\). 
    Because~\Cref{thm:greedy-bisim-exists-unique}, therefore there exists a greedy bisimulation \(≈\) with \(∼\) as its underlying bisimulation. 
    Because the carriers \({∼} ⊆ {≈}\), therefore \(s ∼ u\) implies \(s ≈ u\).
\end{proofEnd}

Despite proving many desirable structural properties, the definition of greedy bisimulation is still coalgebraic.
In order for our equivalence-checking algorithm to construct a greedy bisimulation, we need to turn the coalgebra into a set of concrete criteria, that can be checked by an algorithm.
In the following theorem, we will present such necessary and sufficient conditions to construct a greedy bisimulation.
Our algorithm will be largely based on recursively checking these conditions to establish a greedy bisimulation.

\begin{theoremEnd}{theorem}[Recursive Construction]\label{thm:recursive-construction}
    Given a relation \({≈} ⊆ S × U\), \(≈\) is a carrier of some greedy bisimulation if and only if all the following condition are satisfied: for all pair of states \(s ≈ u\),
    \begin{enumerate}
        \item\label{itm:acc-condition} for all \(α ∈ \At\), 
        \(s \transAcc{S}{α}\) if and only if \(u \transAcc{U}{α}\);
        \item\label{itm:rej-or-dead} if \(s\) reject \(α\) but \(u\) transitions to \(u'\), then \(u'\) is dead; similarly when \(u\) rejects \(α\);  
        \item\label{itm:transition-bisim} If \(δ_{S}(s, α) = (s', p)\) and \(δ_{U}(u, α) = (u', p)\), then \(s' ≈ u'\).
        \item\label{itm:transition-dead} If \(δ_{S}(s, α) = (s', p)\) and \(δ_{U}(u, α) = (u', q)\), s.t. \(p ≠ q\), then both \(s'\) and \(u'\) are dead.
    \end{enumerate}
\end{theoremEnd}

\begin{proofEnd}
    To show the \(⟹\) direction, assuming there exists a greedy bisimulation \(≈\), then it would satisfy all the conditions. 
    We first note that the pairs \((s, u) ∈ {≈}\) can satisfy one of two properties (or both), either \(s\) and \(u\) are both dead in \(S\) and \(U\), or \((s, u)\) is in the underlying bisimulation \(∼\).
    Then we will verify that all the condition is satisfied both when \(s ∼ u\) and when \(s, u\) are dead.

    The condition~\labelcref{itm:acc-condition} holds if \(s ∼ u\):
    \begin{align*}
        s \transAcc{S}{α}
        \text{ iff }& s \transAcc{\norm(S)}{α} \\
        \text{ iff }& (s, u) \transAcc{∼}{α} \\ 
        \text{ iff }& u \transAcc{\norm(U)}{α} \\
        \text{ iff }& u \transAcc{U}{α};
    \end{align*}
    condition~\labelcref{itm:acc-condition} also holds if \(s\) and \(u\) are dead in \(S\) and \(U\), because both can never accept any atoms.

    The condition~\labelcref{itm:rej-or-dead} holds when \(s ∼ u\): 
    \begin{align*}
        & δ_S(s, α) = \reject \text{ and } δ_U(u, α) = (u', p)\\
        ⟹{}& δ_{\norm(S)}(s, α) = \reject \text{ and } δ_U(u, α) = (u', p) \\
        ⟹{}& δ_{\norm(U)}(u, α) = \reject \text{ and } δ_U(u, α) = (u', p)\\
        ⟹{}& \text{\(u'\) is dead}
    \end{align*}
    Similarly, when \(u'\) reject \(α\) and \(s\) transitions to \(s'\).
    The condition~\labelcref{itm:rej-or-dead} holds when \(s, u\) are both dead because both of them can only transition to dead state by~\Cref{thm:dead-iff-all-reachable-dead}.

    The condition~\labelcref{itm:transition-bisim} holds because when \(s \transvia{α ∣ p}_S s'\) and \(u \transvia{α ∣ p}_U u'\), then \((s, u) \transvia{α ∣ p}_{≈} (s', u')\) by definition of greedy bisimulation (\Cref{def:greedy-bisim}).

    The condition~\labelcref{itm:transition-dead} holds, let \(δ_S(s, α) = (s', p)\) and \(δ_U(u, α) = (u', q)\) and \(p ≠ q\). 
    \begin{itemize}
        \item If \(s ∼ u\), then we can show condition~\labelcref{itm:transition-dead} by case analysis on the liveness of \(s'\) and \(u'\):
        if \(s', u'\) are not both dead, then the condition of \({∼} ⊆ \norm(S) × \norm(U)\) cannot be satisfied, hence \((s', u') ∉ {∼}\), which by~\Cref{thm:greedy-bisim-dead-or-bisim} means that \(s'\) and \(u'\) are both dead, contradicting the premise.
        Therefore, \(s'\) and \(u'\) needs to be both dead.
        \item If \(s\) and \(u\) are both dead, then by~\Cref{thm:dead-iff-all-reachable-dead}, both \(s\) and \(u\) can only transition to dead state, hence \(s'\) and \(u'\) are both dead.
    \end{itemize}

    Then we prove the \(⟸\) direction.
    If \(≈\) satisfy all the conditions, then by condition~\labelcref{itm:transition-bisim}, \(≈\) is closed under \(δ_≈\), hence is a GKAT coalgebra.
    We let its underlying bisimulation be \(\norm(≈)\), which is sound if \(\norm(≈)\) is a bisimulation (\cref{thm:greedy-bisim-iff-norm-bisim}).
    Thus, we only need to show that \(\norm(≈)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\). Without loss of generality, we will show that \(π₁: \norm(≈) → \norm(S)\) is a bisimulation.

    For all pair of states \((s, u) ∈ \norm(≈)\) and any atom \(α ∈ \At\), we will verify all the conditions of a homomorphism.
    For the accepting case, by the definition of \(δ_{≈}\):
    \begin{align*}
        (s, u) \transAcc{\norm(≈)}{α}
        & \text{ iff } (s, u) \transAcc{≈}{α} \\  
        & \text{ implies } s \transAcc{≈}{α}
    \end{align*}
    For the transition case, because \((s, u)\) is live in \(≈\) implies that \(s\) is live in \(S\) (\Cref{thm:greedy-bisim-perserve-liveness}):
    \begin{align*}
        & (s, u) \transvia{α ∣ p}_{\norm(S)} (s', u') \\
        & \text{iff }
            (s, u) \transvia{α ∣ p}_{S} (s', u')
            \text{ and \(s, u\) is live in \(≈\)} \\  
        & \text{implies }
            s \transvia{α ∣ p}_S s' 
            \text{ and \(s\) is live in \(S\)} \\  
        & \text{implies }
            s \transvia{α ∣ p}_{\norm(S)} s' 
    \end{align*}
    Finally, the rejection case: \((s, u) \transRej{\norm(≈)}{α}\) either when \((s, u) \transvia{α ∣ p}_{≈} (s', u')\) and \((s', u')\) is dead, or \((s, u) \transRej{≈}{α}\).
    We will show that both cases will lead to \(s \transRej{\norm(S)}{α}\).
    The \((s, u) \transvia{α ∣ p}_{≈} (s', u')\) case can be proven by the definition of \(δ_≈\) and~\Cref{thm:greedy-bisim-perserve-liveness}:
    \begin{align*}
        & (s, u) \transvia{α ∣ p}_{≈} (s', u') 
            \text{ and \((s', u')\) is dead in \(≈\)} \\
        \text{implies } & s \transvia{α ∣ p}_{S} s' 
            \text{ and \(s'\) is dead in \(S\)} \\  
        \text{implies } & s \transRej{\norm(S)}{α}.
    \end{align*}
    Then the hardest case is when \((s, u) \transRej{≈}{α}\) because by definition of \(δ_≈\) (\Cref{def:greedy-bisim}) there are many cases that can lead to rejection.
    We will need to eliminate irrelevant cases using the condition provided by the construction.
    We proceed by case analysis on dynamics of \(s\) and \(u\) on \(α\); first listing some cases that contradicts the premise \((s, u) \transRej{≈}{α}\):
    \begin{itemize}
        \item If \(s\) accepts \(α\) in \(S\), then \(u\) necessarily accepts \(α\) in \(U\) by condition~\labelcref{itm:acc-condition}, which implies \((s, u) \transAcc{≈}{α}\), contradicting the premise.
        \item Similarly, if \(u\) accepts \(α\) in \(U\), then \((s, u) \transAcc{≈}{α}\), contradicting the premise.
        \item If \(s \transvia{α ∣ p}_S s'\) and \(u \transvia{α ∣ p}_U u'\) then \((s, u) \transvia{α ∣ p}_≈ (s', u;)\), contradicting the premise.
    \end{itemize}
    Then we list the other cases, which indeed satisfy \((s, u) \transRej{≈}{α}\), and show that all of them leads to \(s \transRej{\norm(S)}{α}\), using the condition in the theorem:
    \begin{itemize}
        \item If \(s \transRej{S}{α}\), then \(s \transRej{\norm(S)}{α}\).
        \item If \(s \transvia{α ∣ p}_S s'\) and \(u \transRej{U}{α}\), then by condition~\labelcref{itm:rej-or-dead}, \(s'\) is dead, and \(s \transRej{\norm(S)}{α}\).
        \item If \(s \transvia{α ∣ p}_S s'\) and \(u \transvia{α ∣ q}_U u'\), then by condition~\labelcref{itm:transition-dead}, both \(s'\) and \(u'\) are dead, and \(s \transRej{\norm(S)}{α}\).
        \qedhere
    \end{itemize}
\end{proofEnd}



\begin{theoremEnd}{lemma}\label{thm:bisim-between-dead}
    Given any two dead states \(s ∈ S\) and \(u ∈ U\), we can construct a bisimulation \(∼\) between \(\norm(S)\) and \(\norm(U)\):
    \begin{align*}
        ∼ &≜ \{(s, u)\}, & δ_∼((s, u)) & ≜ \reject.
    \end{align*}
    Therefore, for the maximal bisimulation \({≡} ⊆ \norm(S) × \norm(U)\), \(s ≡ u\).
\end{theoremEnd}

\begin{theoremEnd}{theorem}[Equivalence Relation]\label{thm:greedy-bisim-iff-greedy-bisim-equiv}
    Given two states in GKAT coalgebra \(s ∈ S\) and \(u ∈ U\), there exists a greedy bisimulation \(≈\) between \(S\) and \(U\) s.t. \(s ≈ u\), if and only if there exists a greedy bisimulation that is an \emph{equivalence relation} \(≃\) in \(S + U\) s.t. \(s ≃ u\).
\end{theoremEnd}

\begin{proofEnd}
    First, recall that by~\Cref{thm:greedy-bisim-iff-norm-bisim} \(\norm(≈)\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\).
    Because \(S ⊑ S + U\) and \(U ⊑ S + U\) and normalization preserves sub-coalgebra (\Cref{thm:norm-sub-coalg}), we have \(\norm(S) ⊑ \norm(S + U)\) and \(\norm(U) ⊑ \norm(S + U)\); and by~\Cref{thm:sub-coalg-preserve-liveness}, \(s ∈ S\) is dead in \(S + U\) if and only if it is dead in \(S\), and similarly for \(u ∈ U\).

    The \(⟹\) direction. 
    Because sub-coalgebra preserves bisimulation (\Cref{thm:sub-coalg-preserve-bisim}) and \(\norm(S) ⊑ \norm(S + U)\) and \(\norm(U) ⊑ \norm(S + U)\), we have \(\norm(≈)\) is a bisimulation between \(\norm(S + U)\) and itself.
    let \(≡\) to be the maximal bisimulation within \(\norm(S + U)\), then \(≡\) is an equivalence relation~\cite[Corollary 5.6]{rutten_UniversalCoalgebraTheory_2000}, and so is the maximal greedy bisimulation \(≊\) (\Cref{thm:max-bisim-is-max-greedy-bisim}).
    This gives us the following chain of set inclusions:
    \[(s, u) ∈ {≈} = \norm(≈) ⊑ {≡} ⊆ {≊}.\]
    Thus \(≊\) is a equivalence relation~\cite[Corollary 5.6]{rutten_UniversalCoalgebraTheory_2000} and a greedy bisimulation s.t. \(s ≡ u\).

    The \(⟸\) direction can be shown by considering the maximal greedy bisimulation \(≊\) within \(S + U\) and maximal bisimulation \(≡\) within \(\norm(S + U)\).
    By maximality and the carrier of \(≡\) coincide with \(≊\), we have the following set inclusions:
    \[(s, u) ∈ {≃} ⊑ {≊} = {≡}.\]
    Finally, we reflect \(≡\) through the inclusion homomorphisms \(\norm(S) ⊑ \norm(S + U)\) and \(\norm(U) ⊑ \norm(S + U)\) as in~\Cref{thm:sub-coalg-preserve-bisim} to obtain \({∼} ⊆ \norm(S) × \norm(U)\) and \(s ∼ u\).
    Therefore \((s, u)\) is also in the greedy bisimulation \({≈} ⊆ \norm(S) × \norm(U)\), generated by \(∼\).
\end{proofEnd}

% \begin{theoremEnd}{theorem}\label{thm:greedy-bisim-one-dead}
%     Consider two states in GKAT coalgebras \(s ∈ S, u ∈ U\) and an arbitrary bisimulation \({∼} ⊆ \norm(S) × \norm(U)\), if \(s\) is dead, then 
%     \begin{enumerate}
%         \item\label{itm:greedy-bisim-one-dead-then-both-dead} if \(s ∼ u\), then \(u\) is dead.
%         \item\label{itm:remove-dead-pairs-sound} if \(u\) is dead, then \({∼} ∖ ⟨(s, u)⟩_∼\) is also a bisimulation between \(\norm(S)\) and \(\norm(U)\).
%     \end{enumerate}
% \end{theoremEnd}

% \begin{proofEnd}
%     \Cref{itm:greedy-bisim-one-dead-then-both-dead} is a consequence of bisimulation preserves liveness (\Cref{thm:bisim-preserve-liveness}) and normalization preserves liveness (\Cref{thm:norm-perserve-liveness}).

%     \Cref{itm:remove-dead-pairs-sound} can be proven by realizing that \((s, u)\) is a dead state in \(∼\) (\Cref{thm:bisim-preserve-liveness}) which means that all the states in \(⟨(s, u)⟩_∼\) are dead.
%     Because \(∼\) is a normalized GKAT coalgebra (\Cref{thm:}), therefore no state can transition into dead states like \(⟨(s, u)⟩_∼\), hence \({∼} ∖ ⟨(s, u)⟩_∼\) is still closed under its transition, and form a sub-coalgebra of \(∼\).
%     Finally because \(∼\) is a bisimulation between \(\norm(S)\) and \(\norm(U)\), then so is \({∼} ∖ ⟨(s, u)⟩_∼\) (\Cref{thm:})
% \end{proofEnd}

Before we introduce the optimized algorithm, we will first briefly sketch the liveness-check algorithm, which will use a depth-first search to check whether the there exists any accepting states in reachable states of \(s ∈ S\): if there exists any accepting state in \(⟨s⟩_S\), then the algorithm terminates immediately, and return that \(s\) is live, otherwise it would return all the states in \(⟨s⟩\), and by~\cref{thm:dead-iff-all-reachable-dead}, all the states in \(⟨s⟩\) is dead.
In this case, we use depth-first search for its simplicity to implement, other search algorithm will also be sound.

We will cache all the known dead states from the previous searches; whether a state \(s\) is in this cache can be checked by the function call \Call{knownDead\(_S\)}{$s$}. 
The function \Call{isDead\(_S\)}{$s$} will first check if \(s\) is known to be dead, and invoke the search algorithm in the coalgebra \(⟨s⟩\), if \(s\) is not in the cached dead states.
Because the non-symbolic version of liveness checking is similar to the symbolic version, we only provide the symbolic version of this algorithm in~\cref{lst:dead-state-detection}.


\begin{algorithm*}
    \caption{On-the-fly bisimulation algorithm}\label{alg:bisim}
    \begin{algorithmic}
        \Function{equiv}{$s ∈ S, u ∈ U$}
        \If {\Call{eq}{$s, u$}} {\Return true} \EndIf
        \If {\Call{knownDead\(_S\)}{$s$}} 
            \State\Return \Call{isDead\(_U\)}{$u$}
        \EndIf
        \If {\Call{knownDead\(_U\)}{$u$}} 
            \State\Return \Call{isDead\(_S\)}{$s$}
        \EndIf
        \State\Return \(∀ α ∈ \At\) \Comment{conditions in~\Cref{thm:recursive-construction}}
        \\\vspace{5px}
        \(\qquad\begin{aligned}
            & s \transAcc{S}{α} \text{ iff } u \transAcc{U}{α} \mathrel{\&\!\&}\\  
            & s \transRej{S}{α} \text{ and } u \transvia{α ∣ p}_U u' \text{ implies } \Call{isDead\(_U\)}{u'} \mathrel{\&\!\&}\\  
            & s \transvia{α ∣ p}_S s' \text{ and } u \transRej{U}{α} \text{ implies } \Call{isDead\(_U\)}{s'} \mathrel{\&\!\&}\\  
            & s \transvia{α ∣ p}_S s' \text{ and } u \transvia{α ∣ p}_U u' \text{ implies } \Call{union}{s, u}; \Call{equiv}{s', u'} \mathrel{\&\!\&}\\
            & s \transvia{α ∣ p}_S s' \text{ and } u \transvia{α ∣ q}_U u' \text{ and } p ≠ q \text{ implies } \Call{isDead\(_S\)}{s'} \text{ and } \Call{isDead\(_U\)}{u'}
        \end{aligned}\)
        % \Forall{\(α ∈ \At\)}{}{
        %     \Match{$δ_{S}(s, α), δ_{U}(u, α)$}
        %     \Case{\(\accept, \accept\)} {\Continue} \EndCase
        %     \Case{\(\reject, \reject\)} {\Continue} \EndCase
        %     \Case{\(\reject, (u', q)\)} 
        %         \State\Return \Call{isDead\(_U\)}{$u'$} 
        %     \EndCase
        %     \Case{\((s', p), \reject\)}
        %         \State\Return\Call{isDead\(_S\)}{$s'$}
        %     \EndCase
        %     \Case{\((s', p), (u', q)\)} {
        %         \If {\(p = q\)}
        %             \State\Call{union}{$s, u$}
        %             \State\Return \Call{equiv}{$s, t$} 
        %         \EndIf
        %         \If{\Call{isDead\(_S\)}{$s$} and \Call{isDead\(_U\)}{$u$}}
        %             \State\Continue
        %         \EndIf
        %         \State\Return false
        %     } \EndCase
        %     \Default { \Return false } \EndDefault
        %     \EndMatch
        % }\EndFor
        % \State\Return true
        \EndFunction
    \end{algorithmic}
\end{algorithm*}

Finally, we present our equivalence-checking algorithm as~\cref{alg:bisim}, which are mostly checking the conditions in~\cref{thm:recursive-construction}.  
In the algorithm, we use the text ``implies'', ``iff'', ``and'' are logical operators that takes booleans (i.e. true or false) and return a boolean.
Similarly, \(\&\!\&\) is the logical and operation, which is the same as the the written ``and''. 
We opt to use \(\&\!\&\), instead of ``and'', to separate the conditions for clarity; formally \(\&\!\&\) is the same function as ``and'', but binds less tightly.

Intuitively, starting with the pair of state \((s, u)\), it will explore all of its reachable state following \(δ_≈\) in~\cref{def:greedy-bisim}.
In other word, if the algorithm terminates and returned true, we will have explored every state in \(⟨(s, u)⟩_{≊}\) where \(≊\) is the maximal greedy bisimulation between \(S\) and \(U\).
However, our algorithm slightly deviates from the ideal scenario described above, specifically we implemented two optimization.
First, we uses a union-find data structure to keep track of existing state, which organizes the explored states into equivalence classes.
Indeed, by looking for a greedy bisimulation that is also equivalence relation, we can use more efficient structures like union-find, instead of set of pairs; and the correctness of such optimization is justified by~\Cref{thm:greedy-bisim-iff-greedy-bisim-equiv}.
The second optimization is to terminate and return false as soon as we know one of \(s, u\) is dead and the other is not dead.
Indeed, in order for \(s ≈ u\) for any greedy bisimulation \(≈\), \(s\) and \(u\) necessarily have the same liveness in their respective GKAT coalgebra.
And in the other case, if both \(s\) and \(u\) are dead in their respective coalgebra, we will skip the check on all the reachable state of \((s, u)\) following \(δ_≈\), because a pair of dead state is necessarily in the maximal greedy bisimulation \(≊\) (\Cref{thm:bisim-between-dead,thm:max-bisim-is-max-greedy-bisim}).

Concerning the complexity, our algorithm have similar worst case complexity as the original algorithm.
In particular, when deciding the trace equivalence of two states \(s ∈ S\) and \(u ∈ U\), the original algorithm~\cite{smolka_GuardedKleeneAlgebra_2020} requires one pass of \(⟨s⟩\) and \(⟨u⟩\) to normalize them, then the bisimulation will visit each pair of states in \(⟨s⟩ × ⟨u⟩\) at most once, which means that they will visit at most \(|⟨s⟩ + ⟨u⟩ + ⟨s⟩ × ⟨u⟩|\) number of states.

Similarly, our equivalence-checking algorithm attempts to find a bisimulation first, which visits each pair in \(⟨s⟩ × ⟨u⟩\) at most once, and when a mismatch is found in the process, the liveness-checking algorithm is then invoked.
Crucially, our algorithm satisfy the following property: if the liveness-checking algorithm find a live or accepting state, the entire equivalence checking algorithm will halt and return false.
Thus, by caching all the known dead states, the liveness-checking only visits states in \(⟨s⟩\) and \(⟨u⟩\) at most once.
Therefore, the worst case of our algorithm is also \(|⟨s⟩ × ⟨u⟩ + ⟨s⟩ + ⟨u⟩|\).

However, the on-the-fly algorithm will always out-perform the original algorithm, in terms of number of states visited, because this algorithm only invoke liveness check when necessary.
In the extreme case when the two input states are infinite-trace equivalent, the on-the-fly algorithm can skip liveness checking entirely.


% \section{The Algorithm}

% In this section we will present the pseudo-code for our on-the-fly algorithm. 
% In order to implement the the inductive construction theorem (\cref{thm:recursive-construction}), we will need to determine the liveness of the state. This can be simply computed via a DFS from the state being checked. 

% TODO: we should merge the two so that it is easier to 
% \begin{algorithm}
%     \caption{Check whether a state \(s\) is dead}\label{alg:check-dead-main}
%     \begin{algorithmic}
%         \Function{isDeadLoop}{$s ∈ S$, explored}
%         \If {\(s ∈\) explored} {\Return explored} 
%         \Else { 
%             \For{\(α ∈ \At\)}{}{
%                 \Match{\(δ_{S}(s, α)\)}
%                 \Case{\(\accept\)} {\Return none} \Comment{\(s\) transition to accept}
%                 \EndCase
%                 \Case{\(\reject\)} {\texttt{continue}}
%                 \Comment{skip if \(s\) transition to reject}
%                 \EndCase
%                 \Case{($s', p$)}{ 
%                     \If{\Call{IsDeadLoop}{$s'$} = none} {\Return none} \Comment{\(s\) transitions to a live state \(s'\)}
%                     \Else { explored \(←\) (explored \(∪\) \Call{isDeadLoop}{$s'$, explored}) } \EndIf
%                 } \EndCase
%                 \EndMatch
%             }\EndFor}
%         \EndIf
%         \State {\Return explored}    
%         \EndFunction
%     \end{algorithmic}
% \end{algorithm}

% By~\cref{thm:dead-iff-all-reachable-dead}, if \(s\) is dead then all the reachable states of \(s\) (denoted by \(⟨s⟩\)). Then by returning all the reachable states of \(s\), we can cache these states to avoid checking them again. To encapsulate the caching, we have the following function, which we will actually use in our bisimulation algorithm.

% \begin{algorithm}
%     \caption{A cached algorithm to check whether a state is dead}\label{alg:is-dead}
%     \begin{algorithmic}
%         \State{deadStates \(← ∅\)}

%         \Function{isDead}{$s ∈ S$}
%         \If {\(s ∈\) deadStates} {\Return true} 
%         \ElsIf {\Call{isDeadLoop}{$s, ∅$} = none} {\Return false}
%         \Else 
%             \State {deadStates \(←\) (deadStates \(∪\) \Call{isDeadLoop}{$s, ∅$})}
%             \State {\Return {true}}
%         \EndIf
%         \EndFunction
%     \end{algorithmic}
% \end{algorithm}

% Given the direct correspondence between bisimulation and bisimulation equivalence and bisimulation in sub-algebra:
% \begin{align*}
%     & ∃ \text{ bisimulation } {∼} ⊆ ⟨s⟩ × ⟨t⟩ \text{ s.t. } s ∼ t \\
%     & ⟺ ∃ \text{ bisimulation } {∼} ⊆ (⟨s⟩∪⟨t⟩) × (⟨s⟩∪⟨t⟩) \text{ s.t. } s ∼ t 
%         & \text{\cref{thm:sub-coalg-preserve-bisim}}\\  
%     & ⟺ ∃ \text{ bisimulation equivalence } {≃} ⊆ (⟨s⟩∪⟨t⟩) × (⟨s⟩∪⟨t⟩) \text{ s.t. } s ≃ t 
%         & \text{\cref{thm:bisim-iff-bisim-equiv}}
% \end{align*}
% we can safely replace the bisimulation in inductive construction (\cref{thm:recursive-construction}) with bisimulation equivalence. 
% Dealing with equivalence relations allows us to leverage efficient data structures like union find in our bisimulation algorithm. 

% We will use \(\Call{union}{$s,t$}\) to denote the operation to equate \(s\) and \(t\) in a union-find, and use \(\Call{eq}{$s,t$}\) to check if \(s\) and \(t\) belongs to the same equivalence class, i.e. share the same representative.
% Specifically, we will use the union-find structures to keep track of the equivalence classes that we are in the process of checking, hence avoiding repeatedly checking the same pair of states to remove infinite loops.

% Our on-the-fly bisimulation algorithm will decide whether there exists a bisimulation relation in \(⟨s⟩ ∪ ⟨t⟩\) s.t. \(s ∼ t\). This algorithm generally reproduce the setting of inductive construction theorem~\cref{thm:recursive-construction};
% except by~\cref{thm:bisim-one-dead}, in the special case where \(s\) or \(t\) is dead, then we will only need to check whether the other is dead.

% Because the dead state detection algorithm is coalgebra-specific, we use a subscript on ``deadStates'' and ``\textsc{IsDead}'' to indicate the coalgebra. 
% The soundness and completeness of~\cref{alg:bisim} can be observed by the fact that \emph{when the algorithm terminate}, the algorithm returns true if and only if there exists a bisimulation between \(⟨s⟩\) and \(⟨t⟩\) s.t. \(s ∼ t\), which is then logically equivalent to trace equivalence.
% Such equivalence is a direct consequence of~\cref{thm:bisim-one-dead,thm:recursive-construction}.

% \begin{remark}
%     The caching of dead state and the shortcut to check whether \(s\) is dead when \(t\) is dead and vise versa, is not essential to the soundness and completeness of algorithm, they are here to trade speed with memory. 
%     In a memory-constraint situation, the ``\textnormal{deadStates}'' variable can be cleared periodically to save memory.
% \end{remark}

\section{Symbolic Coalgebra and Algorithm}

Although \cref{alg:bisim} is on-the-fly, it still uses GKAT coalgebra, which contains exponentially many transitions with respect to the number of primitive tests \(|T|\).
Concretely, the transition function \(δ: S → \At_T → \{\accept, \reject\} + S × K\) requires computing the transition result for each \emph{atom}, and the number of atoms \(\At_T ≅ 2^{T}\) is exponential to the size of primitive tests in \(T\).
This is also why several GKAT-related complexity results only consider constant sized \(T\).

Symbolic GKAT coalgebra, instead of computing the behavior of each atom individually, groups atoms into boolean expressions. 
This optimization leads to space-efficient coalgebras and an equivalence checking algorithm making use of off-the-shelf SAT solvers.
Specifically, given a set of primitive actions \(K\) and primitive tests \(T\), a \emph{symbolic GKAT coalgebra} \(Ŝ ≜ (S, ϵ̂, δ̂)\) consists of a state set \(S\) and an accepting function \(ϵ̂\) and a transition function \(δ̂\):
\begin{mathpar}
    ϵ̂: S → 𝒫(\BExp_T), \and
    δ̂: S → 𝒫(\BExp_T × S × K).
\end{mathpar}
This coalgebra is also required to satisfy the disjointedness condition, i.e. for all states \(s ∈ S\), the boolean expressions that \(s\) accepts \(ϵ̂(s)\) and the boolean expressions that enables \(s\) to transition \(\{b ∣ ∃ (b, s', p) ∈ δ(s)\}\) is disjoint; also the conjunction of any two distinct expressions from the set \(ϵ̂(s) ∪ \{b ∣ ∃ (b, s', p) ∈ δ(s)\}\) is equivalent to \(0\) under boolean algebra.

We name \(ϵ̂\) the accepting function because intuitively a state \(s\) accepts an atom \(α\) when there exists a \(b ∈ ϵ̂(s)\), s.t. \(α ≤ b\); similarly, \(δ̂\) is called the transition function because a state \(s\) transitions to \(s'\) via atom \(α\) while executing \(p\) when there exists \((b, s', p) ∈ δ̂(s)\) and \(α ≤ b\).
With the above intuition in mind, a symbolic GKAT coalgebra \(Ŝ ≜ (S, ϵ̂, δ̂)\) can be lowered into a GKAT coalgebra \(S ≜ (S, δ)\) in the following manner:
\begin{align}\label{cons:lowering}
δ(s, α) ≜ \begin{cases}
    \accept & ∃ b ∈ ϵ̂(s), α ≤ b \\[5px]
    (s', p) & 
        \begin{aligned}
            & ∃ b ∈ \BExp_T, α ≤ b \\
            & \text{ and } δ(s, b) = (s', p)  
        \end{aligned}\\[5px]
    \reject & \text{otherwise}
\end{cases}
\end{align}
This is well-defined, i.e. exactly one clause can be satisfied for any \(s ∈ S\) and \(α ∈ \At\), because of the disjointedness condition.
We usually use \(S\) to denote the lowering of \(Ŝ\); and the semantics of a state \(s ∈ Ŝ\) is defined as its semantics in the lowering \(⟦s⟧_{Ŝ} ≜ ⟦s⟧_{S}.\)

We will then use \(ρ̂(s): \BExp_T\) to represent all the atoms that the state \(s\) rejects in the lowering, and \(ρ̂(s)\) is computed as follows:
\begin{align*}
    ρ̂(s) ≜ ⋀ \{\overline{b} ∣{}
        & ∃ s' ∈ S, p ∈ K, (b, s', p) ∈ δ̂(s) \\
        & \text{ or } b ∈ ϵ̂(s)\}.
\end{align*}

\begin{remark}[Canonicity]\label{rem:canonicity}
    Symbolic GKAT coalgebra is not canonical, i.e. there exists two different symbolic GKAT coalgebra with the same lowering, consider the state set \(S ≜ \{s\}\):
    \begin{align*}
        {δ̂}₁(s) & ≜ \{b ↦ (s, p), \overline{b} ↦ (s, p)\} \\
        {δ̂}₂(s) & ≜ \{⊤ ↦ (s, p)\},
    \end{align*} 
    and both \(ϵ̂₁, ϵ̂₂\) will return constant \(0\).
    These two symbolic GKAT coalgebra \(Ŝ₁ ≜ (S, δ̂₁, ϵ̂₁)\) and \(Ŝ₂ ≜ (S, δ̂₂, ϵ̂₂)\) have the same lowering and semantics, yet, they are different.
    It is possible to construct symbolic representations that satisfies canonicity, yet we opt to use our current representation for ease of construction and computational efficiency.
\end{remark}

\begin{theoremEnd}{theorem}[Functoriality]\label{thm:lowering-functor}
    The lowering operation is a functor, every symbolic GKAT coalgebra homomorphism \(h: Ŝ → Û\), is also a GKAT coalgebra homomorphism \(h: S → U\).
\end{theoremEnd}

\begin{proofEnd}
    Since \(Ŝ\) and \(Û\) have the same states as their lowering, therefore \(h: S → U\) is indeed a function, then we only need to verify the homomorphism condition on \(h\).
    TODO: finish.
\end{proofEnd}

The functoriality states that a homomorphism on two symbolic coalgebras induces a homomorphism of on their lowing; similarly, a bisimulation on symbolic GKAT coalgebra also induces a bisimulation on their lowering.
However, the converse is not true, precisely because of the canonicity problem noted in~\Cref{rem:canonicity}: take the \(Ŝ₁\) and \(Ŝ₂\) in~\Cref{rem:canonicity}, because they have the same lowering, therefore the identity homomorphism is a homomorphism on their lowerings, but there is no homomorphism from \(Ŝ₁\) to \(Ŝ₂\).

We can then define the symbolic equivalence algorithm, with its correctness stated in~\cref{thm:recursive-construction}.

\begin{theoremEnd}{theorem}[Symbolic Recursive Construction]\label{thm:symb-recursive-construction}
    Given two symbolic GKAT coalgebra \(Ŝ = (S, ϵ̂_S, δ̂_S)\) and \(Û = (U, ϵ̂_U, δ̂_U)\) and two states \(s ∈ S\) and \(u ∈ U\), \(s\) and \(u\) are trace equivalent \(⟦s⟧_{Ŝ} = ⟦u⟧_{Û}\), if and only if all the following holds:
    \begin{itemize}
        \item \(⋁ ϵ̂_S(s) ≡ ⋁ ϵ̂_U(u)\); 
        \item for all \((b, s', p) ∈ δ̂_S(s)\) and \(c ∈ ρ̂_U(u)\), if \(b ∧ c ≢ 0\), then \(s'\) is dead;
        \item for all \(b ∈ ρ̂_S(s)\) and \((c, u', q) ∈ δ̂_U(u)\), if \(b ∧ c ≢ 0\), then \(u'\) is dead;
        \item for all \((b, s', p) ∈ δ̂_S(s)\) and \((c, u', q) ∈ δ̂_U(u)\), if \(b ∧ c ≢ 0\) and \(p ≠ q\) then both \(s'\) and \(u'\) is dead; 
        \item for all \((b, s', p) ∈ δ̂_S(s)\) and \((c, u', q) ∈ δ̂_U(u)\), if \(b ∧ c ≢ 0\) and \(p = q\) then \(⟦s'⟧_{Ŝ} = ⟦u'⟧_{Û}\).
    \end{itemize}
\end{theoremEnd}

\begin{proofEnd}
    Reduces to~\Cref{thm:recursive-construction} i.e. all the above condition holds if and only if all the condition in~\Cref{thm:recursive-construction} holds in the lowering.
\end{proofEnd}

\begin{algorithm*}
    \caption{Symbolic On-the-fly Bisimulation Algorithm}\label{alg:symb-bisim}
    \begin{algorithmic}
        \Function{equiv}{$s ∈ S, u ∈ U$}
        \If {\Call{eq}{$s, u$}} {\Return true}
        \ElsIf {\Call{knownDead\(_S\)}{$s$}} {\Return \Call{isDead\(_U\)}{$u$}} 
        \ElsIf {\Call{knownDead\(_U\)}{$u$}} {\Return \Call{isDead\(_S\)}{$s$}} 
        \Else {}
        \Return {
            \Comment{conditions of ~\Cref{thm:symb-recursive-construction}}
            \\\vspace{5px}
            \(\qquad
            \begin{aligned}
                & ⋁ ϵ̂_S(s) ≡ ⋁ ϵ̂_U(u) \mathrel{\&\!\&} \\  
                & s \transvia{b ∣ p} s' \text{ and } (b ∧ ρ̂_U(u)) ≢ 0 \text{ implies \Call{isDead$_S$}{$s'$}} \mathrel{\&\!\&}\\
                & u \transvia{b ∣ p} u' \text{ and } (ρ̂_S(s) ∧ b) ≢ 0 \text{ implies \Call{isDead$_U$}{$u'$}} \mathrel{\&\!\&}\\
                & s \transvia{b ∣ p} s' \text{ and } u \transvia{c ∣ p} u' \text{ implies } \text{\Call{Union}{$s$, $u$}}; \text{\Call{Equiv}{$s', u'$}} \mathrel{\&\!\&}\\
                & s \transvia{b ∣ p} s' \text{ and } u \transvia{c ∣ q} u' \text{ and } p ≠ q \text{ implies \Call{isDead}{$s'$} and \Call{isDead}{$u'$}}
            \end{aligned}\)
        }
        \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm*}

Similar to the non-symbolic version of the algorithm, \cref{alg:symb-bisim} first check if either of the input states is dead, then  recursively check all the conditions in~\Cref{thm:symb-recursive-construction}, where \(\mathrel{\&\!\&}\) denotes the logical and operator on boolean data type.
The symbolic version of the liveness checking algorithm is also based on a graph search algorithm on all the reachable state, where we try to identify an accepting state by checking whether \(⋁ ϵ̂(s) ≡ 0\).
The only deviation from the non-symbolic cases is that we do not follow empty transitions in the symbolic case, i.e. if \(δ̂(s) ≜ (b, s', p)\) and \(b ≡ 0\), we will not search \(s'\) to check the liveness of \(s'\).
In~\Cref{sec:optimization-implementation}, we will outline a simple modification for the symbolic GKAT coalgebra generation algorithm in~\Cref{sec:symb-gkat-construction} to avoid generating empty transitions, allowing us to skip the emptiness check when deciding whether a state is dead.
TODO: I think we need to make sure that we are presenting the dead state checking algorithm ignoring the empty transition.
After which we can comment it here, to claim that this is for generality.


\section{Symbolic GKAT Coalgebra Construction}\label{sec:symb-gkat-construction}

The final piece of the puzzle is to convert any given expression into an \emph{equivalent} state in some symbolic GKAT coalgebra. 
This goal can be achieved by lifting existing constructions like derivatives and Thompson's construction~\cite{schmid_GuardedKleeneAlgebra_2021,smolka_GuardedKleeneAlgebra_2020} to the symbolic setting.
Then the correctness of non-symbolic version can be used to show the correctness of the symbolic construction i.e. we will prove that the lowering as shown in construction~\labelcref{cons:lowering} of these constructions will yield the conventional derivative.
However, several other important properties, like finiteness of derivative coalgebra and the correctness of the Thompson's construction can be established using a symbolic GKAT coalgebra homomorphism from the Thompson's construction to the derivative.

We introduce some new notations for the transitions of symbolic GKAT coalgebra: for a symbolic GKAT coalgebra \(Ŝ ≜ (S, δ̂, ϵ̂)\) and state \(s ∈ S\), we will use \(s \transAcc{S}{b}\) to denote \(b ∈ ϵ̂(s)\); and use \(s \transvia{b ∣ p}_S s'\) to denote \((b, s', p) ∈ δ̂(s)\).

\begin{figure*}
    \begin{mathpar}
        \inferrule[]{\\}
        {p \transvia{1 ∣ p}_{D̂} 1} \and  
        \inferrule[]{\\}
        {b \transAcc{D̂}{b}} \and  
        \inferrule[]
        {e \transvia{c ∣ p}_{D̂} e'}
        {e +_b f \transvia{b ∧ c ∣ p}_{D̂} e'} 
        \and
        \inferrule[]
        {e \transAcc{D̂}{c}}
        {e +_b f \transAcc{D̂}{b ∧ c}}
        \and
        \inferrule[]
        {f \transvia{c ∣ p}_{D̂} f'}
        {e +_b f \transvia{\overline{b} ∧ c ∣ p}_{D̂} f'}
        \and
        \inferrule[]
        {f \transAcc{D̂}{c}}
        {e +_b f \transAcc{D̂}{\overline{b} ∧ c}}
        \and
        \inferrule[]
        {e \transAcc{D̂}{b} \\ f \transAcc{D̂}{c}}
        {e; f \transAcc{D̂}{b ∧ c}}
        \and 
        \inferrule[]
        {e \transvia{b ∣ p}_{D̂} e'}
        {e; f \transvia{b ∣ p}_{D̂} e'; f}
        \and
        \inferrule[]
        {e \transAcc{D̂}{b} \\ f \transvia{c ∣ p}_{D̂} f'}
        {e; f \transvia{b ∧ c ∣ p}_{D̂} f'}
        \and  
        \inferrule[]
        {\\}
        {e^{(b)} \transAcc{D̂}{\overline{b}}}  
        \and  
        \inferrule[]
        {e \transvia{c ∣ p} e'}
        {e^{(b)} \transvia{b ∧ c ∣ p}_{D̂} e'; e^{(b)}}
    \end{mathpar}
    \caption{Symbolic Derivative Coalgebra \(D̂\)}\label{fig:derivatives-rules}
\end{figure*}

The symbolic derivative coalgebra \(D̂\), with expressions as states, is the least symbolic GKAT coalgebra (ordered by point-wise subset ordering on \(ϵ̂\) and \(δ̂\)) that satisfy the rules in~\Cref{fig:derivatives-rules}.
In more plain words, a transition is in \(D̂\) if and only if it is derivable by the rules in~\Cref{fig:derivatives-rules}.
These rules are very close to the rules of GKAT derivative by~\citeauthor{schmid_GuardedKleeneAlgebra_2021}~\cite{schmid_GuardedKleeneAlgebra_2021}.
This is no coincidence, as our definition exactly lowers to the definition of theirs.
This fact can be proven by case analysis on the shape of the source expression, and forms a basis on our correctness argument.
\begin{theoremEnd}{theorem}[Correctness]\label{thm:derivative-correctness}
    The lowering of \(D̂\), denoted \(D\), is exactly the derivative defined by~\citeauthor{schmid_GuardedKleeneAlgebra_2021}~\cite{schmid_GuardedKleeneAlgebra_2021}.
    Therefore, the semantics of the expression is equal to the semantics generated by the derivative coalgebra, \(⟦e⟧ = ⟦e⟧_{D} = ⟦e⟧_{D̂}\).
\end{theoremEnd}
\begin{proofEnd}
    TODO: unfold the statement.
\end{proofEnd}


\begin{table*}
    \centering
    \begin{tabular}{c||c|c|l|l}
        Exp & \(S\) & \(s^*\)  
        & \(ϵ̂(s)\) & \(δ̂(s)\) \\\hline
        \(b\) & \(\{s^*\}\) 
        & \(s^*\)
        & \(\{b\}\) & \(∅\) \\  
        \(p\) & \(\{s^*, s₁\}\) 
        & \(s^*\)
        & \(\begin{cases}
           ∅ & s = s^* \\  
           \{1\} & s = s₁ 
        \end{cases}\) 
        & \(\begin{cases}
            \{(1, s₁, 0)\} & s = s^* \\  
            ∅ & s = s₁
        \end{cases}\)\\  
        \(e₁ +_b e₂\) & \(\{s^*\} + S₁ + S₂\) &
        \(s^*\) &
        \(\begin{cases}
            ⟨\{b\}| ϵ̂₁(s₁^*) ∪ ⟨\{b\}| ϵ̂₂(s₂^*) & s = s^* \\
            ϵ̂₁(s) & s ∈ S₁\\
            ϵ̂₂(s) & s ∈ S₂\\
        \end{cases}\) & 
        \(\begin{cases}
            ⟨\{b\}| δ̂₁(s₁^*) + ⟨\{b\}| δ̂₂(s₂^*) & s = s^* \\
            δ̂₁(s) & s ∈ S₁\\
            δ̂₂(s) & s ∈ S₂\\
        \end{cases}\) \\  
        \(e₁ ; e₂\) & \(S₁ + S₂\) & 
        \(s₁^*\) & 
        \(\begin{cases}
            ⟨ϵ̂₁(s)| ϵ̂₂(s₂^*)& s ∈ S₁ \\  
            ϵ̂₂(s) & s ∈ S₂
        \end{cases}\)& 
        \(\begin{cases}
            δ̂₁(s) + ⟨ϵ̂(s)| δ̂₂(s₂^*) & s ∈ S₁ \\  
            δ̂2(s) & s ∈ S₂
        \end{cases}\) \\  
        \(e₁^{(b)}\) & \(\{s^*\} + S₁\) & 
        \(s^*\) &
        \(\begin{cases}
            \{\overline{b}\} & s = s^*\\
            ⟨\{\overline{b}\}| ϵ̂1(s) & s ∈ S₁
        \end{cases}\) & 
        \(\begin{cases}
            ⟨\{b\}| δ̂₁(s₁^*) & s = s^* \\
            δ₁(s) ∪ ⟨\{b\}|⟨ϵ̂1(s)| δ̂₁(s₁^*) & s ∈ S₁
        \end{cases}\)
    \end{tabular}
    \caption{Symbolic Thompson's Construction}\label{tab:symb-Thompson-construction}
\end{table*}

Another way to construct a coalgebra from a GKAT expression is via Thompson's construction, we lift the original construction to the symbolic setting.
A useful operation for Thompson's construction is the following guard operation, denoted by \(⟨B|\), where \(B\) is a set of boolean expressions: for a transition function \(δ̂\), a accepting function \(ϵ̂\), and a state \(s\),
\begin{align*}
    ⟨B|ϵ̂(s) & ≜ \{b ∧ c ∣ b ∈ B, c ∈ \hat{\epsilon}(s)\}; \\
    ⟨B|δ̂(s) & ≜ \{(b ∧ c, s', p) ∣ b ∈ B, (c, s', p) ∈ \hat{\delta}(s)\}.
\end{align*}
Besides guarding transition and acceptance with different conditions in if statements and while loops, the guard operator can also be used to simulate uniform continuation. Specifically, we can use \(⟨ϵ̂(s)|δ(s')\) to connecting all the accepting transition of \(s\) to the dynamic \(δ(s')\).

With these definitions in mind, we can define symbolic Thompson's construction inductively as in~\Cref{tab:symb-Thompson-construction}, where we let \((S₁, ϵ̂₁, δ̂₁)\) and \((S₂, ϵ̂₂, δ̂₂)\) to be result of Thompson's construction for \(e₁\) and \(e₂\) respectively.
In this table, \(S₁ + S₂\) denotes the disjoint union of \(S₁\) and \(S₂\), and for any two transition dynamics \(δ₁(s₁): 𝒫(\BExp × S₁ × K)\) and \(δ₂(s₂): 𝒫(\BExp × S₂ × K)\), we can also compose them in parallel as \(δ₁(s₁) + δ₂(s₂)\):
\begin{align*}
    δ₁(s₁) & + δ₂(s₂): 𝒫(\BExp × (S₁ + S₂) × K) \\*
    δ₁(s₁) & + δ₂(s₂) ≜ \\*
        & \{(b, \mathrm{inj}ₗ(s₁'), p) ∣ (b, s₁', p) ∈ δ₁(s₁)\} ∪ \\*
        & \{(b, \mathrm{inj}ᵣ(s₂'), p) ∣ (b, s₂', p) ∈ δ₂(s₂)\},
\end{align*}
where \(\mathrm{inj}ₗ: S₁ → S₁ + S₂\) and \(\mathrm{inj}ᵣ: S₂ → S₁ + S₂\) are the canonical left/right injection of the coproduct.

Our construction deviates from the original construction~\cite{smolka_GuardedKleeneAlgebra_2020} by using a start state \(s^* ∈ S\) instead of a start dynamics (or pseudo-state).
This choice will make the proof of~\Cref{thm:hom-thompson-derivative} slightly easier. 
However, in~\Cref{sec:optimization-implementation}, we will explain that our implementation uses start dynamics instead of start state, to avoid unnecessary lookups and unreachable states.

We would like to explore several desirable theoretical properties of both derivatives and Thompson's construction.
Specifically, the \emph{correctness}, i.e. the semantics of the ``start state'' the both construction have the same preserves the trace semantics of the expression; \emph{finiteness}, i.e. the coalgebra generated is always finite, implying the termination of our equivalence algorithm; and finally, \emph{complexity}, i.e. the relationship between the number of reachable states and the size of the input expression, which serves as an estimated complexity of our equivalence checking algorithm.
Turns out, all of these questions can be answered by a homomorphism from symbolic Thompson's construction to the symbolic derivatives.

\begin{theoremEnd}{theorem}\label{thm:hom-thompson-derivative}
    Given any GKAT expression \(e\), the resulting symbolic GKAT coalgebra from Thompson's construction \(Ŝ_e\) have a homomorphism to derivatives \(h: Ŝ_e → D̂\), s.t. for the start state \(s^* ∈ S, h(s^*) = e\).
\end{theoremEnd}

\begin{proofEnd}
    By induction on the structure of \(e\). We will recall that \(h: Ŝ_e → ⟨e⟩_D\) is a symbolic GKAT coalgebra homomorphism when the following two conditions are true: \(s \transAcc{Sₑ}{b}\) if and only if \(h(s) \transAcc{D̂}{b}\); and \(s \transvia{b ∣ p}_{Sₑ} s'\) if and only if \(h(s) \transvia{b ∣ p}_{D̂} h(s')\).

    When \(e ≜ b\) for some tests \(b\), then the function \(h\) is defined as \(\{s^* ↦ b\}\).
    When \(e ≜ p\) for some primitive action \(p\), then the function \(h\) is defined as \(\{s^* ↦ p, * ↦ 1\}\).
    The homomorphism condition can then be verified by unfolding the definition.

    When \(e ≜ e₁ +_b e₂\), by induction hypothesis, we have homomorphisms \(h₁: Ŝ_{e₁} → ⟨e₁⟩_D\) and \(h₂: Ŝ_{e₂} → ⟨e₂⟩_D\).
    Then we define the homomorphism 
    \[h(s) ≜ \begin{cases}
        e₁ +_b e₂ & s = s^* \\  
        h₁(s) & s ∈ Ŝ_{e₁} \\  
        h₂(s) & s ∈ Ŝ_{e₂}
    \end{cases}\]
    We show that \(h\) is a homomorphism. Because \(Ŝₑ\) preserves the transition and acceptance of \(Ŝ_{e₁}\) and \(Ŝ_{e₂}\), then for all \(s ∈ Ŝ_{e₁} ∩ Ŝ_{e}\), we have
    \begin{align*}
        & s \transAcc{Ŝ_{e}}{c} 
        \text{ iff } s \transAcc{Ŝ_{e₁}}{c} \\*
        & \text{ iff } h₁(s) \transAcc{D̂}{c} 
        \text{ iff } h(s) \transAcc{D̂}{c}; \\ 
        & s \transvia{c ∣ p}_{Ŝ_{e}} s' 
        \text{ iff } s \transvia{c ∣ p}_{Ŝ_{e₁}} s' \\*
        & \text{ iff } h₁(s) \transvia{c ∣ p}_{D̂} h₁(s') 
        \text{ iff } h(s) \transvia{c ∣ p}_{D̂} h(s'). 
    \end{align*}
    And similarly for \(s ∈ Ŝ_{e₂} ∩ Ŝ_{e}\).
    So we only need to show the homomorphic condition for the start state \(s^*\):
    \begin{align*}
        & s^* \transAcc{Ŝ_{e}}{c} \\
        \text{iff } &
        (∃ a, b ∧ a = c \text{ and } 
        s₁^* \transAcc{Ŝ_{e₁}}{a}) \\*
        & \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        s₂^* \transAcc{Ŝ_{e₂}}{a}) \\
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        h₁(s₁^*) \transAcc{D̂}{a}) \\*
        & \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        h₂(s₂^*) \transAcc{D̂}{a}) \\  
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        e₁ \transAcc{D̂}{a}) \\*
        & \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        e₂ \transAcc{D̂}{a}) \\  
        \text{iff }& e₁ +_b e₂ \transAcc{D̂}{c}\\
        \text{iff }& h(s^*) \transAcc{D̂}{c}. \\[5px]
        & s^* \transvia{a ∣ p}_{Ŝ_{e}} s' \\
        \text{iff } &
        (∃ a, b ∧ a = c \text{ and } 
        s₁^* \transvia{a ∣ p}_{Ŝ_{e₁}} s') \\*
        & \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        s₂^* \transvia{a ∣ p}_{Ŝ_{e₂}} s') \\
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        h₁(s₁^*) \transvia{a ∣ p}_{D̂} h(s')) \\*
        & \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        h₂(s₂^*) \transvia{a ∣ p}_{D̂} h(s')) \\  
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        e₁ \transvia{a ∣ p}_{D̂} h(s')) \\*
        & \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        e₂ \transvia{a ∣ p}_{D̂} h(s')) \\  
        \text{iff }& e₁ +_b e₂ \transvia{a ∣ p}_{D̂} h(s') \\ 
        \text{iff }& h(s^*) \transvia{a ∣ p}_{D̂} h(s').
    \end{align*}

    When \(e ≜ e₁; e₂\), by induction hypothesis, we have two homomorphisms \(h₁: Ŝ_{e₁} → D̂\) and \(h₂: Ŝ_{e₂} → D̂\).
    We define \(h\) as follows:
    \[h(s) ≜ \begin{cases}
        h₁(s); e₂ & s ∈ Ŝ_{e₁} \\  
        h₂(s) & s ∈ Ŝ_{e₂}
    \end{cases}\]
    Then we can prove that \(h\) is a homomorphism by case analysis on \(s\). 
    First case is that \(s ∈ Ŝ_{e₁}\):
    \begin{align*}
        & s \transAcc{Ŝ_{e}}{c} \\
        \text{ iff } & 
            ∃ a, b, a ∧ b = c, 
                s \transAcc{Ŝ_{e₁}}{a} 
                \text{ and } 
                s₂^* \transAcc{Ŝ_{e₂}}{b} \\
        \text{ iff } & 
            ∃ a, b, a ∧ b = c, 
                h₁(s) \transAcc{D̂}{a} 
                \text{ and } 
                f \transAcc{D̂}{b} \\
        \text{ iff } & h₁(s); f \transAcc{D̂}{c} 
        \text{ iff } h(s) \transAcc{D̂}{c}.\\[5px]
        & s \transvia{c ∣ p}_{S_{e}} s' \\
        \text{ iff } & 
            \begin{aligned}[t]
                (∃ a, b, a ∧ b = c
                & \text{ and }
                s \transAcc{Ŝ_{e₁}}{a} \\*
                & \text{ and }
                s₂^* \transvia{b ∣ p}_{Ŝ_{e₂}} s') 
            \end{aligned} \\*
            & \text{ or }
            (s \transvia{c ∣ p}_{Ŝ_{e₁}} s') \\  
        \text{ iff } & 
            \begin{aligned}[t]
                (∃ a, b, a ∧ b = c
                & \text{ and }
                h₁(s) \transAcc{D̂}{a} \\*
                &\text{ and }
                e₂ \transvia{b ∣ p}_{D̂} h₂(s'))
            \end{aligned} \\*
            & \text{ or }
            (h₁(s) \transvia{c ∣ p}_{D̂} h₁(s')) \\
        \text{ iff } & h₁(s) \transvia{c ∣ p} h(s') 
        \text{ iff } h(s) \transvia{c ∣ p} h(s').
    \end{align*}
    The case where \(s₂ ∈ Ŝ_{e₂}\) is straightforward, as \(Ŝ_{e}\) preserves the transitions of \(Ŝ_{e₂}\):
    \begin{align*}
        & s \transAcc{Ŝ_{e}}{c} 
        \text{ iff } s \transAcc{Ŝ_{e₂}}{c} \\*
        & \text{ iff } h₂(s) \transAcc{D̂}{c} 
        \text{ iff } h(s) \transAcc{D̂}{c}, \\ 
        & s \transvia{c ∣ p}_{Ŝ_{e}} s' 
        \text{ iff } s \transvia{c ∣ p}_{Ŝ_{e₂}} s' \\*
        & \text{ iff } h₂(s) \transvia{c ∣ p}_{D̂} h₂(s') 
        \text{ iff } h(s) \transvia{c ∣ p}_{D̂} h(s').
    \end{align*}
    

    When \(e ≜ {e₁}^{(b)}\), by induction hypothesis, we have a homomorphism \(h₁: Ŝ_{e₁} → D̂\); the homomorphism \(h\) can be defined as follows: 
    \[h(s) ≜ \begin{cases}
        {e₁}^{(b)} & s ≜ s^* \\  
        h₁(s); e₁^{(b)} & s ∈ Ŝ_{e₁}
    \end{cases}\]
    We prove the homomorphism condition by case analysis on \(s\). First case is that \(s = s^*\), then:
    \begin{align*}
        (s^* \transAcc{Ŝ_e}{c})
        \text{ iff } & (s^* \transAcc{Ŝ_e}{c} \text{ and } c = \overline{b}) \\*
        \text{ iff } & ({e₁}^{(b)} \transAcc{D̂}{c} \text{ and } c = \overline{b}) \\*
        \text{ iff } & (h(s^*) \transAcc{D̂}{c}); \\
        (s^* \transvia{c ∣ p}_{Ŝ_e} s')
        \text{ iff } & (∃ a, b ∧ a = c \text{ and } s₁^* \transvia{a ∣ p}_{Ŝ_{e₁}} s') \\
        \text{ iff } & (∃ a, b ∧ a = c \text{ and } e₁ \transvia{a ∣ p}_{D̂} h₁(s')) \\ 
        \text{ iff } & {e₁}^{(b)} \transvia{a ∣ p}_{D̂} h₁(s') 
        \text{ iff } h(s^*) \transvia{a ∣ p}_{D̂} h(s')
    \end{align*}
    The second case is when \(s ∈ Ŝ_{e₁}\), then:
    \begin{align*}
        & s \transAcc{Ŝ_e}{c}\\
        \text{ iff } & (∃ a, \overline{b} ∧ a = c \text{ and } s \transAcc{Ŝ_{e₁}}{a}) \\  
        \text{ iff } & (∃ a, \overline{b} ∧ a = c \text{ and } h₁(s) \transAcc{D̂}{a}) \\
        \text{ iff } & h₁(s);e₁^{(b)} \transAcc{D̂}{c} 
        \text{ iff } h(s) \transAcc{D̂}{c} \\[5px]
        & s \transvia{c ∣ p}_{Ŝ_e} s' \\
        \text{ iff } & 
            (s \transvia{c ∣ p}_{Ŝ_{e₁}} s' \\
            & \text{ or } 
            \begin{aligned}[t]
                ∃ a₁, a₂, {}
                & b ∧ a₁ ∧ a₂ = c \\*
                & \text{ and } 
                s \transAcc{Ŝ_{e₁}}{a₁} \\*
                & \text{ and } 
                s₁^* \transvia{a₂ ∣ p}_{Ŝ_{e₁}} s')
            \end{aligned} \\
        \text{ iff } & 
            (h₁(s) \transvia{c ∣ p}_{D̂} h₁(s') \\*
            & \text{ or } 
            \begin{aligned}[t]
                ∃ a₁, a₂, &{} b ∧ a₁ ∧ a₂ = c \\*
                & \text{ and } 
                h₁(s) \transAcc{D̂}{a₁} \\*
                & \text{ and } 
                e₁ \transvia{a₂ ∣ p}_{D̂} h₁(s')) 
            \end{aligned}\\
        \text{ iff } & 
            (h₁(s) \transvia{c ∣ p}_{D̂} h₁(s')  \\
            & \text{ or } 
            \begin{aligned}[t]
                ∃ a₁, a₂, &{} b ∧ a₁ ∧ a₂ = c \\
                & \text{ and } 
                h₁(s) \transAcc{D̂}{a₁} \\
                &{} \text{ and } 
                e₁^{(b)} \transvia{b ∧ a₂ ∣ p}_{D̂} h₁(s');e₁^{(b)}) \\ 
            \end{aligned}\\
        \text{ iff } &
            (h₁(s); e₁^{(b)} \transvia{c ∣ p}_{D̂} h₁(s'); e₁^{(b)}) \\
        \text{ iff } & h(s) \transvia{c ∣ p}_{D̂} h(s').
        \qedhere
    \end{align*}
\end{proofEnd}

\Cref{thm:hom-thompson-derivative} have several consequences, one of the more obvious one is that we can use the functoriality of the lowering operation to show the semantic equivalence of the start state in the Thompson's construction and the expression in derivative. 

\begin{theoremEnd}{corollary}[Correctness]
    Given any expression \(e\) and its Thompson's coalgebra \(Ŝ_{e}\) with the start state \(s^* ∈ Ŝ_{e}\), then the semantics of the start state is equivalent to the semantics of \(e\): \(⟦s^*⟧_{Ŝ_{e}} = ⟦e⟧.\)
\end{theoremEnd}

\begin{proofEnd}
    By functoriality of lowering~\cref{thm:lowering-functor}, then \(h: S_{e} → D\) is a homomorphism on their lowerings. 
    Because homomorphism preserves semantics (\Cref{thm:hom-preserves-semantics}) \(⟦s^*⟧^{ω}_{S_{e}} = ⟦h(s^*)⟧^{ω}_{D} = ⟦e⟧^{ω}_{D}\), where \(⟦-⟧^{ω}\) is the infinite trace semantics i.e. the unique map into the final GKAT coalgebra \(𝒢_ω\).

    Finally, because infinite trace equivalence implies finite trace equivalence (\Cref{thm:inf-trace-equiv-implies-fin-trace-equiv}) and the correctness of derivative (\Cref{thm:derivative-correctness}), we obtain the following chain of equalities:
    \(⟦s^*⟧_{S_{e}} = ⟦e⟧_{D} = ⟦e⟧\).
\end{proofEnd}

A not so obvious consequence of the homomorphism in~\Cref{thm:hom-thompson-derivative}, is the complexity of the algorithm based on derivatives.
Our bisimulation algorithm (\Cref{alg:symb-bisim}) only explores the principle sub-coalgebra of the start state, i.e. \(s^*\) in the Thompson's construction \(Ŝ_{e}\) or \(e\) in the derivative \(D̂\); thus, deducing an upper bound on the size of the principle sub-coalgebras \(⟨s^*⟩_{Ŝ_{e}}\) and \(⟨e⟩_{D̂}\) are crucial to our complexity analysis.
An upper bound on \(⟨s^*⟩_{Ŝ_{e}}\) is easy to obtain, as the size of \(Ŝ_{e}\), which subsumes the states of \(⟨s^*⟩_{Ŝ_{e}}\), is linear to the size of expression \(e\); therefore \(⟨s^*⟩_{Ŝ_{e}}\) is at most linear to the size of the expression \(e\).
On the other hand the size of \(⟨e⟩_{D̂}\) can, again, be derived from the homomorphism in~\cref{thm:hom-thompson-derivative}.

\begin{theoremEnd}{corollary}\label{thm:suj-hom-thompson-derivative}
    There exists a surjective homomorphism \(h': ⟨s^*⟩_{Ŝ_{e}} → ⟨e⟩_{D̂}\). 
    Because the size of \(⟨s^*⟩_{Ŝ_{e}}\) is linear to \(e\), the size of \(⟨e⟩_{D̂}\) is at most linear to the size of expression \(e\).
\end{theoremEnd}

\begin{proofEnd}
    We define \(h'\) to be point-wise equal to \(h\), i.e. \(h'(s) ≜ h(s)\), i.e. \(h'\) is \(h\) restricted on the domain \(⟨s^*⟩_{Ŝ_{e}}\). 
    We need to show that \(h'\) is well-defined and surjective, which is a consequence of homomorphic image preserves principle sub-coalgebra (\Cref{thm:homo-img-preserve-principle-sub-coalg}): \(h(⟨s^*⟩_{Ŝ_{e}}) = ⟨h(s)⟩_{D̂} = ⟨e⟩_{D̂}.\)
    In other words, the image of \(h\) on \(⟨s^*⟩_{Ŝ_{e}}\) is equal to \(⟨e⟩_{D̂}\); thus, because \(h'\) the restriction of \(h\) on \(⟨s^*⟩_{Ŝ_{e}}\), the range of \(h'\) contains its codomain \(⟨e⟩_{D̂}\), showing that \(h'\) is surjective.
\end{proofEnd}

Because of the surjectivity of \(h'\) in~\Cref{thm:suj-hom-thompson-derivative}, \(⟨s^*⟩_{Ŝ_{e}}\) has no fewer states than \(⟨e⟩_{D̂}\).
And, the number of states in \(Ŝ_{e}\) is greater than \(⟨s^*⟩_{Ŝ_{e}}\), which is greater than \(⟨e⟩_{D̂}\); by induction, the number of states in \(Ŝ_{e}\) is linear to the size of the input expression, therefore the number of states in \(⟨e⟩_{D̂}\) is at most linear to the size of the expression \(e\). 

Although the size of the coalgebra generated by the derivative is smaller than Thompson's construction, the decision procedure based on derivative is not always more efficient than those based on Thompson's construction. 
Crucially, the states in the derivative coalgebra \(D̂\) are expressions, which are more expensive to store; and computing the next transition of the coalgebra can also be more computationally expensive.
Whereas, Thompson's construction only requires inductively going through the expression once to construct the entire coalgebra \(Ŝ_{e}\), and its states can be represented by more efficient constructs, like integers. 

\section{Implementation and Evaluation}\label{sec:implementation}
We implement our symbolic on-the-fly bisimulation algorithm~(\Cref{alg:symb-bisim}) in Rust along with derivative based~(\Cref{fig:derivatives-rules}) and Thompson's construction based~(\Cref{tab:symb-Thompson-construction}) algorithms for constructing symbolic GKAT coalgebras. The source code and benchmark suite is freely available in our repository \url{https://anonymous.4open.science/r/rust-gkat-071E}.

\subsection{Optimization}\label{sec:optimization-implementation}
\begin{definition}[blocked transition]
    For any \(s \in S\), a transition \((b, s', p) \in \hat{\delta}_S(s)\) is blocked if \(b \equiv 0\).
\end{definition}

Notice that during bisimulation~(\Cref{alg:symb-bisim}) for some \(s \in S\) and \(u \in U\), if there is \(s \transvia{b ∣ p} s'\), i.e. \((b,s',p) \in \hat{\delta}_S(s)\), where \(b\) is a semantically false boolean expression, then implications with premises of the form \((b \land \cdot) \not{\equiv} 0\) are trivially satisfied. In other words, \textit{blocked} transitions in \(S\) do not contribute to the result of bisimulation. The same observation holds true symmetrically for \(U\). So blocked transitions of a coalgebra can be safely pruned without impacting the result of bisimulation.

While blocked transitions are irrelevant regarding the result of bisimulation, they can negatively impact the performance of bisimulation as the algorithm may perform many unneeded satisfiability checks. To prevent performance degradation due to blocked transitions, we remove them through an \textit{eager-pruning} optimization. Basically, in our coalgebra construction algorithms, \(\hat{\delta}\) is constructed with only transitions \((b, s, p)\) where \(b \not{\equiv} 0\). The coalgebras produced in this manner essentially have their blocked transitions pruned at the time of construction (eager). 

Eager-pruning also improves the efficiency of the Thompson's construction algorithm~(\Cref{tab:symb-Thompson-construction}) significantly. This is due to the fact that eager-pruning can reduce the size of \(\hat{\delta}'\) computed recursively for sub-expressions, which in turn makes computing \(\hat{\delta}\) for the overall expression faster.

As mentioned in \Cref{sec:symb-gkat-construction}, our implementation of Thompson's construction uses start dynamics (pseudo-state) instead of an explicit start state. This reduces the number of indirections needed to access the behaviors \(\hat{\epsilon}(s^*) \) and \(\hat{\delta}(s^*)\) of start state \(s^*\). More specifically, we track the behaviors \(\epsilon^*\) and \(\delta^*\) which characterize \(\hat{\epsilon}(s^*) \) and \(\hat{\delta}(s^*)\) respectively. The start state \(s^*\) does not appear explicitly during coalgebra construction. After the construction algorithm terminates, we generate a fresh state \(s^*\) and set \(\hat{\epsilon}(s^*) = \epsilon^*\) and \(\hat{\delta}(s^*) = \delta^*\). The final coalgebra that our implementation produces is equivalent to what would have been obtained from the original algorithm (\Cref{tab:symb-Thompson-construction}) modulo consistent renaming of states.

The final optimization that we perform is collapsing the boolean expression set \(\hat{\epsilon}(s)\) into a single boolean expression. Basically, each \(\hat{\epsilon}(s)\) is represented by the boolean disjunction of its elements. Consider the guard operation defined in \Cref{sec:symb-gkat-construction}.
\begin{align*}
    ⟨\hat{\epsilon_1}(s_1)|\hat{\epsilon_2}(s_2) & ≜ \{b ∧ c ∣ b ∈ \hat{\epsilon_1}(s_1), c ∈ \hat{\epsilon_2}(s_2)\}
\end{align*}
If \(\hat{\epsilon_1}(s_1)\) and \(\hat{\epsilon_2}(s_2)\) are sets of boolean expressions, then the guard operation would produce a set whose size is proportional to \(|\hat{\epsilon_1}(s_1)| \times |\hat{\epsilon_2}(s_2)|\). However, if the collapsed representation is used, \(\hat{\epsilon_1}(s_1)\) and \(\hat{\epsilon_2}(s_2)\) become just boolean expressions instead of sets of expressions. The guard operation now produces a single boolean expression. The distributivity of boolean conjunction and disjunction ensures the correctness of this optimization.

\subsection{Evaluation}\label{sec:performance-implementation}
\begin{table}
\footnotesize
\centering
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l r r r r r}
    Benchmark & DV$_{\textsf{BDD}}$ & TC$_{\textsf{BDD}}$ & DV$_{\textsf{SAT}}$ & TC$_{\textsf{SAT}}$ & SK \\
    \hline
    e250b5p10ne    & 0.18   & 0.18   & 0.16 & 0.17 &    5.82 \\
    e250b5p10eq    & 0.19   & 0.18   & 0.15 & 0.18 &    2.83 \\
    e500b5p50ne    & 0.21   & 0.21   & 0.21 & 0.22 &   37.28 \\
    e500b5p50eq    & 0.22   & 0.21   & 0.20 & 0.27 &   14.06 \\
    e1000b10p100ne & 0.26   & 0.28   & 0.34 & 0.38 & timeout \\
    e1000b10p100eq & 0.28   & 0.26   & 0.28 & 0.41 &   77.83 \\
    e2000b20p200ne & 1.32   & 2.60   & 0.68 & 0.79 & timeout \\
    e2000b20p200eq & 1.92   & 2.95   & 0.41 & 1.00 & timeout \\
    e3000b30p200ne & 1.57   & 24.86  & 1.30 & 1.60 & timeout \\
    e3000b30p200eq & 10.82  & 22.83  & 0.66 & 1.54 & timeout \\
    degenerate     & 99.48  & 228.16 & 0.24 & 0.30 & timeout
\end{tabular}
\caption{Total Time Usage (seconds)}\label{tab:benchmark-time}
\end{table}
\begin{table}
\footnotesize
\centering
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l r r r r r}
    Benchmark & DV$_{\textsf{BDD}}$ & TC$_{\textsf{BDD}}$ & DV$_{\textsf{SAT}}$ & TC$_{\textsf{SAT}}$ & SK \\
    \hline
    e250b5p10ne    & 15.17  & 14.54   & 7.07  & 7.01  &  114.06 \\
    e250b5p10eq    & 15.69  & 14.69   & 7.06  & 7.29  &  100.48 \\
    e500b5p50ne    & 15.99  & 15.33   & 7.02  & 7.29  &  524.89 \\
    e500b5p50eq    & 16.85  & 15.26   & 6.99  & 7.01  &  546.91 \\
    e1000b10p100ne & 17.51  & 21.31   & 7.78  & 7.50  & timeout \\
    e1000b10p100eq & 19.52  & 17.48   & 8.17  & 7.04  & 5822.66 \\
    e2000b20p200ne & 237.17 & 280.70  & 12.95 & 11.43 & timeout \\
    e2000b20p200eq & 107.61 & 102.18  & 12.76 & 13.25 & timeout \\
    e3000b30p200ne & 112.06 & 1232.24 & 21.00 & 19.30 & timeout \\
    e3000b30p200eq & 245.23 & 228.86  & 21.18 & 17.64 & timeout \\
    degenerate     & 632.70 & 1225.24 & 19.13 & 17.94 & timeout
\end{tabular}
\caption{Peak Memory Usage (megabytes)}\label{tab:benchmark-memory}
\vspace{-1em}
\end{table}

Due to the fact that our symbolic algorithms are generic regarding the choice of solvers for boolean satisfiability, we compare their performance using BDD to using a SAT solver (MiniSat~\cite{een_MINISAT_2004}). Our experiments are performed on a laptop with an Apple M4 Pro CPU and 24 GB RAM. \Cref{tab:benchmark-time,tab:benchmark-memory} present the experimental results of equivalence checking based on derivatives (DV), Thompson's construction (TC) and SymKAT~\cite{pous_SymbolicAlgorithmsLanguage_2015} (SK). Note that SymKAT uses BDD internally for checking boolean satisfiability. Each benchmark consists of 50 expression pairs with properties described by the benchmark's name. For example, expressions in `e250b5p10eq' have approximately 250 primitive actions in total (e250), a maximum boolean expression size of 5 (b5), 10 possible unique primitive tests (p10) and are known to be equivalent (eq). Benchmarks with the suffix `ne' have expression pairs that are known to be non-equivalent. 

We observe that DV performs better than TC consistently when both algorithms use the same solver backend and all variants of DV and TC perform better than SymKAT. When comparing the solver backends, BDD and SAT perform similarly for smaller benchmarks, however, BDD's time and memory usage increase considerably for larger benchmarks. On the other hand, the performance of SAT is stable for all benchmarks. Furthermore, we have encountered degenerative expression pairs (the `degenerate' benchmark) which greatly reduce BDD performance but can be efficiently checked by SAT. We did not encounter any expression pairs where BDD performed significantly better than SAT.

\section{Related Works and Discussions}

\subsection{Generic Symbolic Techniques}

There are many studies that utilizes symbolic techniques to solve various problems surrounding (extended) regular expressions, and have found a wide range of real-world applications, including but not limited to low level program analysis~\cite{dallapreda_AbstractSymbolicAutomata_2015a}, list comprehension~\cite{saarikivi_FusingEffectfulComprehensions_2017}, constraint solving~\cite{stanford_SymbolicBooleanDerivatives_2021}, HTML decoding, malware fingerprinting, image blurring, location privacy~\cite{veanes_SymbolicFiniteState_2012}, regex processing, and string sanitizer~\cite{veanes_ApplicationsSymbolicFinite_2013}.

Our study, on the other hand,  focus on GKAT and GKAT automata (represented as coalgebra throughout the paper). 
Although previous works on deterministic symbolic transducer~\cite{saarikivi_FusingEffectfulComprehensions_2017,veanes_SymbolicFiniteState_2012} might seem similar to that of GKAT, the automata shape are subtly different, specifically, instead of having accept and rejecting states, states in GKAT automata will accept or reject its input.

Another difference between aforementioned works and ours is that we utilize the coalgebraic theory of GKAT to streamline some of our proofs.
In fact, we are not the first to look at symbolic transition system through the lens of coalgebra, \citeauthor{bonchi_CoalgebraicSymbolicSemantics_2009}~\cite{bonchi_CoalgebraicSymbolicSemantics_2009} have an elegant coalgebra theory surrounding symbolic transition system.
However, instead of defining the symbolic semantics via the coalgebra theory, we opt to use the notion of lowering to connect symbolic GKAT coalgebra and GKAT coalgebra.
This approach allows us to leverage previous correctness proofs like in~\Cref{thm:derivative-correctness}, and also enables a non-symbolic equivalence-checking algorithm as in~\cref{alg:bisim}.
Another notable difference is that our equivalence checking also need to handle normalization, which is a unique property of GKAT not found in general coalgebra.

\subsection{KAT and GKAT}

GKAT is a guarded fragment of Kleene Algebra with Tests (KAT), which enjoys a symbolic algorithm~\cite{pous_SymbolicAlgorithmsLanguage_2015}.
This algorithm by~\citeauthor{pous_SymbolicAlgorithmsLanguage_2015} generalizes the notion of derivatives also to boolean expressions, which uses binary decision diagram (BDD) as transition between symbolic expressions.
Although some of our examples utilizes BDD to solve equivalence and inequivalence of boolean expressions, because of the structure of GKAT, our algorithm is not bounded by a particular boolean representation or solvers.
In fact, we have demonstrated that in many scenarios, SAT solvers like MiniSat~\cite{een_MINISAT_2004} can achieve better performance than BDD.

In similar veins, KATch~\cite{moeller_KATchFastSymbolic_2024} is a symbolic solver for NetKAT~\cite{anderson_NetKATSemanticFoundations_2014} based on forwarding decision diagram, and achieved outstanding performance among state-of-the-art tools for reasoning about software-defined networks.
GKAT also stem from the research of the research of software-defined networks~\cite{smolka_ScalableVerificationProbabilistic_2019,smolka_GuardedKleeneAlgebra_2020}, and quickly found applications in probabilistic verification~\cite{ro.zowski_ProbabilisticGuardedKAT_2023}, probabilistic program logic~\cite{gomes_KleeneAlgebraTests_2024}, networks~\cite{wasserstein_GUARDEDNETKATSOUNDNESS_2023}, and control-flow validation~\cite{zhang_CFGKATEfficientValidation_2025}.
It would be interesting to see how our symbolic algorithm can be used to speed up these applications.

\printbibliography

\clearpage
\appendix

\section{Detailed Proof}
\printProofs

\begin{onecolumn}
\begin{lstlisting}[
    caption=Dead State Detection Implementation,
    label=lst:dead-state-detection
]
pub type Deriv<B> = Vec<(B, Exp<B>, u64)>;

pub struct Solver<B> {
    // search states
    dead_states: HashSet<Exp<B>>,
    explored: HashSet<Exp<B>>,
    uf_table: HashMap<Exp<B>, UnionFindNode<()>>,
    // caching
    eps_cache: HashMap<Exp<B>, B>,
    drv_cache: HashMap<Exp<B>, Deriv<B>>,
}

impl<B: BExp> Solver<B> {
    #[inline]
    pub fn known_dead(&self, exp: &Exp<B>) -> bool {
        self.dead_states.contains(&exp)
    }

    pub fn is_dead<G: Gkat<B>>(&mut self, gkat: &mut G, exp: &Exp<B>) -> bool {
        let mut stack = Vec::new();
        stack.push(exp.clone());
        self.explored.clear();
        while let Some(exp) = stack.pop() {
            if self.known_dead(&exp) || self.explored.contains(&exp) {
                continue;
            }
            self.explored.insert(exp.clone());
            let eps = self.epsilon(gkat, &exp);
            if gkat.is_false(&eps) {
                for (_, e, _) in self.derivative(gkat, &exp) {
                    stack.push(e);
                }
            } else {
                return false;
            }
        }
        self.dead_states.extend(self.explored.iter().cloned());
        return true;
    }
}
\end{lstlisting}
\end{onecolumn}


\end{document}
% Please compile this document using LuaLaTeX.
% because of the use of unicode-math
% XeLaTeX and PDFLaTeX will result in error.

\newif\iffull\fulltrue
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

% \usepackage{cite}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%%%%% PACKAGES
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[margin=2.5cm]{geometry}

% Biblatex
\usepackage[style=numeric]{biblatex}
\addbibresource{refs.bib}

% For adding inline comments in the text.
\usepackage[margin=false,inline=true]{fixme}
\FXRegisterAuthor{aaa}{anaaa}{\color{cyan}AAA}
\FXRegisterAuthor{mg}{anmg}{\color{red}MG}
\FXRegisterAuthor{cz}{ancz}{\color{orange}CZ}
% \newcommand{\aaa}[1]{\aaanote{#1}}
% \newcommand{\mg}[1]{\mgnote{#1}}
% \newcommand{\cz}[1]{\cznote{#1}}
\newcommand{\aaa}[1]{}
\newcommand{\mg}[1]{}
\newcommand{\cz}[1]{}

\usepackage{stmaryrd}

\usepackage{stackengine}
\usepackage{mathrsfs}
\usepackage{braket}
\usepackage{annotate-equations}
\usepackage{scalerel}

% graphs
\usepackage{tikz}
\usetikzlibrary{arrows,automata,positioning}

% commutative diagram
\usepackage{tikz-cd}

% ref
\usepackage{hyperref}
\usepackage{cleveref}

% inference rule
\usepackage{mathpartir}
% cross-referencing infer rule
% based on https://tex.stackexchange.com/questions/340788/cross-referencing-inference-rules
\makeatletter
\let\originferrule\inferrule
\DeclareDocumentCommand \inferrule { s O {} m m}{%
  \IfBooleanTF{#1}%
  {%
    \mpr@inferstar[#2]{#3}{#4}%
  }{%
    \mpr@inferrule[#2]{#3}{#4}%
  }%
  \IfValueT{#2}%
  {%
    \my@name@inferrule{#2}%
  }%
}
\NewDocumentCommand \my@name@inferrule { m }{%
  \def\@currentlabelname{\textsc{#1}}%
}
\makeatother

% item spacing
\usepackage{enumitem}

% for code
\usepackage{listings}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{framextopmargin=50pt,frame=bottomline}
\lstset{language=caml, escapeinside={[*}{*]}}

% for algorithm
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
% switch statement for pattern matching
\algnewcommand\algorithmicmatch{\textbf{match}}
\algnewcommand\algorithmicwith{\textbf{with}}
\algnewcommand\algorithmiccase{\textbf{case}}
\algnewcommand\algorithmicdefault{\textbf{default}}
\algnewcommand\Skip{\textbf{skip}}
\algdef{SE}[MATCH]{Match}{EndMatch}[1]{\algorithmicmatch\ #1\ \algorithmicwith}{\algorithmicend\ \algorithmicmatch}%
\algdef{SE}[CASE]{Case}{EndCase}[1]{\algorithmiccase\ #1 \algorithmicthen}{\algorithmicend\ \algorithmiccase}%
\algdef{SE}[DEFAULT]{Default}{EndDefault}{\algorithmicdefault\ }{\algorithmicend\ \algorithmicdefault}%
\algtext*{EndMatch}%
\algtext*{EndCase}%
\algtext*{EndDefault}%

% for better table
\usepackage{booktabs}

% subcaption
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption

% unicode math symbols
\usepackage{unicode-math}
% support for hat, overline, underline, vec, and sim combining charactors
\protected\def\afteracc{\directlua{
    local nest = tex.nest[tex.nest.ptr]
    local last = nest.tail
    if not (last and last.id == 18) then
      error'I can only put accents on simple noads.'
    end
    if last.sub or last.sup then
      error'If you want accents on a superscript or subscript, please use braces.'
    end
    local acc = node.new(21, 1)
    acc.nucleus = last.nucleus
    last.nucleus = nil
    local is_bottom = token.scan_keyword'bot' and 'bot_accent' or 'accent'
    acc[is_bottom] = node.new(23)
    acc[is_bottom].fam, acc[is_bottom].char = 0, token.scan_int()
    nest.head = node.insert_after(node.remove(nest.head, last), nil, acc)
    nest.tail = acc
    node.flush_node(last)
  }}
\AtBeginDocument{
\begingroup
  \def\UnicodeMathSymbol#1#2#3#4{%
    \ifx#3\mathaccent
      \def\mytmpmacro{\afteracc#1 }%
      \global\letcharcode#1=\mytmpmacro
      \global\mathcode#1="8000
    \else\ifx#3\mathbotaccentwide
      \def\mytmpmacro{\afteracc bot#1 }%
      \global\letcharcode#1=\mytmpmacro
      \global\mathcode#1="8000
    \fi\fi
  }
  \input{unicode-math-table}
\endgroup
}

% math font, this is needed to render \setminus command
\setmathfont{latinmodern-math}
\setmathfont[range=\setminus]{STIX Two Math}
\setmathfont[range=\similarrightarrow]{STIX Two Math}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}

\newtheorem{definition}{Definition}
\newtheorem*{definition*}{Definition}
\newtheorem{example}[definition]{Example}
\newtheorem{remark}[definition]{Remark}

%%%% Macros %%%%%

% Math?
\newcommand{\true}{\mathrm{true}}
\newcommand{\false}{\mathrm{false}}
\newcommand{\At}{\mathbf{At}}


% operators
\newcommand{\dom}[1]{\mathrm{dom}(#1)}
\newcommand{\cod}[1]{\mathrm{cod}(#1)}
\DeclareMathOperator{\post}{\mathrm{post}}
\newcommand{\reject}{\mathinner{\mathrm{rej}}}
\newcommand{\accept}{\mathinner{\mathrm{acc}}}
\DeclareMathOperator*{\bigplus}{\scalerel*{+}{\sum}}
\newcommand{\clos}[1]{\mathrel{\overline{#1}}}
\DeclareMathOperator{\norm}{\mathrm{norm}}
\DeclareMathOperator{\dead}{\mathrm{dead}}
\DeclareMathOperator{\symb}{\mathrm{symb}}
\DeclareMathOperator{\unsymb}{\symb^{-1}}


% commands 
\newcommand{\command}[1]{{\mathtt{#1}}}
\newcommand{\comAssume}[1]{\command{assume}~#1}
\newcommand{\comITE}[3]{\command{if}~#1~\command{then}~#2~\command{else}~#3}
\newcommand{\comWhile}[2]{\command{while}~#1~\command{do}~#2}

% set of models
\newcommand{\theoryOf}[1]{\ensuremath{\mathsf{#1}}}
\newcommand{\Exp}{\theoryOf{Exp}}
\newcommand{\GKAT}{\theoryOf{GKAT}}
\newcommand{\Bool}{\theoryOf{Bool}}
\DeclareMathOperator{\GS}{\mathrm{GS}}

\newcommand\altxrightarrow[2][0pt]{\mathrel{\ensurestackMath{\stackengine%
  {\dimexpr#1-7.5pt}{\xrightarrow{\phantom{#2}}}{\scriptstyle\!#2\,}%
  {O}{c}{F}{F}{S}}}}
\newcommand{\transvia}[1]{
    \mathrel{\raisebox{-2px}{\(\altxrightarrow[-2px]{#1}\)}}
}
\newcommand{\transAcc}[2]{⇒_{#1} #2}

 

\begin{document}

\title{Symbolic On-the-fly Algorithms for GKAT Equivalences
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Cheng Zhang\textsuperscript{\textsection}}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{University College London}\\
London, United Kingdom \\
0000-0002-8197-6181}
\and
\IEEEauthorblockN{Qiancheng Fu}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
email address or ORCID}
\and
\IEEEauthorblockN{Hang Ji}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
email address or ORCID}
\and
\IEEEauthorblockN{Ines Santacruz}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
email address or ORCID}
\and
\IEEEauthorblockN{Marco Gaboardi}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Boston University}\\
Boston, USA \\
email address or ORCID}
}

\maketitle
\begingroup\renewcommand\thefootnote{\textsection}
\footnotetext{Work largely performed at Boston University}
\endgroup

\begin{abstract}
This document is a model and instructions for \LaTeX.
This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert.
\end{IEEEkeywords}



\section{Introduction}

\paragraph{Notation: } In this paper, we will use un-curried notation to apply curried functions, for example, given a function \(δ: X → Y → Z\), we will write the function applications as follow \(δ(x): Y → Z\) and \(δ(x, y): Z\). And when drawing commutative diagram, we will leave function restriction implicit. Specifically given \(A' ⊆ A\), and a function \(h: A → B\), we will draw:
\[
    \begin{tikzcd}
        A' \ar{r}{h} & B
    \end{tikzcd}
\]
where the function \(h\) is implicitly restricted to \(A'\).
For bifunctors like \((-) × (-)\), we will write function lifting by applying the bifunctors on these functions: for example, given \(h₁: A₁ → B₁\) and \(h₂: A₂ → B₂\), we will use \[h₁ × h₂: A₁ × A₂ → B₁ × B₂\] to denote the bifunctorial lift of \(h₁\) and \(h₂\) via product \((-) × (-)\).

\section{Preliminary}

\subsection{Concepts in Universal Coalgebra}

In this paper, we will make heavy use of coalgebraic theory, thus it is empirical for us to recall some notions and useful theorems in universal coalgebra.
Given a functor \(F\) on the category of set and functions, a \emph{coalgebra over \(F\)} or \emph{\(F\)-coalgebra} consists of a set \(S\) and a function \(σ_S: S → F(S)\).
We typically call elements in \(S\) the \emph{states} of the coalgebra, and \(σ_S(s)\) the \emph{dynamic} of state \(s\).
We will sometimes use the states \(S\) to denote the coalgebra, when no ambiguity can arise. 

A homomorphism between two \(F\)-coalgebra \(S\) and \(U\) is a map \(h: S → U\) that preserves the function \(σ\); diagrammatically, the following diagram commutes:
\[
    \begin{tikzcd}
        S \ar{r}{h} \ar[swap]{d}{σ_S} & U \ar{d}{σ_U} \\  
        F(S) \ar{r}{F(h)} & F(U)
    \end{tikzcd}    
\]

When we can restrict the homomorphism map into a inclusion map \(i: S' → S\) for \(S' ⊆ S\), then we say that \(S'\) is a \emph{sub-coalgebra} of \(S\), denoted as \(S' ⊑ S\). Specifically, the following diagram commutes when \(S' ⊑ S\):
\[
    \begin{tikzcd}
        S' \ar[hook]{r}{i} \ar[swap]{d}{σ_{S'}} & S \ar{d}{σ_S} \\  
        F(S') \ar[hook]{r}{F(i)} & F(S)
    \end{tikzcd}    
\]
In fact, the function \(σ_{S'}\) is uniquely determined by the states \(S'\)~\cite[Proposition 6.1]{rutten_UniversalCoalgebraTheory_2000}.

The sub-coalgebras are preserved under homomorphic images and pre-images: 
\begin{lemma}[Theorem 6.3~\cite{rutten_UniversalCoalgebraTheory_2000}]\label{thm:hom-(pre)img-preserve-sub-coalg}
    given a homomorphism \(h: S → U\), and sub-coalgebras \(S' ⊑ S\) and \(U' ⊑ U\), then 
    \[h(S') ⊑ U \text{ and } h^{-1}(U') ⊑ S.\]
\end{lemma}

One particularly important sub-coalgebra of and coalgebra \(S\) is the least coalgebra generated by a single element \(s\). 
We will denote this sub-coalgebra as \(⟨s⟩_{S}\), and call it \emph{principle sub-coalgebra} generated by \(s\). 
We sometimes omit the subscript \(S\) when it can be inferred from context or irrelevant.
Intuitively, we usually think of principle sub-coalgebra \(⟨s⟩_S\) as the sub-coalgebra that is formed by all the ``reachable state'' form the state \(s\).
This coalgebraic characterization of reachable state can allow us to avoid induction on the length of path from \(s\) to reach another state.

For all coalgebra \(S\) and a state \(s ∈ S\), principle sub-coalgebra \(⟨s⟩_S\) always exists and is unique, because sub-coalgebra of any coalgebra forms a complete lattice~\cite[theorem 6.4]{rutten_UniversalCoalgebraTheory_2000}; thus taking the meet of all the sub-coalgebra that contains \(s\) will yield \(⟨s⟩_S\).

Similar to sub-coalgebra, principle sub-coalgebra is also preserved under homomorphic image:
\begin{theorem}\label{thm:homo-img-preserve-principle-sub-coalg}
    Homomorphic image preserves principle sub-GKAT coalgebra. Specifically, given a homomorphism \(h: S → U\):
    \[h(⟨s⟩_S) = ⟨h(s)⟩_U\]
\end{theorem}

\begin{proof}
    We will need to show that \(h(⟨s⟩_{S})\) is the smallest sub-GKAT coalgebra of \(S\) that contain \(h(s)\). First by definition of image, \(h(s) ∈ h(⟨s⟩_{S})\); second by \cref{thm:hom-(pre)img-preserve-sub-coalg}, \(h(⟨S⟩_S) ⊑ U\).

    Finally, take any \(U' ⊑ U\) and \(h(s) ∈ U'\), recall that by \cref{thm:hom-(pre)img-preserve-sub-coalg}, \(h^{-1}(U') ⊑ S\). We can then derive that \(h(⟨s⟩_S) ⊑ U'\): 
    \begin{align*}
        h(s) ∈ U' 
        & ⟹ s ∈ h^{-1}(U') \\  
        & ⟹ ⟨s⟩_S ⊑ h^{-1}(U') & \text{definition of \(⟨s⟩_S\)}\\  
        & ⟹ h(⟨s⟩_S) ⊑ U' & \text{\cref{thm:hom-(pre)img-preserve-sub-coalg}}
    \end{align*}
    Hence \(h(⟨s⟩_S)\) is the smallest sub-GKAT coalgebra of \(U\) that contains \(h(s)\).
\end{proof}

% bisimulation

A \emph{final coalgebra} \(ℱ\) over a signature \(F\), sometimes called the \emph{behavior} of coalgebras over \(F\), is a \(F\)-coalgebra s.t. for all \(F\)-coalgebra \(S\), there exists a unique homomorphism \(⟦-⟧_S: S → ℱ\).

Given two \(F\)-coalgebra \(S\) and \(U\), the \emph{behavioral equivalence} between states in \(S\) and \(U\) can be computed by a notion called \emph{bisimulation}.
A relation \({∼} ⊆ S × U\) is called a \emph{bisimulation relation} if it forms an \(F\)-coalgebra: \[σ_{∼}: {∼} → F(∼),\] 
And its projections \(π₁: S × U → S\) and \(π₂: S × U → U\) are both homomorphisms:
\[
    \begin{tikzcd}
        S \ar[swap]{d}{σ_S}
            & ∼ \ar[swap]{l}{π₁} \ar{r}{π₂} \ar{d}{σ_{∼}} 
            & U \ar{d}{σ_T}\\  
        F(S) & F(∼) \ar{l}{F(π₁)} \ar[swap]{r}{F(π₂)} & F(T)
    \end{tikzcd}
\]
In a special case, when there exists a homomorphism \(h: S → U\), then we can simply pick \({∼} ⊆ S × U\) to be \(\{(s, h(s)) ∣ s ∈ S\}\), which gives us a bisimulation with the lift \(σ_{∼} ≜ σ_S × σ_U\).

\begin{corollary}
    Given two \(F\)-coalgebra \(S, U\) and a homomorphism \(h: S → U\), then all for all \(s\), \(⟦s⟧_{S} = ⟦h(s)⟧_{U}.\)
\end{corollary}

\subsection{Guarded Kleene Algebra With Tests}



\subsection{GKAT Coalgebra}

% definition
GKAT coalgebra~\cite{smolka_GuardedKleeneAlgebra_2020,schmid_GuardedKleeneAlgebra_2021}, is a coalgebraic systems for GKAT.
Specifically GKAT coalgebras over a alphabet \(K, B\) are coalgebras over the following functor:
\[G(S) ≜ (2 + S × K)^{\At_B},\] 
where \(2 ≜ \{\accept, \reject\}\). Intuitively given a state \(s ∈ S\) and an atom \(α ∈ \At\), \(δ(s, α)\) will deterministically execute one of the following: reject \(α\), denoted as \(δ(s, α) = \reject\); accept \(α\), denoted \(δ(s, α) = \accept\); or transition to a state \(s' ∈ S\) and execute action \(p ∈ K\), denoted as \(δ(s, α) = (s', p)\).

This deterministic behavior contrast that of Kleene coalgebra with tests~\cite{kozen_CoalgebraicTheoryKleene_2017}, where for each atom, the state can accept or  reject the atom (but not both), yet the state can also non-deterministically transition to multiple different state via the same atom, while executing different actions.
% TODO: I think this belongs in the intro
As we will see later, the deterministic behavior of GKAT coalgebra not only enables a further optimized symbolic algorithm than KCT~\cite{pous_SymbolicAlgorithmsLanguage_2015}, but also present challenges. Specifically, GKAT coalgebra requires normalization to compute finite trace equivalences~\cite{smolka_GuardedKleeneAlgebra_2020}, where we will remove all the state that cannot lead to acceptance. 
It seems like we need to traverse the entire automaton to identify these ``dead states'', however we have shown that these dead state detection can be invoked lazily, only when discrepancy between bisimulation are found.  

\subsection{Liveness and Sub-GKAT coalgebras}

% liveness

Traditionally, live and dead states are defined by whether they can reach an accepting state~\cite{smolka_GuardedKleeneAlgebra_2020}. However, recall that principle sub-coalgebra \(⟨s⟩_S\) models reachable states of \(s\) in coalgebra \(S\). Thus the classical definition is equivalent to the following:
\begin{definition}[liveness of states]\label{def:liveness-of-states}
    A state \(s\) is \emph{accepting} if there exists a \(α ∈ \At\) s.t. \(δ(s, α) = \accept\). A state \(s'\) is \emph{live} if there exists an accepting state \(s' ∈ ⟨s⟩\). A state \(s'\) is \emph{dead} if there is no accepting state in \(⟨s⟩\).
\end{definition}
This alternative liveness definition can help us formally prove important theorems regarding reachability and liveness without performing induction on traces. We can show the following lemmas as examples:
\begin{lemma}\label{thm:dead-iff-all-reachable-dead}
    A state \(s\) is dead if and only if all elements in \(⟨s⟩\) is dead.
\end{lemma}
\begin{proof}
    \(⟸\) direction is true, because \(s ∈ ⟨s⟩\): if all \(⟨s⟩\) is dead, then \(s\) is dead. 
    \(⟹\) direction can be proven as follows.
    Take \(s' ∈ ⟨s⟩\), then \(⟨s'⟩ ⊑ ⟨s⟩\) by definition. 
    Since there is no accepting state in \(⟨s⟩\), thus there cannot be any accepting state in \(⟨s'⟩\), hence \(⟨s'⟩\) is also dead.
\end{proof}

\begin{theorem}[homomorphism perserves liveness]\label{thm:hom-preserve-liveness}
    Given a homomorphism \(h: S → T\) and a state \(s ∈ S\):
    \[\text{\(s\) is live} ⟺ \text{\(h(s)\) is live}\]
\end{theorem}

\begin{proof}
    Because homomorphic image preserves principle sub-GKAT coalgebra (\cref{thm:homo-img-preserve-principle-sub-coalg})
    \[h(⟨s⟩_S) = ⟨h(s)⟩_T;\]
    therefore for any state \(s' ∈ S\):
    \[s' ∈ ⟨s⟩_S ⟺ h(s') ∈ h(⟨s⟩_S) ⟺ h(s') ∈ ⟨h(s)⟩_T.\]
    And because \(s'\) is accepting if and only if \(h(s')\) accepting by definition of homomorphism; then \(⟨s⟩_S\) contains an accepting state if and only if \(h(⟨s⟩_S) = ⟨h(s)⟩_T\) contains an accepting state. 
    Therefore \(s\) is live in \(S\) if and only if \(h(s)\) is live in \(T\).
\end{proof}

\begin{corollary}[sub-coalgebra perserves liveness]\label{thm:sub-coalg-preserve-liveness}
    Given a sub-coalgebra \(S' ⊑ S\), then for all states \(s ∈ S'\),
    \[\text{\(s\) is live in \(S'\)} ⟺ \text{\(s\) is live in \(S\)}.\]
\end{corollary}

\begin{proof}
    take the homomorphism \(h\) in \cref{thm:hom-preserve-liveness} to be the inclusion homomorphism \(i: S' → S\).
\end{proof}

\begin{corollary}[bisimulation preserves liveness]\label{thm:bisim-preserve-liveness}
    If there exists a bisimulation \(∼\) between GKAT coalgebra \(S\) and \(T\) s.t. \(s ∼ t\) for some states \(s ∈ S\) and \(t ∈ T\), then \(s\) and \(t\) has to be either both accepting, both live or both dead.
\end{corollary}

\begin{proof}
    Because for a \(∼\) is a bisimulation when both \(π₁: {∼} → S\) and \(π₂: {∼} → T\) are homomorphisms.
    Therefore, 
    \begin{align*}
        s \text{ is live in } S 
        & ⟺ π₁((s, t)) \text{ is live in } S 
            & π₁((s, t)) = s\\  
        & ⟺ (s, t) \text{ is live in } {∼} 
            & \text{\(π₁\) is a homomorphism}\\  
        & ⟺ π₂((s, t)) \text{ is live in } T 
            & \text{\(π₂\) is a homomorphism} \\
        & ⟺ t \text{ is live in } T 
            & π₂((s, t)) = t
    \end{align*}
\end{proof}


% The signature of GKAT coalgebra \(G\) is a simple polynomial functor~\cite[Definition 2.2.1]{jacobs_IntroductionCoalgebraMathematics_2016}, implying that it preserves the inclusion function:
% \begin{lemma}%used in the comments below
%     Given a inclusion \(i: S' → S\) for \(S' ⊆ S\), then \(G(i): G(S') → G(S)\) is also a inclusion in the sense that:
%     \[∀ m ∈ G(S'), G(i)(m) = m.\]
% \end{lemma}
% This implies that for a sub-GKAT coalgebra \(S' ⊑ S\), then the restriction of transition function will satisfy the the diagram for sub-coalgebra.
% \[
%     \begin{tikzcd}
%         S' \ar{d}{δ_S} \ar[hook]{r}{i} & S \ar{d}{δ_S} \\  
%         G(S') \ar[hook]{r}{G(i)} & G(S)
%     \end{tikzcd}
% \]
% Then by the uniqueness of transition for sub-coalgebra~\cite[Proposition 6.1]{rutten_UniversalCoalgebraTheory_2000}, the transition of \(S'\) is necessarily the restriction of \(δ_S\). A nice consequence of this observation is that the ordering between sub-GKAT coalgebra is exactly the subset ordering.
% \begin{lemma}[ordering in sub-GKAT coalgebra]\label{thm:sub-GKAT coalgebra-is-subset}
%     Given two sub-GKAT coalgebra \(S₁ ⊑ S\) and \(S₂ ⊑ S\), then 
%     \[S₁ ⊑ S₂ ⟺ S₁ ⊆ S₂.\]
% \end{lemma}

% \begin{proof}
%     \(⟹\) direction is true by definition, and \(⟸\) direction is true because both \(S₁\) and \(S₂\) are sub-GKAT coalgebra, thus their transition functions are both restrictions of \(δ_S\). Hence the commutativity of the following diagram can be verified by computation:
%     \[
%     \begin{tikzcd}
%         S₁ \ar[hook]{r}{i} \ar{d}{δ_S} & S₂ \ar[hook]{r}{i} \ar{d}{δ_S} & S \ar{d}{δ} \\  
%         G(S₁) \ar[hook]{r}{G(i)} & G(S₂) \ar[hook]{r}{G(i)} & G(S)
%     \end{tikzcd}
% \]
% \end{proof}

% trace semantics
\subsection{Normalization And Trace Semantics}

(Possibly infinite) trace model \(𝒢_ω\) is the natural semantics of states in a GKAT coalgebra, specifically it forms the final coalgebra of GKAT coalgebras~\cite{schmid_GuardedKleeneAlgebra_2021}.
The finality of the model means that every state in any GKAT coalgebra \(S\) can be assigned a semantics under the unique homomorphism \(⟦-⟧^{ω}_{S}: S → 𝒢_ω\); and such semantical equivalences can indeed be identified by bisimulation~\cite{schmid_GuardedKleeneAlgebra_2021}:
\[⟦s⟧^{ω}_{S} = ⟦t⟧^{ω}_{T} ⟺ \text{exists a bisimulation \({∼} ⊆ S × T\), s.t. \(s ∼ t\)}.\]

The infinite trace equivalences is relatively easy to compute efficiently, as bisimulation is, in general, compatible with derivative based computation on-the-fly algorithm~\cite{kozen_CoalgebraicTheoryKleene_2017,almeida_DecidingKATHoare_2012,pous_SymbolicAlgorithmsLanguage_2015}. 
However, the \emph{finite} trace model \(𝒢\) is only the final coalgebra of GKAT coalgebras without dead states, which we call \emph{normal GKAT coalgebra}~\cite{smolka_GuardedKleeneAlgebra_2020}. Fortunately every GKAT coalgebra can be normalized by rerouting all the transition from dead states to rejection
\begin{align*}
    \norm(δ_S) & : S → G(S) \\
    \norm(δ_S)(s, α) & ≜ \begin{cases}
        \reject & \text{if } δ_S(s, α) = (s', p) \text{ and \(s'\) is dead}\\
        δ_S(s, α) & \text{otherwise}
    \end{cases}
\end{align*}
We denote the normalized coalgebra \((S, \norm(δ_S))\) as \(\norm(S)\).
This means that the finite trace semantics \(⟦-⟧\) is the unique coalgebra homomorphism \(\norm(S) → \norm(𝒢)\). Hence the finite trace equivalence between \(s ∈ S\) and \(t ∈ T\) can be computed by first normalizing \(S\) and \(T\), then decide whether there is a bisimulation on \(\norm(S)\) and \(\norm(T)\) that includes \((s, t)\).
For a more intuitive account for the trace semantics, we refer the reader to the work of \Citeauthor{smolka_GuardedKleeneAlgebra_2020}~\cite{smolka_GuardedKleeneAlgebra_2020}.

Then by the finality of GKAT colagebra, the soundness and completeness of bisimulation is a simple corollary in universal coalgebra~\cite{smolka_GuardedKleeneAlgebra_2020,jacobs_IntroductionCoalgebraMathematics_2016,rutten_UniversalCoalgebraTheory_2000}. No matter through explicit construction~\cite{smolka_GuardedKleeneAlgebra_2020} or (weak) pullback~\cite{jacobs_IntroductionCoalgebraMathematics_2016,rutten_UniversalCoalgebraTheory_2000}, both completeness proof shows that the language equivalence: \[{≡} ≜ \{(s,t) ∣ ⟦s⟧_S = ⟦t⟧_T\}\] is indeed a bisimulation. 
This result, together with soundness result \(s ∼ t ⟹ ⟦s⟧_S = ⟦t⟧_T\) means that the language equivalence \(≡\) is indeed the largest bisimulation, which allows us to work with bisimulation equivalence instead of bisimulation.

\begin{definition}
    A \emph{bisimulation equivalence} in \(S\) is a bisimulation between \(S\) and itself, and it is also an equivalence relation.
\end{definition}

\begin{theorem}\label{thm:bisim-iff-bisim-equiv}
    Given two states \(s, t ∈ S\), then there exists a bisimulation \(∼\) s.t. \(s ∼ t\) if and only if there exists a bisimulation equivalence \(≃\) s.t. \(s ≃ t\).
\end{theorem}

\begin{proof}
    The \(⟹\) direction can just take \(≃\) to be the language equivalence \(≡\), which is a bisimulation equivalence, and because \(≡\) is maximal, therefore \({∼} ⊑ {≡}\), and \((s, t) ∈ {∼} ⊆ {≡}\).

    The \(⟸\) direction is true because all bisimulation is a bisimulation equivalence, thus we can take \(∼\) to just be the given bisimulation equivalence \(≃\).
\end{proof}

% Although \(\norm(S)\) might not be a homomorphic image of \(S\), normalization does satisfy many nice properties of homomorphic images. For example, it is monotonic and it preserves principle sub-GKAT coalgebra, like in \cref{thm:homo-img-preserve-principle-sub-coalg} 
% \begin{lemma}[monotonicity]\label{thm:monotonicity-norm}
%     For all GKAT coalgebra \(S\) and \(T\):
%     \[S ⊑ T ⟹ \norm(S) ⊑ \norm(T).\]
% \end{lemma}

% \begin{theorem}[\(\norm\) preserves principle sub coalgebra]\label{thm:norm-preserve-principle-sub-coalg}
%     Given a GKAT coalgebra \(S\) and a live state \(s ∈ S\):
%     \[⟨s⟩_{\norm(S)} = \norm(⟨s⟩_{S}).\]
% \end{theorem}

% \begin{proof}
%     We need to show \(\norm(⟨s⟩_{S})\) is the smallest sub-GKAT coalgebra in \(\norm(S)\) that contains \(s\). 
    
%     First, \(\norm(⟨s⟩_{S})\) contains \(s\) because \(s\) is live. 
%     Second, take any \(T ⊑ \norm(S)\), we will use \(⟨T⟩_S\) to denote the smallest sub-GKAT coalgebra in \(S\) that contains all the states in \(T\). 
%     Since \(T\) is a sub-GKAT coalgebra of \(\norm(S)\), \(⟨T⟩_S ∖ T\) can only contain dead states, thus \(\norm(⟨T⟩_S) = T\).
%     \begin{align*}
%         & ⟨s⟩_{S} ⊑ ⟨T⟩_S & \text{\(⟨T⟩_S\) contains \(s\)} \\  
%         ⟹ {}& \norm(⟨s⟩_{S}) ⊑ \norm(⟨T⟩_S) & \text{monotonicity: \cref{thm:monotonicity-norm}} \\  
%         ⟹ {}& \norm(⟨s⟩_{S}) ⊑ T & \norm(⟨T⟩_S) = T
%     \end{align*}
% \end{proof}


\section{On-The-Fly Bisimulation}

The original algorithm for deciding GKAT equivalences~\cite{smolka_GuardedKleeneAlgebra_2020} requires the entire automaton to be known prior to the execution of the bisimulation algorithm; specifically, in order to compute the liveness of a state \(s\), it is necessary iterate through all its reachable states \(⟨s⟩\) to see if there are any accepting states within.
This limitation poses challenges to design an efficient on-the-fly algorithm for GKAT.
In order to make the decision procedure scalable, we will need to merge the normalization and bisimulation procedure, so that our algorithm can normalized the automaton only when we need to.

In this section, we introduce an algorithm that merges bisimulation and normalization where we only need to test the liveness of the state when a disparity in the bisimulation has been found.
For example, when one automaton leads to reject where the other transition to a state, then we will need to verify whether that state is dead or not.

This on-the-fly algorithm inherits the efficiency of the original algorithm~\cite{smolka_GuardedKleeneAlgebra_2020}, where the worst case will require two passes of the automaton, where one pass will try to establish a bisimulation, when failed the other pass will kick in and compute whether the failed states are dead.
In some special case, the on-the-fly algorithm can even out perform the original algorithm; for example, when the two input automata are bisimular (even when they are not normal), the on-the-fly algorithm can skip the liveness checking, only performing the bisimulation.



\begin{theorem}[sub-coalgebra perserve bisimulation]\label{thm:sub-coalg-preserve-bisim}
    Given any sub-coalgebra \(S' ⊑ S\) and \(T' ⊑ T\),
    \begin{itemize}
        \item Given a bisimulation \(∼\) between \(S'\) and \(T'\), then \(∼\) is also a bisimulation between \(S\) and \(T\);
        \item if there exists a bisimulation \(∼\) between \(S\) and \(T\), then the restriction 
        \begin{mathpar}
            ∼_{S', T'} ≜ \{(s, t) ∣ s ∈ S', t ∈ T', s ∼ t\}
        \end{mathpar}
        forms a bisimulation between \(S'\) and \(T'\).
    \end{itemize}
\end{theorem}

\begin{proof}
    To prove that bisimulation \(∼\) between \(S'\) and \(T'\) is also a bisimulation of \(S\) and \(T\), we can simply enlarge the diagram by the inclusion homomorphism
    \[
        \begin{tikzcd}
            S \ar{d}{δ_S} & S' \ar[hook',swap]{l}{i} \ar{d}{δ_S}
            & ∼ \ar[swap]{l}{π₁} \ar{r}{π₂} \ar{d}{δ_∼}
            & T' \ar[hook]{r}{i} \ar{d}{δ_S} & T \ar{d}{δ_T}\\  
            G(S) & G(S') \ar[hook',swap]{l}{G(i)} 
            & G(∼) \ar[swap]{l}{G(π₁)} \ar{r}{G(π₂)} & T' \ar[hook]{r}{G(i)} & T \\  
        \end{tikzcd}
    \]
    Because the inclusion homomorphism \(i\) doesn't change the input thus, we have:
    \begin{mathpar}
        {∼} \xrightarrow{π₁} S' \xrightarrow{i} S = {∼} \xrightarrow{π₁} S \and 
        {∼} \xrightarrow{π₂} T' \xrightarrow{i} T = {∼} \xrightarrow{π₂} T
    \end{mathpar}

    To prove that the bisimulation can be restricted, we first realize that \(∼_{S', T'}\) is a pre-image of the maximal bisimulation \(≣_{S', T'}\) along the inclusion homomorphism \(i: {∼} → {≡_{S, T}}\).
    This means that \(∼_{S', T'}\) can be formed by a pullback square:
    \[
        \begin{tikzcd}
            ∼_{S', T'} \ar{r}{i} \ar[swap]{d}{i} \ar[phantom, very near start]{dr}{\scalebox{1.5}{\(\lrcorner\)}} & ≣_{S', T'} \ar{d}{i}\\ 
            {∼} \ar[swap]{r}{i} & {≡_{S, T}}
        \end{tikzcd}
    \]
    Recall that elementary polynomial functor~\cite{jacobs_IntroductionCoalgebraMathematics_2016} like \(G\) preserves pullback, hence the pullback also uniquely generates a GKAT coalgebra~\cite{rutten_UniversalCoalgebraTheory_2000}
\end{proof}

\begin{lemma}[bisimulation between dead states]\label{thm:bisim-between-dead}
    Given two dead states \(s ∈ S\) and \(t ∈ T\), then the singleton bisimulation 
    \begin{mathpar}
        {∼} ≜ \{(s,t)\} \and 
        δ_{∼}((s,t), α) ≜ \reject
    \end{mathpar}
    is a bisimulation between \(S\) and \(T\).
\end{lemma}

\begin{proof}
    By computation
\end{proof}

\begin{theorem}[inductive construction]\label{thm:inductive-construction}
    Given two GKAT coalgebra \(S\) and \(T\), and two of their elements \(s ∈ S\) and \(t ∈ T\),
    there exists a bisimulation \({∼} ⊆ ⟨s⟩ × ⟨t⟩\) s.t. \(s ∼ t\), if and only if all of the following holds:
    \begin{enumerate}
        \item\label{itm:acc-condition} for all \(α ∈ \At\), \(δ_{S}(s, α) = \accept ⟺ δ_{T}(t, α) = \accept\);
        \item\label{itm:transition-bisim} If \(δ_{S}(s, α) = (s', p)\) and \(δ_{T}(t, α) = (t', p)\), then there exists a bisimulation \({∼_{s',t'}}\) on \(⟨s'⟩\) and \(⟨t'⟩\), s.t. \(s' ∼_{s',t'} t'\);
        \item\label{itm:transition-dead} If \(δ_{S}(s, α) = (s', p)\) and \(δ_{T}(t, α) = (t', q)\), s.t. \(p ≠ q\), then both \(s'\) and \(t'\) are dead;
        \item\label{itm:rej-or-dead} \(s\) reject \(α\) or transition to a dead state via \(α\) if and only if \(t\) rejects \(α\) or transition to a dead state via \(α\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    We first prove \(⟹\) direction, recall the definition of bisimulation:
    \[
        \begin{tikzcd}
            S \ar[swap]{d}{\norm(δ_S)} 
            & ∼ \ar{d}{δ_∼} \ar[swap]{l}{π₁} \ar{r}{π₂}
            & T \ar{d}{\norm(δ_T)}\\  
            G(S) 
            & G(∼) \ar{l}{G(π₁)} \ar[swap]{r}{G(π₂)} 
            & G(T) 
        \end{tikzcd}
    \]

    The condition \labelcref{itm:acc-condition} holds:
    \begin{align*}
        & δ_S(s, α) = \accept \\
        ⟺ & \norm(δ_S)(s, α) = \accept \\
        ⟺ & \norm(δ_∼)((s,t), α) = \accept \\ 
        ⟺ & \norm(δ_T)(t, α) = \accept \\
        ⟺ & δ_T(t, α) = \accept
    \end{align*}

    The condition~\labelcref{itm:transition-bisim} holds, by case analysis on the liveness of \(s'\) and \(t'\).
    First note that \(s'\) and \(t'\) has to be both live or both dead: because \(δ_S(s, α) = (s', p)\), then \(\norm(δ_S)(s', α)\) can either be rejection or \((s',p)\), and so is \(\norm(δ_T)(t', α)\). Finally because \(s ∼ t\), then
    \begin{align*}
        s' \text{ is live} 
        & ⟺ \norm(δ_S)(s, α) = (s', p) \\
        & ⟺ \norm(δ_T)(t, α) = (t', p) \\
        & ⟺ t' \text{ is live}.
    \end{align*}
    \begin{itemize}
        \item If both \(s'\) and \(t'\) are live, then \(s' ∼ t'\). By~\cref{thm:sub-coalg-preserve-bisim}, the bisimulation \(∼_{s', t'}\) is just \(∼\) restricted to \(⟨s'⟩\) and \(⟨t'⟩\).
        \item If both \(s'\) and \(t'\) are dead, then \(∼_{s', t'}\) can just be the singleton relation, according to~\cref{thm:bisim-between-dead}.
    \end{itemize}

    The condition~\labelcref{itm:transition-dead} holds: by the proof of condition~\labelcref{itm:transition-bisim}, \(s'\) and \(t'\) has to be either both live or both dead; if they are both live, then there cannot be a element in \(G(∼)\) that can project to \((s', p)\) under \(π₁\) but projects to \((t', q)\) under \(π₂\). Thus both \(s'\) and \(t'\) has to be dead.

    The condition~\labelcref{itm:rej-or-dead} holds: 
    \begin{align*}
        & δ_S(s, α) \text{ rejects or transition to dead states} \\
        ⟺ & \norm(δ_S)(s, α) = \reject \\
        ⟺ & \norm(δ_T)(t, α) = \reject \\
        ⟺ & δ_T(t, α) \text{ rejects or transition to dead states}.
    \end{align*}

    We then show the \(⟸\) direction, we use \(≡_{s', t'}\) to denote the maximal bisimulation between \(⟨s'⟩\) and \(⟨t'⟩\).
    \[{∼'} ≜ ⋃ \{≡_{s', t'} ∣ ∃ α ∈ \At, p ∈ K, δ_S(s, α) = (s', p) \text{ and } δ_T(t, α) = (t', p)\}.\]
    Notice because \(⟨s'⟩ ⊑ ⟨s⟩\) and \(⟨t'⟩ ⊑ ⟨t⟩\), then \(≡_{s', t'}\) is a bisimulation between \(⟨s⟩\) and \(⟨t⟩\), and because bisimulation is closed under arbitrary union~\cite{rutten_UniversalCoalgebraTheory_2000}, then \(∼'\) is a bisimulation between \(⟨s⟩\) and \(⟨t⟩\).

    We then augment \(∼'\) with \((s, t)\) to obtain the bisimulation we required:
    \begin{mathpar}
        {∼} ≜ {∼'} ∪ \{(s, t)\} \and 
        δ_{∼}((s₁, t₁), α) ≜ \begin{cases}
            δ_{∼'}((s,t), α) & (s,t) ≠ (s₁, t₁) \\  
            \accept & \norm(δ_S)(s, α) = \norm(δ_T)(s, α) = \accept \\  
            \reject & \norm(δ_S)(s, α) = \norm(δ_T)(s, α) = \reject \\  
            ((s₂, t₂), p) & \norm(δ_S)(s, α) = (s₂, p) \text{ and } \norm(δ_T)(s, α) = (t₂, p) \\  
        \end{cases}
    \end{mathpar}
    The above definition of \(δ_{∼}\) is indeed well-defined, by case analysis on the result of \(δ_S\) and \(δ_T\) using the condition above:
    \begin{itemize}
        \item If \(δ_S(s, α) = \accept\), then by condition~\labelcref{itm:acc-condition}, \(δ_T(t, α) = \accept\) therefore \[\norm(δ_S)(s, α) = \norm(δ_T)(s, α) = \accept.\]
        \item If \(δ_S(s, α)\) transitions to a dead state or reject, then by conditions~\labelcref{itm:rej-or-dead} \(δ_T(t, α)\) will also transition to a dead state or reject, then \[norm(δ_S)(s, α) = \norm(δ_T)(s, α) = \reject.\]
        \item If \(δ_S(s, α) = (s', p)\), then by condition~\labelcref{itm:acc-condition} and condition~\labelcref{itm:rej-or-dead}, \(δ_T(t, α) = (t', q)\). By the contrapositive of condition~\labelcref{itm:transition-dead}, if either \(s, t\) are live, then \(p = q\).

        Then by condition~\labelcref{itm:transition-bisim}, there exists a bisimulation \(∼_{s', t'}\) between \(⟨s'⟩\) and \(⟨t'⟩\) s.t. \(s' ∼_{s', t'} t'\). Because bisimulation preserves liveness (\cref{thm:bisim-preserve-liveness}), \(s', t'\) has to be both dead or live, the both dead case is handled by the previous item, both live case will give us the case we desired:
        \[\norm(δ_S)(s, α) = (s', p) \text{ and } \norm(δ_T)(t, α) = (t', p)\]
    \end{itemize}
    And the diagram bisimulation needing to satisfy can be verified by unfolding the definition.
\end{proof}

The above theorem already gives us a way to recursively construct a algorithm that include \(s ∼ t\), this consequently will let us decide the trace equivalence of \(s\) and \(t\): \(⟦s⟧ = ⟦t⟧\).
However, this algorithm can be further optimized, we will then derive that a dead state can never relate to live states. This means that when checking the bisimulation of states \(s\) and \(t\), if we already know one of them is dead, we only need to check whether the other is dead, instead of going through the convoluted process mentioned in~\cref{thm:inductive-construction}.

However because homomorphism preserves liveness, if we already know one of the \(s\) and \(t\) is dead, the other has to be dead.

\begin{theorem}\label{thm:bisim-one-dead}
    Given two states \(s ∈ S\) and \(t ∈ T\), if \(s\) is a dead state in \(S\), then there exists a bisimulation \(∼\) between \(S\) and \(T\) where \(s ∼ t\) if and only if \(t\) is dead. Similarly for \(t ∈ T\).
\end{theorem}

\begin{proof}
    if there exists a bisimulation \(∼\), s.t. \(s ∼ t\), because \(s\) is dead and bisimulation preserves liveness~\cref{thm:bisim-preserve-liveness}, then \(t\) is dead. 

    And if both \(t\) and \(s\) is dead, then a bisimulation can by constructed by~\cref{thm:bisim-between-dead}.
\end{proof}

\section{The Algorithm}

In this section we will present the pseudo-code for our on-the-fly algorithm. 
In order to implement the the inductive construction theorem (\cref{thm:inductive-construction}), we will need to determine the liveness of the state. This can be simply computed via a DFS from the state being checked. 

TODO: we should merge the two so that it is easier to 
\begin{algorithm}
    \caption{Check whether a state \(s\) is dead}\label{alg:check-dead-main}
    \begin{algorithmic}
        \Function{isDeadLoop}{$s ∈ S$, explored}
        \If {\(s ∈\) explored} {\Return explored} 
        \Else { 
            \For{\(α ∈ \At\)}{}{
                \Match{\(δ_{S}(s, α)\)}
                \Case{\(\accept\)} {\Return none} \Comment{\(s\) transition to accept}
                \EndCase
                \Case{\(\reject\)} {\texttt{continue}}
                \Comment{skip if \(s\) transition to reject}
                \EndCase
                \Case{($s', p$)}{ 
                    \If{\Call{IsDeadLoop}{$s'$} = none} {\Return none} \Comment{\(s\) transitions to a live state \(s'\)}
                    \Else { explored \(←\) (explored \(∪\) \Call{isDeadLoop}{$s'$, explored}) } \EndIf
                } \EndCase
                \EndMatch
            }\EndFor}
        \EndIf
        \State {\Return explored}    
        \EndFunction
    \end{algorithmic}
\end{algorithm}

By~\cref{thm:dead-iff-all-reachable-dead}, if \(s\) is dead then all the reachable states of \(s\) (denoted by \(⟨s⟩\)). Then by returning all the reachable states of \(s\), we can cache these states to avoid checking them again. To encapsulate the caching, we have the following function, which we will actually use in our bisimulation algorithm.

\begin{algorithm}
    \caption{A cached algorithm to check whether a state is dead}\label{alg:is-dead}
    \begin{algorithmic}
        \State{deadStates \(← ∅\)}

        \Function{isDead}{$s ∈ S$}
        \If {\(s ∈\) deadStates} {\Return true} 
        \ElsIf {\Call{isDeadLoop}{$s, ∅$} = none} {\Return false}
        \Else 
            \State {deadStates \(←\) (deadStates \(∪\) \Call{isDeadLoop}{$s, ∅$})}
            \State {\Return {true}}
        \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm}

Given the direct correspondence between bisimulation and bisimulation equivalence and bisimulation in sub-algebra:
\begin{align*}
    & ∃ \text{ bisimulation } {∼} ⊆ ⟨s⟩ × ⟨t⟩ \text{ s.t. } s ∼ t \\
    & ⟺ ∃ \text{ bisimulation } {∼} ⊆ (⟨s⟩∪⟨t⟩) × (⟨s⟩∪⟨t⟩) \text{ s.t. } s ∼ t 
        & \text{\cref{thm:sub-coalg-preserve-bisim}}\\  
    & ⟺ ∃ \text{ bisimulation equivalence } {≃} ⊆ (⟨s⟩∪⟨t⟩) × (⟨s⟩∪⟨t⟩) \text{ s.t. } s ≃ t 
        & \text{\cref{thm:bisim-iff-bisim-equiv}}
\end{align*}
we can safely replace the bisimulation in inductive construction (\cref{thm:inductive-construction}) with bisimulation equivalence. 
Dealing with equivalence relations allows us to leverage efficient data structures like union find in our bisimulation algorithm. 

We will use \(\Call{union}{$s,t$}\) to denote the operation to equate \(s\) and \(t\) in a union-find, and use \(\Call{eq}{$s,t$}\) to check if \(s\) and \(t\) belongs to the same equivalence class, i.e. share the same representative.
Specifically, we will use the union-find structures to keep track of the equivalence classes that we are in the process of checking, hence avoiding repeatedly checking the same pair of states to remove infinite loops.

Our on-the-fly bisimulation algorithm will decide whether there exists a bisimulation relation in \(⟨s⟩ ∪ ⟨t⟩\) s.t. \(s ∼ t\). This algorithm generally reproduce the setting of inductive construction theorem~\cref{thm:inductive-construction};
except by~\cref{thm:bisim-one-dead}, in the special case where \(s\) or \(t\) is dead, then we will only need to check whether the other is dead.

\begin{algorithm}
    \caption{On-the-fly bisimulation algorithm}\label{alg:bisim}
    \begin{algorithmic}
        \Function{equiv}{$s ∈ S, t ∈ T$}
        \If {\Call{eq}{$s, t$}} {\Return true}
        \ElsIf {\(s ∈\) deadStates\(_S\)} {\Return \Call{isDead\(_T\)}{$t$}} 
        \ElsIf {\(t ∈\) deadStates\(_T\)} {\Return \Call{isDead\(_S\)}{$s$}} 
        \Else 
        \For{\(α ∈ \At\)}{}{
            \Comment{Inductive construction,~\cref{thm:inductive-construction}}
            \Match{$δ_{S}(s, α), δ_{T}(t, α)$}
            \Case{\(\accept, \accept\)} {\Skip} \EndCase
            \Case{\(\reject, \reject\)} {\Skip} \EndCase
            \Case{\(\reject, (t', q)\)} {\Call{isDead}{$t'$}} \EndCase
            \Case{\((s', p), \reject\)} {\Call{isDead}{$s'$}} \EndCase
            \Case{\((s', p), (t', q)\)} {
                \If {\(p = q\)} {\Call{union}{$s, t$}; \Call{equiv}{$s, t$}} 
                \ElsIf {\Call{isDead}{$s$} and \Call{isDead}{$s$}} {\Skip}
                \Else { \Return false }
                \EndIf
            } \EndCase
            \Default {\Return false} \Comment{the results format does not match} \EndDefault
            \EndMatch
        }\EndFor
        \EndIf
        \Return true \Comment{no mismatch found}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

Because the dead state detection algorithm is coalgebra-specific, we use a subscript on ``deadStates'' and ``\textsc{IsDead}'' to indicate the coalgebra. 
The soundness and completeness of~\cref{alg:bisim} can be observed by the fact that \emph{when the algorithm terminate}, the algorithm returns true if and only if there exists a bisimulation between \(⟨s⟩\) and \(⟨t⟩\) s.t. \(s ∼ t\), which is then logically equivalent to trace equivalence.
Such equivalence is a direct consequence of~\cref{thm:bisim-one-dead,thm:inductive-construction}.

\begin{remark}
    The caching of dead state and the shortcut to check whether \(s\) is dead when \(t\) is dead and vise versa, is not essential to the soundness and completeness of algorithm, they are here to trade speed with memory. 
    In a memory-constraint situation, the ``\textnormal{deadStates}'' variable can be cleared periodically to save memory.
\end{remark}

\section{Symbolic Algorithm}

Given the alphabet \(K, B\), a \emph{symbolic GKAT coalgebra} \(Ŝ ≜ ⟨S, ϵ̂, δ̂⟩\) consists of a state set \(S\) and a accepting function \(ϵ̂\) and a transition function \(δ̂\):
\begin{mathpar}
    ϵ̂: S → 𝒫(\Bool_B), \and
    δ̂: S → 𝒫(\Bool_B × S × K),
\end{mathpar}
where \(\Bool_B\) is the free boolean algebra over \(B\) (boolean expressions modulo boolean algebra axioms); for all states \(s ∈ S\), all the booleans are ``disjoint''; namely the conjunction of any two expression from the set \(\{ϵ̂(s)\} ∪ \{b ∣ ∃ (b, s', p) ∈ δ(s)\}\) are false. 
We will then use \(ρ̂(s): \Bool_B\) to denote the boolean expressions that contain all the atoms that the state \(s\) rejects, and \(ρ̂(s)\) can be computed as follows:
\[ρ̂(s) ≜ ¬ ϵ̂(s) ∨ ¬ \left( ⋁_{(b, s', p) ∈ δ(s)} b \right)\]

Instead of modeling each atom individually in the automata, we group them into boolean expressions, this leads to a much more space efficient automata, and enables efficient bisimulation algorithms using off-the-shelf SAT solvers.

With the above intuition in mind, a symbolic GKAT coalgebra \(Ŝ ≜ ⟨S, ϵ̂, δ̂⟩\) can be lowered into a GKAT coalgebra \(⟨S, δ⟩\) in the following manner:
\begin{align}\label{cons:lowering}
δ(s, α) ≜ \begin{cases}
    \accept & ∃ b ∈ ϵ̂(s), α ≤ b \\  
    (s', p) & ∃ b ∈ \Bool_B, α ≤ b \text{ and } δ(s, b) = (s', p) \\  
    \reject & \text{otherwise}
\end{cases}
\end{align}
This is well-defined, i.e. no more than one clause can be satisfied precisely because the boolean expressions appear in \(ϵ̂\) and \(δ̂\) are disjoint.
The trace semantics of a GKAT coalgebra \(⟨S, ϵ̂, δ̂⟩\) is then defined as the trace semantics of its lowering \(⟨S, δ⟩\).

\begin{remark}[Canonicity]
    Notice that symbolic GKAT coalgebra is not canonical, i.e. there exists two different symbolic GKAT colagebra with the same lowering, consider the state set \(S ≜ \{*\}\):
    \[{δ̂}₁(*) ≜ \{b ↦ (*, p), ¬ b ↦ (*, p)\} \qquad 
    {δ̂}₂(*) ≜ \{⊤ ↦ (*, p)\},\] 
    and both \(ϵ̂\) will return constant \(⊥\).
    These two symbolic GKAT coalgebra obviously have the same lowering hence behavior, yet, they are different.
    There are other symbolic representation that will satisfy canonicity, yet we opt to use our current representation for ease of construction and computational efficiency.
\end{remark}

\begin{theorem}[Functoriality]
    The lowering operation is a functor, given a symbolic GKAT coalgebra homomorphism \(h: Ŝ → Û\), then \(h\) is also a homomorphism \(h: S → U\).
\end{theorem}

\begin{proof}
    
\end{proof}

We can then migrate the normalized bisimulation algorithm to the symbolic setting, we will first prove an inductive construction theorem like~\cref{thm:inductive-construction}.

\begin{theorem}[Symbolic Inductive Construction]\label{thm:symb-inductive-construction}
    Given two symbolic GKAT coalgebra \(Ŝ = ⟨S, ϵ̂_S, δ̂_S⟩\) and \(T̂ = ⟨T, ϵ̂_T, δ̂_T⟩\) and two states \(s ∈ S\) and \(t ∈ T\), there exists a normalized bisimulation on the lowered coalgebra \({∼} ⊆ S × T\) s.t. \(s ∼ t\) if and only if all the following holds:
    \begin{itemize}
        \item \(⋁ ϵ̂_S(s) ≡ ⋁ ϵ̂_T(t)\);
        \item for all \((b, s', p) ∈ δ̂_S(s)\) and \((c, t', q) ∈ δ̂_T(t)\), if \(b ∧ c ≢ 0\) and \(p = q\) then here exists a normalized bisimulation \({∼_{s',t'}} ⊆ S × T\) s.t. \(s' ∼_{s',t'} t'\);
        \item for all \((b, s', p) ∈ δ̂_S(s)\) and \((c, t', q) ∈ δ̂_T(t)\), if \(b ∧ c ≢ 0\) and \(p ≠ q\) then both \(s'\) and \(t'\) is dead;  
        \item for all \((b, s', p) ∈ δ̂_S(s)\) and \(c ∈ ρ̂_T(t)\), if \(b ∧ c ≢ 0\), then \(s'\) is dead;
        \item for all \(b ∈ ρ̂_S(s)\) and \((c, t', q) ∈ δ̂_T(t)\), if \(b ∧ c ≢ 0\), then \(t'\) is dead;
    \end{itemize}
\end{theorem}

\begin{proof}
    Reduces to~\cref{thm:inductive-construction} i.e. all the above condition holds if and only if all the condition in~\cref{thm:inductive-construction} holds in the lowered coalgebra.
\end{proof}

Then for the algorithm, we can just recursively check all the conditions in~\cref{thm:symb-inductive-construction}.

\begin{algorithm*}
    \caption{On-the-fly bisimulation algorithm}\label{alg:symb-bisim}
    \begin{algorithmic}
        \Function{equiv}{$s ∈ S, t ∈ T$}
        \If {\Call{eq}{$s, t$}} {\Return true}
        \ElsIf {\(s ∈\) deadStates\(_S\)} {\Return \Call{isDead\(_T\)}{$t$}} 
        \ElsIf {\(t ∈\) deadStates\(_T\)} {\Return \Call{isDead\(_S\)}{$s$}} 
        \Else {}
        \Return {
            \Comment{conditions of ~\cref{thm:symb-inductive-construction}}
            \\\vspace{5px}
            \(\qquad
            \begin{aligned}
                & ⋁ ϵ̂_S(s) ≡ ⋁ ϵ̂_T(t) \mathrel{\&\!\&} \\  
                & \forall (b, s', p) ∈ δ̂_S(s), (c, t', q) ∈ δ̂_T(t), (b ∧ c) ≢ ⊥ ⟹ 
                \begin{cases}
                    \text{\Call{IsDead$_S$}{$s$}} ∧ \text{\Call{IsDead$_T$}{$t$}} & \text{if \(p ≠ q\)} \\
                    \text{\Call{Union}{$s$, $t$}}; \text{\Call{Equiv}{$s', t'$}} & \text{if \(p = q\)}
                \end{cases} \mathrel{\&\!\&}\\
                & \forall (b, s', p) ∈ δ̂_S(s), c ∈ ρ̂_T(t), (b ∧ c) ≢ ⊥ ⟹ \text{\Call{IsDead$_S$}{$s'$}} \mathrel{\&\!\&}\\
                & \forall b ∈ ρ̂_S(s), (c, t', q) ∈ δ̂_T(t), (b ∧ c) ≢ ⊥ ⟹ \text{\Call{IsDead$_T$}{$t'$}}
            \end{aligned}\)
        }
        \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm*}

Inspired by the syntax of Ocaml, the \(\mathrel{\&\!\&}\) is the logical-and operator on the language level, specifying that all four conditions in the return statements must be satisfied to return true. 
Notice just like the non-symbolic case, this algorithm can be modified to perform symbolic bisimulation of (non-normalized) GKAT automaton, which coincides with infinite trace equivalence~\cite{schmid_GuardedKleeneAlgebra_2021}, by letting \textsc{IsDead} always return false and keep deadStates empty.


\section{Construction of Symbolic GKAT Automata}

The final piece of the puzzle is to convert any given expression into an ``equivalent'' Symbolic GKAT Automata. This goal can be achieved by lifting existent constructions like derivatives and Thompson's construction~\cite{schmid_GuardedKleeneAlgebra_2021,smolka_GuardedKleeneAlgebra_2020}.
The correctness of these conversions is a consequence of  correctness of their non-symbolic counter-part, i.e. we will prove that the lowering as shown in~\labelcref{cons:lowering} of these constructions will yield the conventional derivative and Thompson's construction. 

\begin{figure*}
    \begin{mathpar}
        \inferrule[]{\\}
        {p \transvia{1 ∣ p}_{D̂} 1} \and  
        \inferrule[]{\\}
        {b \transAcc{D̂}{b}} \and  
        \inferrule[]
        {e \transvia{c ∣ p}_{D̂} e'}
        {e +_b f \transvia{b ∧ c ∣ p}_{D̂} e'} 
        \and
        \inferrule[]
        {e \transAcc{D̂}{c}}
        {e +_b f \transAcc{D̂}{b ∧ c}}
        \and
        \inferrule[]
        {f \transvia{c ∣ p}_{D̂} f'}
        {e +_b f \transvia{\overline{b} ∧ c ∣ p}_{D̂} f'}
        \and
        \inferrule[]
        {f \transAcc{D̂}{c}}
        {e +_b f \transAcc{D̂}{\overline{b} ∧ c}}
        \and
        \inferrule[]
        {e \transAcc{D̂}{b} \\ f \transAcc{D̂}{c}}
        {e; f \transAcc{D̂}{b ∧ c}}
        \and 
        \inferrule[]
        {e \transvia{b ∣ p}_{D̂} e'}
        {e; f \transvia{b ∣ p}_{D̂} e'; f}
        \and
        \inferrule[]
        {e \transAcc{D̂}{b} \\ f \transvia{c ∣ p}_{D̂} f'}
        {e; f \transvia{b ∧ c ∣ p}_{D̂} f'}
        \and  
        \inferrule[]
        {\\}
        {e^{(b)} \transAcc{D̂}{\overline{b}}}  
        \and  
        \inferrule[]
        {e \transvia{c ∣ p} e'}
        {e^{(b)} \transvia{b ∧ c ∣ p}_{D̂} e'; e^{(b)}}
    \end{mathpar}
    \caption{Symbolic Derivative of GKAT Automata.}\label{fig:derivatives-rules}
\end{figure*}

The symbolic derivative coalgebra \(D̂\), with expressions as states, is the least symbolic GKAT coalgebra (ordered by point-wise subset ordering on \(ϵ̂\) and \(δ̂\)) that satisfy the rules in~\Cref{fig:derivatives-rules}.
Notice that the rules listed on~\Cref{fig:derivatives-rules} is very close to that of Schmid et al.~\cite{schmid_GuardedKleeneAlgebra_2021}.
This is no coincidence, as our definition exactly lowers to the definition of theirs.
This fact can be proven by case analysis on the shape of the source expression, and forms a basis on our correctness argument.

\begin{theorem}[Correctness]
    The lowering of \(D̂\) is exactly the derivative defined by Schmid et. al.~\cite{schmid_GuardedKleeneAlgebra_2021}.
    TODO: unfold the statement.
\end{theorem}

% \begin{table*}
%     \centering
%     \begin{tabular}{c||c|c|c|c|c|c}
%         Exp & \(S\) 
%         & \(ϵ^*\) & \(δ^*\) 
%         & \(ϵ̂(s)\) & \(δ̂(s)\) \\\hline
%         \(b\) & \(∅\) 
%         & \{b\} & \(∅\) 
%         & N/A & N/A \\  
%         \(p\) & \(\{*\}\) 
%         & \(∅\) & \(\{(1, *, p)\}\) 
%         & \(\{1\}\) & \(∅\)\\  
%         \(e₁ +_b e₂\) & \(S₁ + S₂\) &
%         \(⟨\{b\}|~ϵ₁^* ∪ ⟨\{\overline{b}\}|~ϵ₂^*\) &
%         \(⟨\{b\}|~δ₁^* + ⟨\{\overline{b}\}|~δ₂^*\) & 
%         \(\begin{cases}
%             ϵ̂₁(s) & s ∈ S₁\\
%             ϵ̂₂(s) & s ∈ S₂\\
%         \end{cases}\) & 
%         \(\begin{cases}
%             δ̂₁(s) & s ∈ S₁\\
%             δ̂₂(s) & s ∈ S₂\\
%         \end{cases}\) \\  
%         \(e₁ ; e₂\) & \(S₁ + S₂\) & 
%         \(⟨ϵ₁^*|~ϵ₂^*\) & \(δ₁^* + ⟨ϵ₁^*|~δ₂^*\) & 
%         \(\begin{cases}
%             ⟨ϵ̂₁(s)| ~ ϵ₂^*& s ∈ S₁ \\  
%             ϵ̂₂(s) & s ∈ S₂
%         \end{cases}\)& 
%         \(\begin{cases}
%             δ̂₁(s) ∪ ⟨ϵ̂(s)|~δ₂^* & s ∈ S₁ \\  
%             δ̂2(s) & s ∈ S₂
%         \end{cases}\) \\  
%         \(e₁^{(b)}\) & \(S₁\) & 
%         \(\{\overline{b}\}\) & \(⟨\{b\}|~δ₁^*\) & 
%         \(⟨\{\overline{b}\}|~ϵ̂1(s)\) & 
%         \( ⟨\{b\}|⟨ϵ̂1(s)|~δ₁^* ∪ δ₁(s)\)
%     \end{tabular}
%     \caption{Symbolic Thompson's Construction}\label{tab:symb-thomposon-construction}
% \end{table*}


\begin{table*}
    \centering
    \begin{tabular}{c||c|c|l|l}
        Exp & \(S\) & \(s^*\)  
        & \(ϵ̂(s)\) & \(δ̂(s)\) \\\hline
        \(b\) & \(\{s^*\}\) 
        & \(s^*\)
        & \(\{b\}\) & \(∅\) \\  
        \(p\) & \(\{s^*, s₁\}\) 
        & \(s^*\)
        & \(\begin{cases}
           ∅ & s = s^* \\  
           \{1\} & s = s₁ 
        \end{cases}\) 
        & \(\begin{cases}
            \{(1, s₁, 0)\} & s = s^* \\  
            ∅ & s = s₁
        \end{cases}\)\\  
        \(e₁ +_b e₂\) & \(\{s^*\} + S₁ + S₂\) &
        \(s^*\) &
        \(\begin{cases}
            ⟨\{b\}| ϵ̂₁(s₁^*) ∪ ⟨\{b\}| ϵ̂₂(s₂^*) & s = s^* \\
            ϵ̂₁(s) & s ∈ S₁\\
            ϵ̂₂(s) & s ∈ S₂\\
        \end{cases}\) & 
        \(\begin{cases}
            ⟨\{b\}| δ̂₁(s₁^*) + ⟨\{b\}| δ̂₂(s₂^*) & s = s^* \\
            δ̂₁(s) & s ∈ S₁\\
            δ̂₂(s) & s ∈ S₂\\
        \end{cases}\) \\  
        \(e₁ ; e₂\) & \(S₁ + S₂\) & 
        \(s₁^*\) & 
        \(\begin{cases}
            ⟨ϵ̂₁(s)| ϵ̂₂(s₂^*)& s ∈ S₁ \\  
            ϵ̂₂(s) & s ∈ S₂
        \end{cases}\)& 
        \(\begin{cases}
            δ̂₁(s) + ⟨ϵ̂(s)| δ̂₂(s₂^*) & s ∈ S₁ \\  
            δ̂2(s) & s ∈ S₂
        \end{cases}\) \\  
        \(e₁^{(b)}\) & \(\{s^*\} + S₁\) & 
        \(s^*\) &
        \(\begin{cases}
            \{\overline{b}\} & s = s^*\\
            ⟨\{\overline{b}\}| ϵ̂1(s) & s ∈ S₁
        \end{cases}\) & 
        \(\begin{cases}
            ⟨\{b\}| δ̂₁(s₁^*) & s = s^* \\
            δ₁(s) ∪ ⟨\{b\}|⟨ϵ̂1(s)| δ̂₁(s₁^*) & s ∈ S₁
        \end{cases}\)
    \end{tabular}
    \caption{Symbolic Thompson's Construction}\label{tab:symb-thomposon-construction}
\end{table*}

Another way to construct an automaton is via Thompson's construction, we lift the original construction to the symbolic setting.
A common expression to construct is a guard operation, denoted by \(⟨B|\), where \(B\) is a set of boolean expressions.
TODO: define transition dynamics and accepting dynamics earlier.
Concretely, this guard can be defined on both accepting dynamics and transition dynamics:
\begin{align*}
    ⟨B|ϵ̂(s) & ≜ \{b ∧ c ∣ b ∈ B, c ∈ ϵ(s)\}; \\
    ⟨B|δ̂(s) & ≜ \{(b ∧ c, s', p) ∣ b ∈ B, (c, s', p) ∈ δ(s)\}.
\end{align*}
Notably, besides guarding transition and acceptance with different conditions, like in if statements, the guard expression can be used to simulate uniform continuation. Specifically, we can use \(⟨ϵ̂(s)|~δ(s)\) to connecting all the accepting state of \(s\) to the dynamic \(δ(s)\).

With these definitions in mind, we can define symbolic Thompson's construction inductively as in~\Cref{tab:symb-thomposon-construction}, where we let \((S₁, ϵ̂₁, δ̂₁)\) and \((S₂, ϵ̂₂, δ̂₂)\) to be result of Thompson's construction for \(e₁\) and \(e₂\) respectively.
The \(S₁ + S₂\) is the disjoint union of \(S₁\) and \(S₂\), and for any two transition dynamics \(δ₁(s₁): 𝒫(\Bool × S₁ × K)\) and \(δ₂(s₂): 𝒫(\Bool × S₂ × K)\), then \(δ₁^*(s₁) + δ₂^*(s₂): 𝒫(\Bool × (S₁ + S₂) × K)\) is the bifunctorial lift via \(+\).

One notable difference between the original construction~\cite{smolka_GuardedKleeneAlgebra_2020} and our construction is that we use a start state \(s^* ∈ S\), instead of a start dynamics (or pseudo-state).
This choice will make the proof slightly easier. 
However, in~\Cref{sec:optimization-implementation}, we will explain that our implementation uses start dynamics instead of start state, to avoid unnecessary lookups and unreachable states.

We would like to explore several desirable theoretical properties of both derivatives and Thompson's construction.
Specifically, the correctness, i.e. the semantics of the ``start state'' the both construction have the same preserves the trace semantics of the expression; finiteness, i.e. the coalgebra generated is always finite, which means that our equivalence algorithm will eventually terminate; and finally, how does the number of reachable state relate to the size of the expression, so that we can estimate the complexity of the equivalence checking algorithm.
Turns out all of these questions can be answered by a connection by a homomorphism from symbolic Thompson's construction to the symbolic derivatives.

\begin{theorem}\label{thm:hom-thompson-derivative}
    Given any GKAT expression \(e\), the resulting symbolic GKAT coalgebra from Thompson's construction \(Ŝ_e\) have a homomorphism to derivatives \(h: Ŝ_e → D̂\), s.t. for the start state \(s^* ∈ S, h(s^*) = e\).
\end{theorem}

\begin{proof}
    By induction on the structure of \(e\). We will recall that \(h: Ŝ_e → ⟨e⟩_D\) is a symbolic GKAT coalgebra homomorphism when the following two conditions are true: \(s \transAcc{Sₑ}{b}\) if and only if \(h(s) \transAcc{D̂}{b}\); and \(s \transvia{b ∣ p}_{Sₑ} s'\) if and only if \(h(s) \transvia{b ∣ p}_{D̂} h(s')\).

    When \(e ≜ b\) for some tests \(b\), then the function \(h\) is defined as \(\{s^* ↦ b\}\).
    When \(e ≜ p\) for some primitive action \(p\), then the function \(h\) is defined as \(\{s^* ↦ p, * ↦ 1\}\).
    The homomorphism condition can then be verified by unfolding the definition.

    When \(e ≜ e₁ +_b e₂\), by induction hypothesis, we have homomorphisms \(h₁: Ŝ_{e₁} → ⟨e₁⟩_D\) and \(h₂: Ŝ_{e₂} → ⟨e₂⟩_D\).
    Then we define the homomorphism 
    \[h(s) ≜ \begin{cases}
        e₁ +_b e₂ & s = s^* \\  
        h₁(s) & s ∈ Ŝ_{e₁} \\  
        h₂(s) & s ∈ Ŝ_{e₂}
    \end{cases}\]
    We show that \(h\) is a homomorphism. Because \(Ŝₑ\) preserves the transition and acceptance of \(Ŝ_{e₁}\) and \(Ŝ_{e₂}\), then for all \(s ∈ Ŝ_{e₁} ∩ Ŝ_{e}\), we have
    \begin{align*}
        & s \transAcc{Ŝ_{e}}{c} \text{ iff }
        s \transAcc{Ŝ_{e₁}}{c} \text{ iff }
        h₁(s) \transAcc{D̂}{c} \text{ iff }
        h(s) \transAcc{D̂}{c} \\ 
        & s \transvia{c ∣ p}_{Ŝ_{e}} s' \text{ iff }
        s \transvia{c ∣ p}_{Ŝ_{e₁}} s' \text{ iff }
        h₁(s) \transvia{c ∣ p}_{D̂} h₁(s') \text{ iff }
        h(s) \transvia{c ∣ p}_{D̂} h(s')  
    \end{align*}
    And similarly for \(s ∈ Ŝ_{e₂} ∩ Ŝ_{e}\).
    So we only need to show the homomorphic condition for the start state \(s^*\):
    \begin{align*}
        & s^* \transAcc{Ŝ_{e}}{c} \\
        \text{iff } &
        (∃ a, b ∧ a = c \text{ and } 
        s₁^* \transAcc{Ŝ_{e₁}}{a})
        \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        s₂^* \transAcc{Ŝ_{e₂}}{a}) \\
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        h₁(s₁^*) \transAcc{D̂}{a})
        \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        h₂(s₂^*) \transAcc{D̂}{a}) \\  
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        e₁ \transAcc{D̂}{a})
        \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        e₂ \transAcc{D̂}{a}) \\  
        \text{iff }& e₁ +_b e₂ \transAcc{D̂}{c}\\
        \text{iff }& h(s^*) \transAcc{D̂}{c}. \\[5px]
        & s^* \transvia{a ∣ p}_{Ŝ_{e}} s' \\
        \text{iff } &
        (∃ a, b ∧ a = c \text{ and } 
        s₁^* \transvia{a ∣ p}_{Ŝ_{e₁}} s')
        \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        s₂^* \transvia{a ∣ p}_{Ŝ_{e₂}} s') \\
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        h₁(s₁^*) \transvia{a ∣ p}_{D̂} h(s'))
        \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        h₂(s₂^*) \transvia{a ∣ p}_{D̂} h(s')) \\  
        \text{iff } & 
        (∃ a, b ∧ a = c \text{ and } 
        e₁ \transvia{a ∣ p}_{D̂} h(s'))
        \text{ or } 
        (∃ a, \overline{b} ∧ a = c \text{ and } 
        e₂ \transvia{a ∣ p}_{D̂} h(s')) \\  
        \text{iff }& e₁ +_b e₂ \transvia{a ∣ p}_{D̂} h(s') \\ 
        \text{iff }& h(s^*) \transvia{a ∣ p}_{D̂} h(s')
    \end{align*}

    When \(e ≜ e₁; e₂\), by induction hypothesis, we have two homomorphisms \(h₁: Ŝ_{e₁} → D̂\) and \(h₂: Ŝ_{e₂} → D̂\).
    We define \(h\) as follows:
    \[h(s) ≜ \begin{cases}
        h₁(s); e₂ & s ∈ Ŝ_{e₁} \\  
        h₂(s) & s ∈ Ŝ_{e₂}
    \end{cases}\]
    Then we can prove that \(h\) is a homomorphism by case analysis on \(s\). 
    First case is that \(s ∈ Ŝ_{e₁}\):
    \begin{align*}
        s \transAcc{Ŝ_{e}}{c} 
        \text{ iff } & 
            ∃ a, b, a ∧ b = c, 
                s \transAcc{Ŝ_{e₁}}{a} 
                \text{ and } 
                s₂^* \transAcc{Ŝ_{e₂}}{b} \\
        \text{ iff } & 
            ∃ a, b, a ∧ b = c, 
                h₁(s) \transAcc{D̂}{a} 
                \text{ and } 
                f \transAcc{D̂}{b} \\
        \text{ iff } & h₁(s); f \transAcc{D̂}{c} 
        \text{ iff } h(s) \transAcc{D̂}{c}.\\[5px]
        s \transvia{c ∣ p}_{S_{e}} s'
        \text{ iff } & 
            (s \transvia{c ∣ p}_{Ŝ_{e₁}} s')
            \text{ or }
            (∃ a, b, a ∧ b = c
                \text{ and }
                s \transAcc{Ŝ_{e₁}}{a}
                \text{ and }
                s₂^* \transvia{b ∣ p}_{Ŝ_{e₂}} s') \\  
        \text{ iff } & 
            (h₁(s) \transvia{c ∣ p}_{D̂} h₁(s')) 
            \text{ or }
            (∃ a, b, a ∧ b = c
                \text{ and }
                h₁(s) \transAcc{D̂}{a}
                \text{ and }
                e₂ \transvia{b ∣ p}_{D̂} h₂(s')) \\
        \text{ iff } & h₁(s) \transvia{c ∣ p} h(s') 
        \text{ iff } h(s) \transvia{c ∣ p} h(s').
    \end{align*}
    The case where \(s₂ ∈ Ŝ_{e₂}\) is straightforward, as \(Ŝ_{e}\) preserves the transitions of \(Ŝ_{e₂}\):
    \begin{align*}
        & s \transAcc{Ŝ_{e}}{c} \text{ iff }
        s \transAcc{Ŝ_{e₂}}{c} \text{ iff }
        h₂(s) \transAcc{D̂}{c} \text{ iff }
        h(s) \transAcc{D̂}{c}, \\ 
        & s \transvia{c ∣ p}_{Ŝ_{e}} s' \text{ iff }
        s \transvia{c ∣ p}_{Ŝ_{e₂}} s' \text{ iff }
        h₂(s) \transvia{c ∣ p}_{D̂} h₂(s') \text{ iff }
        h(s) \transvia{c ∣ p}_{D̂} h(s').
    \end{align*}
    

    When \(e ≜ {e₁}^{(b)}\), by induction hypothesis, we have a homomorphism \(h₁: Ŝ_{e₁} → D̂\); the homomorphism \(h\) can be defined as follows: 
    \[h(s) ≜ \begin{cases}
        {e₁}^{(b)} & s ≜ s^* \\  
        h₁(s); e₁^{(b)} & s ∈ Ŝ_{e₁}
    \end{cases}\]
    We prove the homomorphism condition by case analysis on \(s\). First case is that \(s = s^*\), then:
    \begin{align*}
        (s^* \transAcc{Ŝ_e}{c})
        \text{ iff } & (s^* \transAcc{Ŝ_e}{c} \text{ and } c = \overline{b})
        \text{ iff } ({e₁}^{(b)} \transAcc{D̂}{c} \text{ and } c = \overline{b})
        \text{ iff } (h(s^*) \transAcc{D̂}{c}) \\
        (s^* \transvia{c ∣ p}_{Ŝ_e} s')
        \text{ iff } & (∃ a, b ∧ a = c \text{ and } s₁^* \transvia{a ∣ p}_{Ŝ_{e₁}} s') \\
        \text{ iff } & (∃ a, b ∧ a = c \text{ and } e₁ \transvia{a ∣ p}_{D̂} h₁(s')) \\ 
        \text{ iff } & {e₁}^{(b)} \transvia{a ∣ p}_{D̂} h₁(s') 
        \text{ iff } h(s^*) \transvia{a ∣ p}_{D̂} h(s')
    \end{align*}
    The second case is when \(s ∈ Ŝ_{e₁}\), then:
    \begin{align*}
        & s \transAcc{Ŝ_e}{c}\\
        \text{ iff } & (∃ a, \overline{b} ∧ a = c \text{ and } s \transAcc{Ŝ_{e₁}}{a}) \\  
        \text{ iff } & (∃ a, \overline{b} ∧ a = c \text{ and } h₁(s) \transAcc{D̂}{a}) \\
        \text{ iff } & h₁(s);e₁^{(b)} \transAcc{D̂}{c} 
        \text{ iff } h(s) \transAcc{D̂}{c} \\[5px]
        & s \transvia{c ∣ p}_{Ŝ_e} s' \\
        \text{ iff } & 
            (s \transvia{c ∣ p}_{Ŝ_{e₁}} s' 
            \text{ or } 
            ∃ a₁, a₂, 
                b ∧ a₁ ∧ a₂ = c 
                \text{ and } 
                s \transAcc{Ŝ_{e₁}}{a₁} 
                \text{ and } 
                s₁^* \transvia{a₂ ∣ p}_{Ŝ_{e₁}} s') \\
        \text{ iff } & 
            (h₁(s) \transvia{c ∣ p}_{D̂} h₁(s') 
            \text{ or } 
            ∃ a₁, a₂, b ∧ a₁ ∧ a₂ = c 
                \text{ and } 
                h₁(s) \transAcc{D̂}{a₁} 
                \text{ and } 
                e₁ \transvia{a₂ ∣ p}_{D̂} h₁(s'))\\
        \text{ iff } & 
            (h₁(s) \transvia{c ∣ p}_{D̂} h₁(s') 
            \text{ or } 
            ∃ a₁, a₂, b ∧ a₁ ∧ a₂ = c 
                \text{ and } 
                h₁(s) \transAcc{D̂}{a₁} 
                \text{ and } 
                e₁^{(b)} \transvia{b ∧ a₂ ∣ p}_{D̂} h₁(s');e₁^{(b)}) \\ 
        \text{ iff } &
            (h₁(s); e₁^{(b)} \transvia{c ∣ p}_{D̂} h₁(s'); e₁^{(b)}) 
        \text{ iff } h(s) \transvia{c ∣ p}_{D̂} h(s')
        \qedhere
    \end{align*}
\end{proof}

\Cref{thm:hom-thompson-derivative} have several consequences, one of the more obvious one is that we can use the functoriality of the lowering operation to show the semantic equivalence of the start state in the thompson's construction and the expression in derivative. 

\begin{corollary}[Correctness]
    Given any expression \(e\) and its Thompson's coalgebra \(Ŝ_{e}\) with a start state \(s^* ∈ Ŝ_{e}\), then the semantics of the start state is equivalent to the semantics of \(e\): \(⟦s^*⟧_{Ŝ_{e}} = ⟦e⟧.\)
\end{corollary}

A not so obvious consequence of the homomorphism in~\Cref{thm:hom-thompson-derivative}, is the complexity of the algorithm based on derivatives.
Our bisimulation algorithm (\Cref{alg:symb-bisim}) only explores the principle sub-coalgebra of the start state, i.e. \(s^*\) in the Thompson's construction \(Ŝ_{e}\) or \(e\) in the derivative \(D̂\); thus, deducing an upper bound on the size of the principle sub-coalgebras \(⟨s^*⟩_{Ŝ_{e}}\) and \(⟨e⟩_{D̂}\) are crucial to our complexity analysis.
An upper bound on \(⟨s^*⟩_{Ŝ_{e}}\) is easy to obtain, as the size of \(Ŝ_{e}\), which subsumes the states of \(⟨s^*⟩_{Ŝ_{e}}\), is linear to the size of expression \(e\); therefore \(⟨s^*⟩_{Ŝ_{e}}\) is at most linear to the size of the expression \(e\).
On the other hand the size of \(⟨e⟩_{D̂}\) can, again, be derived from the homomorphism in~\cref{thm:hom-thompson-derivative}.

\begin{corollary}\label{thm:suj-hom-thompson-derivative}
    There exists a surjective homomorphism \(h': ⟨s^*⟩_{Ŝ_{e}} → ⟨e⟩_{D̂}\). 
    Because the size of \(⟨s^*⟩_{Ŝ_{e}}\) is linear to \(e\), the size of \(⟨e⟩_{D̂}\) is at most linear to the size of expression \(e\).
\end{corollary}

\begin{proof}
    We define \(h'\) to be point-wise equal to \(h\), i.e. \(h'(s) ≜ h(s)\). 
    Then we need to show that \(h'\) is well-defined ans surjective, which is a consequence of homomorphic image preserves principle sub-coalgebra (\Cref{thm:homo-img-preserve-principle-sub-coalg}): \(h(⟨s^*⟩_{Ŝ_{e}}) = ⟨h(s)⟩_{D̂} = ⟨e⟩_{D̂}.\)
    In other words, the image of \(h\) on \(⟨s^*⟩_{Ŝ_{e}}\) is equal to \(⟨e⟩_{D̂}\); thus, because \(h'\) is point-wise equal to \(h\) and is defined on \(⟨s^*⟩_{Ŝ_{e}}\), the range of \(h'\) contains its codomain \(⟨e⟩_{D̂}\), showing that \(h'\) is surjective.
\end{proof}

An important consequence of~\cref{thm:suj-hom-thompson-derivative} is that \(⟨s^*⟩_{Ŝ_{e}}\) will have no less state than \(⟨e⟩_{D̂}\).
However, it is important to notice that this does not mean running bisimulation algorithm on 

\section{Implementation}

\subsection{Optimization}\label{sec:optimization-implementation}

\subsection{Performance}\label{sec:performance-implementation}


\section{Future Work}

Can weak symbolic coalgebra leads to a simpler completeness proof.



\printbibliography

\newpage
\appendix


\end{document}